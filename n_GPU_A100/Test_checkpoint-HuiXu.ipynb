{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76838c1-daab-456f-8985-6467734c3b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pip', 'install', '-q', '--extra-index-url', 'https://download.pytorch.org/whl/cu117', 'torch==2.0.0', 'torchvision==0.15.0', 'omegaconf', 'torchmetrics==0.10.3', 'fvcore', 'iopath', 'xformers==0.0.18', 'submitit', 'numpy<2.0']\n",
      "['pip', 'install', '-q', '--extra-index-url', 'https://pypi.nvidia.com', 'cuml-cu11']\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "import os\n",
    "import subprocess\n",
    "pip_commands = [\n",
    "    [\"pip\", \"install\",\"-q\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\", \n",
    "     \"torch==2.0.0\", \"torchvision==0.15.0\", \"omegaconf\", \"torchmetrics==0.10.3\", \n",
    "     \"fvcore\", \"iopath\", \"xformers==0.0.18\", \"submitit\", \"numpy<2.0\"],\n",
    "    [\"pip\", \"install\", \"-q\",  \"--extra-index-url\", \"https://pypi.nvidia.com\", \"cuml-cu11\"],\n",
    "    [\"pip\", \"install\",\"-q\",  \"black==22.6.0\", \"flake8==5.0.4\", \"pylint==2.15.0\"],\n",
    "    [\"pip\", \"install\", \"-q\", \"mmsegmentation==0.27.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"mmcv-full==1.5.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"nibabel\"]\n",
    "]\n",
    "\n",
    "for cmd in pip_commands:\n",
    "    try:\n",
    "        print(cmd)\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install packages with command: {cmd}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13db0142-1176-4c27-82b7-aa62b26db777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 23 13:35:40 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   42C    P0             59W /  400W |       3MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb97cd4c-a835-4f65-9f2e-643e3684ab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/tmp/ipykernel_443/1877029792.py:26: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if getattr(m,n).storage().size()==0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights • backbone missing=0 • agg keys=26\n",
      "▶ Found 1428 folders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [18:24<00:00,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# ═════════════  CONFIG  ════════════════════════════════════════════════\n",
    "CHECKPOINT_DIR = \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/output_ddp_p16_23_1150/checkpoints\"\n",
    "BASE_CKPT       = f\"{CHECKPOINT_DIR}/base_model_epoch_19.pt\"  #base_model_final.pt   # or base_model_epoch_X.pt\n",
    "AGG_CKPT        = f\"{CHECKPOINT_DIR}/aggregator_epoch_19.pt\"  #aggregator_final.pt   # or aggregator_epoch_X.pt\n",
    "ROOT_DIR  = \"/rsrch7/home/ip_rsrch/wulab/HuiXu/testData/Projects/SABR/SABR_Dataset_Final_V1\"\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import os, sys, torch, nibabel as nib, numpy as np, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- 1. CombinedModel ---------------------------------------------------------\n",
    "sys.path.insert(0, \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6\")\n",
    "from dinov2.models.vision_transformer import DinoVisionTransformer\n",
    "\n",
    "def build_backbone():\n",
    "    m = DinoVisionTransformer(\n",
    "        img_size=448, patch_size=16, drop_path_rate=0.0,\n",
    "        block_chunks=1, drop_path_uniform=True,\n",
    "        embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        num_register_tokens=5, init_values=1e-5,\n",
    "    )\n",
    "    # materialise empty tokens\n",
    "    with torch.no_grad():\n",
    "        for n in (\"cls_token\",\"register_tokens\",\"mask_token\"):\n",
    "            if getattr(m,n).storage().size()==0:\n",
    "                real = torch.zeros_like(getattr(m,n))\n",
    "                torch.nn.init.normal_(real,std=0.02)\n",
    "                setattr(m,n,torch.nn.Parameter(real,requires_grad=True))\n",
    "    return m\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, base_model, num_attn_heads=3, num_layers=2,\n",
    "                 hidden_dim=1024, chunk_feat_dim=768, dropout_rate=0.3, lse_tau=1.0):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.lse_tau = lse_tau\n",
    "        self.chunk_feat_dim = chunk_feat_dim + 3                # 768 + spacing(3) = 771\n",
    "\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=self.chunk_feat_dim, nhead=num_attn_heads,\n",
    "            dim_feedforward=hidden_dim, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(enc, num_layers=num_layers)\n",
    "        self.classifier  = nn.Linear(self.chunk_feat_dim, 6)    # not used here\n",
    "\n",
    "    def _chunk_embed(self, chunk):\n",
    "        # backbone returns [1, 768]\n",
    "        with torch.no_grad():          # frozen backbone for inference\n",
    "            return self.base(chunk.unsqueeze(0))\n",
    "\n",
    "    def pooled(self, x, spacing=(1.0,1.0,1.0)):\n",
    "        S, device, dtype = x.size(0), x.device, x.dtype\n",
    "        feats = torch.cat([ self._chunk_embed(x[i]) for i in range(S) ], dim=0)   # [S,768]\n",
    "        spacing_vec = torch.tensor(spacing,dtype=dtype,device=device).expand(S,3)\n",
    "        feats = torch.cat([feats, spacing_vec], dim=1)                            # [S,771]\n",
    "        enc = self.transformer(feats.unsqueeze(0))                                # [1,S,771]\n",
    "        return self.lse_tau * torch.logsumexp(enc / self.lse_tau, dim=1).squeeze(0)  # [771]\n",
    "\n",
    "# build\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CombinedModel(build_backbone()).to(device).eval()\n",
    "\n",
    "# --- 2. load weights ----------------------------------------------------------\n",
    "missing, _ = model.base.load_state_dict(torch.load(BASE_CKPT, map_location=\"cpu\"), strict=False)\n",
    "agg_state  = torch.load(AGG_CKPT, map_location=\"cpu\")\n",
    "# drop backbone keys if present\n",
    "agg_state  = {k:v for k,v in agg_state.items() if not k.startswith(\"base.\")}\n",
    "model.load_state_dict(agg_state, strict=False)\n",
    "print(f\"Loaded weights • backbone missing={len(missing)} • agg keys={len(agg_state)}\")\n",
    "\n",
    "# --- 3. volume → batch of chunks ---------------------------------------------\n",
    "CHUNK_DEPTH = 3; V_MIN, V_MAX = -1000, 150; OUT_SZ = (448,448)\n",
    "@torch.no_grad()\n",
    "def patient_vector_771(nii_path):\n",
    "    \"\"\"\n",
    "    Return one 771-D pooled vector for a CT whose in-plane size is 512×512.\n",
    "    Skip (return None) if that condition isn’t met.\n",
    "    \"\"\"\n",
    "    hdr  = nib.load(nii_path)\n",
    "    vol  = hdr.get_fdata().astype(np.float32)                # numpy array\n",
    "\n",
    "    # --- quick shape guard ---------------------------------------------------\n",
    "    if vol.shape[0] != 512 or vol.shape[1] != 512:\n",
    "        print(f\"⚠️  {nii_path} skipped – in-plane dims {vol.shape[:2]} not 512×512\")\n",
    "        return None                                          # skip this CT\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # window + scale to [0,1]\n",
    "    vol = np.clip((vol - V_MIN) / (V_MAX - V_MIN), 0, 1)     # shape (512,512,D)\n",
    "    D   = vol.shape[2]\n",
    "    S   = D // CHUNK_DEPTH\n",
    "    if S == 0:\n",
    "        print(f\"⚠️  {nii_path} has <{CHUNK_DEPTH} slices\"); return None\n",
    "\n",
    "    # --- build [S,3,448,448] tensor -----------------------------------------\n",
    "    S = max(S, 72)\n",
    "    chunks = []\n",
    "    for start in range(0, D - CHUNK_DEPTH + 1, CHUNK_DEPTH):\n",
    "        chunk = vol[:, :, start:start + CHUNK_DEPTH]    # exact 3 slices (512,512,3)\n",
    "        chunk = chunk.transpose(2, 0, 1)                # → (3,512,512)\n",
    "    \n",
    "        t = torch.from_numpy(chunk).unsqueeze(0)        # [1,3,H,W]\n",
    "        t = torch.nn.functional.interpolate(\n",
    "                t, OUT_SZ, mode=\"bilinear\", align_corners=False)\n",
    "        t = (t - 0.5) / 0.5\n",
    "        chunks.append(t)\n",
    "\n",
    "    x = torch.cat(chunks).to(device)                         # [S,3,448,448]\n",
    "    spacing = hdr.header.get_zooms()[:3]\n",
    "    vec = model.pooled(x, spacing).cpu().numpy()             # [771]\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. iterate folders -------------------------------------------------------\n",
    "folders = [os.path.join(ROOT_DIR,p) for p in os.listdir(ROOT_DIR)\n",
    "           if os.path.isdir(os.path.join(ROOT_DIR,p))]\n",
    "print(f\"▶ Found {len(folders)} folders\")\n",
    "folders.sort()\n",
    "folders = folders[1142+263:]\n",
    "for fdir in tqdm(folders):\n",
    "    nii_path = os.path.join(fdir, \"CT.nii.gz\")\n",
    "    if not os.path.exists(nii_path):\n",
    "        print(\"⚠️ \", nii_path, \" missing\"); continue\n",
    "\n",
    "    vec = patient_vector_771(nii_path)\n",
    "    if vec is None:\n",
    "        print(\"⚠️ \", fdir, \" had <3 slices\"); continue\n",
    "\n",
    "    pd.DataFrame([vec], columns=[f\"feat_{i}\" for i in range(771)]\n",
    "                 ).to_csv(os.path.join(\n",
    "                     fdir, f\"{os.path.basename(fdir)}_aggMori.csv\"),\n",
    "                     index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0801781e-dcfe-45f0-bbef-920adaaa805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 23 13:54:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             89W /  400W |    3529MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540ad32-549f-4558-ae70-535002e47f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de5826-773a-4c6d-b93d-8b360ef55adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
