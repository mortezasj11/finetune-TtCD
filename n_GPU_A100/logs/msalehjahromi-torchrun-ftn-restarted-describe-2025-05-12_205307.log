Name:                 msalehjahromi-torchrun-ftn-m4l44
Namespace:            yn-gpu-workload
Priority:             1000
Priority Class Name:  high-nonpreempting
Service Account:      default
Node:                 hpcgpu12/10.113.120.211
Start Time:           Mon, 12 May 2025 20:49:26 -0500
Labels:               batch.kubernetes.io/controller-uid=0271f436-92bf-454c-af50-c99647ef7548
                      batch.kubernetes.io/job-name=msalehjahromi-torchrun-ftn
                      controller-uid=0271f436-92bf-454c-af50-c99647ef7548
                      job-name=msalehjahromi-torchrun-ftn
                      k8s-user=msalehjahromi
                      runai/queue=msalehjahromi-yn-gpu-workload-queue
Annotations:          cni.projectcalico.org/containerID: 90beca50699f9187df302bff9ef6d65987156ee4cbd26290c880a52ecc26c2ca
                      cni.projectcalico.org/podIP: 10.0.117.142/32
                      cni.projectcalico.org/podIPs: 10.0.117.142/32
                      pod-group-name: pg-msalehjahromi-torchrun-ftn-m4l44-0271f436-92bf-454c-af50-c99647ef7548
                      received-resource-type: Regular
                      runai-job-id: 0271f436-92bf-454c-af50-c99647ef7548
Status:               Running
IP:                   10.0.117.142
IPs:
  IP:           10.0.117.142
Controlled By:  Job/msalehjahromi-torchrun-ftn
Containers:
  main:
    Container ID:  containerd://6a1bf371445de97072eefa3bcbf185a35a2c0396eba6c2114022787b97f27424
    Image:         hpcharbor.mdanderson.edu/nnunetv2/nnunetv2@sha256:4ab016ba4b356842be74fbf58159480598bfc015c8454339022aa0fcbfdc196d
    Image ID:      hpcharbor.mdanderson.edu/nnunetv2/nnunetv2@sha256:4ab016ba4b356842be74fbf58159480598bfc015c8454339022aa0fcbfdc196d
    Port:          <none>
    Host Port:     <none>
    Command:
      torchrun
    Args:
      --nproc_per_node=8
      --nnodes=1
      --node_rank=0
      --master_addr=$(MASTER_ADDR)
      --master_port=$(MASTER_PORT)
      /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_fsdp_launcher.py
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Mon, 12 May 2025 20:49:28 -0500
      Finished:     Mon, 12 May 2025 20:53:05 -0500
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:             192
      nvidia.com/gpu:  8
    Requests:
      cpu:             192
      nvidia.com/gpu:  8
    Environment:
      HOME:                /rsrch1/ip/msalehjahromi
      MASTER_ADDR:          (v1:status.podIP)
      MASTER_PORT:         29500
      NCCL_DEBUG:          INFO
      NCCL_SOCKET_IFNAME:  eth0
      NCCL_IB_DISABLE:     1
    Mounts:
      /dev/shm from shm (rw)
      /rsrch1/ip/msalehjahromi/ from home (rw)
      /rsrch7/home/ip_rsrch/wulab from ifp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ccc7w (ro)
Conditions:
  Type              Status
  PodBound          True 
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  shm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  32Gi
  ifp:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  msalehjahromi-gpu-rsrch7-home-ip-rsrch
    ReadOnly:   false
  home:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  msalehjahromi-gpu-home
    ReadOnly:   false
  kube-api-access-ccc7w:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              nvidia.com/cuda.runtime.major=12
                             nvidia.com/gpu.machine=DGXA100-920-23687-2530-000
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
