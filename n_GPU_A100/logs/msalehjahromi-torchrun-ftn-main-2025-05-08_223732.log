[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:43.211949638-05:00 WARNING:torch.distributed.run:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:43.212005404-05:00 *****************************************
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:43.212010454-05:00 Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:43.212014372-05:00 *****************************************
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:45.813521475-05:00 Running command: torchrun --nproc_per_node=4 --master_port=54225 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py --csv /rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv --accum-steps 100 --num-workers 10 --epochs 50 --lr 0.0001 --weight-decay 0.0001 --optimizer adamw --num-attn-heads 3 --num-layers 2 --dropout 0.3 --unfreeze-strategy all --output /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu --print-every 200 --val-every 400 --metrics-dir /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:45.813721556-05:00 Running command: torchrun --nproc_per_node=4 --master_port=50771 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py --csv /rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv --accum-steps 100 --num-workers 10 --epochs 50 --lr 0.0001 --weight-decay 0.0001 --optimizer adamw --num-attn-heads 3 --num-layers 2 --dropout 0.3 --unfreeze-strategy all --output /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu --print-every 200 --val-every 400 --metrics-dir /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:45.813834601-05:00 Running command: torchrun --nproc_per_node=4 --master_port=33107 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py --csv /rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv --accum-steps 100 --num-workers 10 --epochs 50 --lr 0.0001 --weight-decay 0.0001 --optimizer adamw --num-attn-heads 3 --num-layers 2 --dropout 0.3 --unfreeze-strategy all --output /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu --print-every 200 --val-every 400 --metrics-dir /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:45.845334342-05:00 Running command: torchrun --nproc_per_node=4 --master_port=56721 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py --csv /rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv --accum-steps 100 --num-workers 10 --epochs 50 --lr 0.0001 --weight-decay 0.0001 --optimizer adamw --num-attn-heads 3 --num-layers 2 --dropout 0.3 --unfreeze-strategy all --output /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu --print-every 200 --val-every 400 --metrics-dir /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.463121368-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.463170452-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.464202656-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.464215881-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.465943590-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.465953949-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.466900831-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.555917311-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.555932169-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.557038264-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.557047301-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.558646976-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.558654370-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.559891595-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.589110465-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.589120013-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.590089338-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.590101070-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.592177743-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.592184355-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.593190299-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.604008850-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.604018949-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.604844199-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.604851403-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.606025378-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.606033213-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.607512648-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.900508197-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.900530690-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.901708882-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.901717108-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.903426671-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.903437071-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.904610244-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.937267258-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.937281074-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.938190234-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.938199863-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.939720807-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.939734303-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.940644075-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.965928367-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.965943566-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.966813933-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.966826136-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.968325149-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.968332273-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:55.969301397-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.050443699-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.050458477-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.051463841-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.051475603-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.053590989-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.053601179-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.053695728-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.053701459-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.054685482-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.054699188-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.054912685-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.056366582-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.056372743-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.056571853-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.056577032-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.057643111-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.057652299-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.057659422-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.059407920-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.059415044-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.060479770-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.067825581-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.067833527-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.068761743-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.068771452-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.070186134-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.070198748-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.071234059-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.073017293-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.073026580-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.073916083-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.073925030-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.075337318-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.075345524-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.076149674-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.084838521-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.084850754-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.085740558-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.085751068-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.087048076-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.087080848-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.087887063-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.091978249-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.091987307-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.092960218-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.092971370-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.094478157-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.094487024-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.095480194-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.102671611-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.102689666-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.103524024-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.103537008-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.105075797-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.105085856-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.106622942-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.118700879-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.118711789-05:00   warnings.warn("xFormers is available (SwiGLU)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.119745857-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.119756207-05:00   warnings.warn("xFormers is available (Attention)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.121882263-05:00 /rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.121890118-05:00   warnings.warn("xFormers is available (Block)")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.123049395-05:00 Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.474789267-05:00 2025-05-09 03:37:56,474 - INFO - Starting training on 4 GPUs with full model copies (DDP)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.651972819-05:00 2025-05-09 03:37:56,651 - INFO - Starting training on 4 GPUs with full model copies (DDP)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:56.674328708-05:00 2025-05-09 03:37:56,674 - INFO - Starting training on 4 GPUs with full model copies (DDP)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:37:57.406727724-05:00 2025-05-09 03:37:57,406 - INFO - Starting training on 4 GPUs with full model copies (DDP)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.117036655-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:99 [0] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.117605427-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:99 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.117907612-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:99 [0] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.117949782-05:00 NCCL version 2.14.3+cuda11.7
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.121131909-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.126220704-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.126241504-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.184939109-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:87 [0] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.185440543-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:87 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.185783175-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:87 [0] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.185829544-05:00 NCCL version 2.14.3+cuda11.7
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.188597993-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.190143304-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.190158112-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.305553787-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:105 [0] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.306023721-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:105 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.306300818-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:105 [0] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.306311018-05:00 NCCL version 2.14.3+cuda11.7
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.311064545-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.358461122-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:107 [2] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.367843388-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:106 [1] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.371619856-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.371636738-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.374081862-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:107 [2] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.374160101-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:107 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.376968145-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.377478316-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:106 [1] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.377577445-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:106 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.378059652-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.378065413-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.380511228-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:103 [2] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.380601360-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.384970986-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.384987037-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.385963896-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:89 [2] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.388471849-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:103 [2] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.388544307-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:103 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.391735310-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:89 [2] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.391807217-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.391826033-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:89 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.392069286-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.392096227-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.402221377-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.403586536-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.403596866-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.404819232-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:104 [3] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.409725730-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:90 [3] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.411631998-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:104 [3] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.411653720-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:93 [0] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.411707852-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:104 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.412303025-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:93 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.412900112-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:93 [0] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.412918607-05:00 NCCL version 2.14.3+cuda11.7
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.414725766-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:97 [2] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.415007062-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.415468169-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.415478869-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.423777895-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:101 [1] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.423899165-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.426508762-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:90 [3] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.426530053-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.426600737-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.426637427-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:90 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.429769889-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.435416295-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:97 [2] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.435604193-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:97 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.437368090-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.437381045-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.437546910-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:101 [1] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.437561017-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:101 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.437726041-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:98 [3] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.439392402-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.440548162-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.441311585-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.441325321-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.442459239-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.442469058-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.444079112-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:98 [3] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.444175265-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:98 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.447242803-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.448235353-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.448242917-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.453677901-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:108 [3] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.486812554-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:95 [1] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.487709993-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:108 [3] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.487812357-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:108 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.494902612-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:88 [1] NCCL INFO cudaDriverVersion 12040
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.498897064-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:95 [1] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.498987145-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:95 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.503996599-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:88 [1] NCCL INFO Bootstrap : Using eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.504067004-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:88 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.515884375-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.516346735-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.516357816-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.519400777-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.519676692-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.519689216-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.532486041-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.532931299-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.111.22<0>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:01.532942450-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Using network Socket
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.534174019-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Setting affinity for GPU 1 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.534303024-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.559998489-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Setting affinity for GPU 3 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.587122813-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629762145-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 00/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629773798-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 01/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629776753-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 02/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629779559-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 03/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629782174-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 04/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629788205-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629792944-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 05/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629797954-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629803003-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 06/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629838842-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 07/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629842138-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 08/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629844913-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 09/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629847258-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629849823-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 10/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629852207-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 11/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629854531-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 12/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629861665-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 13/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629864360-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 14/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629866704-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 15/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629869931-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 16/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629872275-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 17/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629874950-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 18/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629877696-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 19/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629879940-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 20/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629882214-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 21/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629884789-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 22/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629887033-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 23/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.629889518-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.967206180-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 00/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.978474055-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 01/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:03.985011948-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 00/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.002205422-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 02/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.013066443-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 03/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.013085109-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 01/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.019548490-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 04/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.019564310-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 02/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.019977787-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 00/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.031247676-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 05/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.037893274-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 03/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.040088051-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 01/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.040395046-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 04/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.040464548-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 02/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.047384779-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 06/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.047649433-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 05/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.047895531-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 03/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.048069332-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 07/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.048180804-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 06/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.048484532-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 04/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.048679944-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 08/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.051993240-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 05/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.052004211-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.052045970-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 07/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.058159105-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 09/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.060238923-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 06/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.060425068-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.060797266-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 08/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.060846129-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 10/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.060935729-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 07/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.061116844-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.061446191-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 09/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.061455779-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 11/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.061596798-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 08/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.061746853-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.062091970-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 12/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.062128539-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 10/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.062217719-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 09/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.062385118-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 04/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.062737489-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 13/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.062776142-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 11/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.062863428-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 10/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.063038872-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 05/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.063381684-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 14/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.063487726-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 12/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.063502344-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 11/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.063666998-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 06/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.063980905-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 15/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.064149305-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 13/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.064184713-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 12/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.064262110-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 07/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.064530942-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 16/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.065204724-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.071081469-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 14/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.071241684-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 13/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.073862542-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 14/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.073875877-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 08/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.074021174-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 15/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.074533088-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 15/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.074550481-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Setting affinity for GPU 1 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.074583675-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 09/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.074629532-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 17/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.079452070-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 16/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.079705052-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 16/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.079831512-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 10/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.085729678-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 17/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.091657610-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 17/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.091674102-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 18/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.091836260-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Setting affinity for GPU 3 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.091919449-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 11/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.092104651-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.092193720-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.092380726-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 18/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.092391697-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 19/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.094404238-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 12/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.094417232-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 19/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.095779946-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 20/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.096036173-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 20/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.096174948-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 21/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.096472394-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 13/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.096643549-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 18/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.096839332-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 21/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.097162127-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 14/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.097489470-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 22/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.098179964-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 22/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.098418899-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 19/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.099127177-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 15/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.099331165-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 23/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.099779398-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 16/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.100165172-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 23/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.100827773-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 20/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.100840889-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 17/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.102235673-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 21/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.102252705-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 18/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.102579017-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 22/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.102636025-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 19/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.102923953-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 23/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.102976443-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 20/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.103312733-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 21/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.104045518-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 22/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.104868314-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Channel 23/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.113953915-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 00/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.113965227-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.113968403-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 01/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.113971219-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 02/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.113973483-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 03/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.113976950-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.113998691-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 04/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114006716-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 05/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114009020-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 06/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114012698-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114014962-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 07/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114027265-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 08/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114029550-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 09/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114032856-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 10/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114034900-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 11/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114036974-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 12/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114039138-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 13/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114041182-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 14/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114048446-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 15/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114050730-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 16/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114052974-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 17/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114057062-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 18/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114059116-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 19/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114061150-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 20/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114063374-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 21/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114065468-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 22/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114067502-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 23/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.114069706-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.123739079-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.125490031-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.141995355-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Setting affinity for GPU 3 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.155081080-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Setting affinity for GPU 3 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.168944936-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Setting affinity for GPU 1 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.169070154-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.169186125-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Setting affinity for GPU 1 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193773260-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193786555-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193790292-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 00/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193793258-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 01/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193798378-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 02/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193801333-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 03/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193804028-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 04/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193818175-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 05/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193822624-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 06/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193825008-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 07/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193827373-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 08/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193829828-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 09/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193835519-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 10/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193837923-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 11/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193840518-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 12/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193843143-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 13/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193880574-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 14/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193883590-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 15/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193894391-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 16/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193896635-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 17/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193899721-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 18/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193906443-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 19/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193908858-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 20/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193911052-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 21/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193913848-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 22/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193916673-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 23/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193919388-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.193921732-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196053169-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196064471-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196068168-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196092564-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 00/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196103916-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 01/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196107332-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 02/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196110137-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 03/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196112552-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 04/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196122912-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 05/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196125346-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 06/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196127651-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 07/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196129985-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 08/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196135566-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 09/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196138111-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 10/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196140375-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 11/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196142690-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 12/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196144823-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 13/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196147318-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 14/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196150895-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 15/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196153260-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 16/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196155614-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 17/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196161045-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 18/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196163699-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 19/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196166054-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 20/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196168408-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 21/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196174821-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 22/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196177696-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 23/24 :    0   1   2   3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.196185761-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.282036866-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.286986827-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 00/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.289875625-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.293856211-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 00/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.294416678-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 01/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.295489940-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 00/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.296638857-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.298593788-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.298618074-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.299533937-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 01/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.300092450-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 02/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.301411931-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 01/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.302148774-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.302165886-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 00/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.302318717-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.303247525-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 02/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.303423530-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 03/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.304798868-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 02/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.305100862-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 01/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.305198598-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.306182942-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 03/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.306324752-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 04/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.307696913-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 03/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.307869822-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 02/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.308043403-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 04/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.309175878-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 05/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.309192419-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 04/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.310522040-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 04/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.310624866-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 03/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.310884831-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 05/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.311919650-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 06/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.311985635-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 05/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.313327179-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 05/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.313350203-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 04/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.313605088-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 06/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.314731512-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 07/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.315533648-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 08/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.316133099-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 07/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.316477344-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 09/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.317077827-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 08/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.317481766-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 10/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.317991677-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 09/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.318559838-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 11/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.318913020-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 10/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.319510357-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 12/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.319899127-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 11/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.320470124-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 13/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.320945509-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 12/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.321514602-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 06/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.321898493-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 00/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.322088704-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 06/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.322234382-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 05/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.322240553-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 00/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.322621819-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 14/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.323396444-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.323772720-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 13/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.324308329-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 07/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.324769186-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 01/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.325005556-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 07/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.325049049-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 06/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.325159619-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 01/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.325475541-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 15/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.326212663-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.326641961-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 14/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.326890443-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 00/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.327075926-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 08/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.327603981-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 02/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.327750099-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 08/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.327793762-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 07/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.328008061-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 02/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.328318902-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 16/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.329023493-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.329482857-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 15/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.329765445-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 01/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.329821842-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 09/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.330500294-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 03/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.330618168-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 09/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.330625893-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 08/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.330884785-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 03/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.331240192-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 17/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.331869971-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.332558150-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 16/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.332836921-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 02/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.332904379-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 10/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.333561570-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 04/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.333634208-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 10/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.333665909-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 09/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.333921605-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 04/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.334263105-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 18/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.334951255-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 04/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.335325918-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 17/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.335608295-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 03/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.335690803-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 11/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.336326121-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 05/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.336421834-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 10/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.336438395-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 11/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.336696907-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 05/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.337050530-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 19/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.337680259-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 05/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.338092213-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 18/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.338329845-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 04/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.338389699-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 12/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.339037923-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 06/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.339065125-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 11/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.339145738-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 12/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.339390494-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 06/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.339735180-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 20/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.340198141-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 00/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.340313049-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 06/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.340360620-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 00/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.340541354-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.340842407-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 19/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.341031868-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 13/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.341140625-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 00/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.341410538-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 05/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.341707734-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 12/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.341716651-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 07/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.341776545-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 13/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.342097546-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 07/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.342433686-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 21/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.342835410-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 01/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.342922747-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 07/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.343017166-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 01/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.343149979-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.343528920-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 20/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.343648688-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 14/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.343831797-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 01/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.344182294-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 06/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.344288476-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 13/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.344367256-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 08/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.344517031-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 14/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.344869101-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 08/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.345138584-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 22/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.345547542-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 02/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.345631412-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 08/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.345723367-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 02/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.345855458-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.346220403-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 21/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.346358926-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 15/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.346478734-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 02/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.346876792-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 07/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.346893344-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 14/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.347161414-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 09/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.347218502-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 15/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.347540255-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 09/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.347871355-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 23/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.348224727-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 03/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.348267439-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 09/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.348372559-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 03/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.348543484-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.348922575-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 22/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.348944728-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 16/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.349225211-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 03/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.349588924-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 15/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.349606617-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 08/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.349821586-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 10/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.349912379-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 16/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.350208994-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 10/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.350951327-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 04/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.351073940-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 10/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.351097334-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 04/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.351320659-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 04/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.351659094-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Channel 23/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.351703679-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 17/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.351937594-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 04/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.352165908-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 16/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.352249547-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 09/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.352362452-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 11/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.352483423-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 17/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.352957314-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 11/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.353440665-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 05/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.353601791-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 05/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.353614255-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 11/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.353821990-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 05/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.354297455-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 18/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.354510000-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 05/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.354684873-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 17/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.354779493-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 10/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.354885905-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 12/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.355020100-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 18/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.355182740-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 12/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.355867173-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 06/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.356029481-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 12/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.356038579-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 06/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.356252706-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 06/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.356626668-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 19/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.356823573-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 06/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.356970553-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 18/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.357052960-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 11/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.357162228-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 13/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.357280593-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 19/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.358133837-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 13/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.358145549-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 07/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.358392709-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 07/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.358606887-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 20/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.358783443-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 07/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.358858606-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 19/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.359077633-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 12/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.359126827-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 14/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.359319122-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 20/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.359993977-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 08/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.360157538-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 14/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.360350626-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 08/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.360576656-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 21/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.360751178-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 08/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.360891185-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 20/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.361168502-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 13/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.361191556-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 15/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.361775347-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 21/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.361962293-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 09/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.362097020-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 15/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.362373135-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 09/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.362559840-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 22/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.362709475-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 09/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.362887354-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 21/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.363164862-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 16/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.363248701-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 14/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.363790613-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 22/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.363797886-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 00/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.363923646-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 10/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.364125721-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 16/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.364365206-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 10/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.364580576-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 23/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.364793872-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 10/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.364964066-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 22/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.365203552-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 17/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.365356884-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 15/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.365589366-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 13/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.365914986-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 01/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.366117521-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 11/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.366348211-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 07/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.366390761-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 17/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.366442219-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 23/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.366576565-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 11/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.367183961-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 11/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.367260948-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Channel 23/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.367487098-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 18/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.369075120-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 16/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.369329364-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 14/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.369438091-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 02/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.369860104-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 12/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.370010120-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 00/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.370110962-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 08/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.370140057-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 18/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.371616607-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 12/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.374059176-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 12/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.374821447-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 19/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.375030425-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 17/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.376434307-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 15/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.377085616-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 03/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.378890902-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 13/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.379598549-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 01/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.379611083-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 09/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.379616663-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 19/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.385282917-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 14/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.385452881-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 13/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.386246522-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 20/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.387820738-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 13/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.388203075-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 02/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.388489030-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 20/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.388623666-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 10/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.388633284-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 18/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.388641359-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 04/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.388698879-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 03/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.388845428-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 21/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.389344347-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 14/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.389543156-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 21/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.389713750-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 11/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.389835132-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 05/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.390090618-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 19/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.390103813-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 22/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.390217960-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 04/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.390406509-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 15/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.390473106-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 12/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.390694287-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 22/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.390945795-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 06/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.391203937-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 20/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.391222081-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 23/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.391424557-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 13/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.391437601-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 05/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.391606873-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 16/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.391921192-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Channel 23/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.392163433-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 07/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.392362091-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 21/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.392423327-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 14/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.392547343-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 06/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.395671229-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 17/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.395956081-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 16/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.396100817-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 08/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.396752677-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 22/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.398474324-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 07/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.398482389-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 18/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.398531132-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 15/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.398915443-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 14/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.399274336-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 23/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.399718622-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 19/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.399730294-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 08/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.399742107-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 16/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.400619336-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 15/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.400782146-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 15/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.403109886-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 20/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.406825949-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 09/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.406845496-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 17/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.408549780-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 17/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.408988715-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 16/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.409266684-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 09/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.409279639-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 16/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.410684432-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 21/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.411310354-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 10/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.411790137-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 18/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.414595686-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 18/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.414610134-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 17/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.415557247-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 17/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.415575060-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 10/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.416587998-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 11/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.420200313-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 12/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.421791502-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 11/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.422617263-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 18/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.422677568-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 22/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.423293530-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 19/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.425679962-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 18/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.426352482-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Channel 23/0 : 0[7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.427012879-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 19/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.428328033-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 20/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.429053183-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 19/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.429789023-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 13/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.432846362-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 21/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.433441114-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 12/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.434698317-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 22/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.435804402-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 13/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.442318059-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 23/0 : 2[b7000] -> 3[bd000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.442332286-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 14/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.442363906-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 19/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.442592121-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 20/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.443307091-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 20/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.443551637-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 14/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.447845359-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 15/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.447869675-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 20/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.449251034-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 21/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.449357366-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 15/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.449898526-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 21/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.452697534-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 16/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.456231108-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 17/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.461257505-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 16/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.462647571-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 17/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.464648499-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 18/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.465990944-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 18/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.466004169-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 22/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.466010090-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 21/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.467362284-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 22/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.471112631-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 19/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.475047971-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 23/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.482043415-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 22/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.484362679-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 23/0 : 1[f000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.484723446-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 20/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.486680581-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 21/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.488491878-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 22/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.489584748-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 19/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.493981596-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Channel 23/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.495898404-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 23/0 : 3[bd000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.502418003-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 20/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.520107059-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 21/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.529431716-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 22/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.531052811-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Channel 23/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.611715872-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.611786216-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 00/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.613181812-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 01/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.620549675-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 02/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.636796426-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.650942901-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.680958076-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 03/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.728234845-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 04/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.778668312-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 05/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.843411482-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 06/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.846414778-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.846433624-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.846637202-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.861003875-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 07/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.874665486-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.874713648-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 00/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.888571493-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 08/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.896111603-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.900473585-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 01/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.903147144-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 02/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.904448340-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 03/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.906212177-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 04/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.926801604-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 05/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.927818930-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 06/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.929122340-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 07/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.929823324-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 09/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.930571759-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 08/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.931382953-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 10/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.932149111-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 09/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.932991484-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.933189581-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 11/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.941031847-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.949092999-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 10/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.953193944-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 11/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.953791932-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.953809666-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.954017602-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.954168419-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 12/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.954795973-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 13/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.956053877-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 14/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.961770217-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 15/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.969500639-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 12/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.980801668-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.980860300-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.981056884-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.981888337-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 13/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.982817655-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 14/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.984171973-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 15/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.988240065-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 16/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:04.994383037-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 16/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.015886613-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 17/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.019535097-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 17/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.022561156-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 18/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.025436689-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.026115802-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 18/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.026123907-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 19/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.028498025-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 20/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.028520899-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.028574461-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 19/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.034141336-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 21/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.034337118-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 20/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.041832283-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 22/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.041984163-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 21/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.046296650-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Channel 23/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.046497272-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.046602823-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 22/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.047749245-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Channel 23/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.050840508-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.050855848-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 00/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.051493070-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 01/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.052136725-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 02/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.053171324-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 03/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.055951015-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.055962597-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.056176344-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.056299368-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 00/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.056546728-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 04/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.057503680-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 01/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.057525070-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 05/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.058571572-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 06/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.058726096-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 02/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.059790512-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 07/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.060205622-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 03/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.060735521-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 08/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.062020116-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 04/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.062583107-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 09/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.064638609-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Connected all rings
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.064651163-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 05/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.064778756-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 10/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.066287177-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 11/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.066307846-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 06/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.067417498-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 12/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.067932188-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 07/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.068559381-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 13/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.069422444-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 08/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.069440949-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 00/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.069827816-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 14/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.071617532-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 00/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.075919539-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 09/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.078571657-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:232 [3] NCCL INFO comm 0x5574631cdc30 rank 3 nranks 4 cudaDev 3 busId bd000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.078770756-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 15/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.078863963-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:229 [2] NCCL INFO comm 0x555e921006e0 rank 2 nranks 4 cudaDev 2 busId b7000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.079238406-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 01/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.081721352-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 16/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.082028827-05:00 msalehjahromi-torchrun-ftn-pwjxr:99:222 [0] NCCL INFO comm 0x55c8df4305a0 rank 0 nranks 4 cudaDev 0 busId 7000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.082950421-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 01/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.084020998-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 17/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.084031228-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 02/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.084387426-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 02/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.084825830-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 10/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.084894521-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 03/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.086040452-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:236 [1] NCCL INFO comm 0x56292a414e60 rank 1 nranks 4 cudaDev 1 busId f000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.086243909-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 04/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.086417079-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 18/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.087134985-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 03/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.087220528-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 05/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.087573290-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 11/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.089336345-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 06/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.089497051-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 19/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.090313766-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 20/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.090447460-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 07/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.091241932-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 21/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.091637495-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 08/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.092083884-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 22/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.092830996-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 09/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.092880531-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 04/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.094926525-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Channel 23/0 : 3[bd000] -> 2[b7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.095553057-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 12/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.096146777-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 10/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.096324165-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 13/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.096957921-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 11/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.097169143-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 05/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.097342092-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 14/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.097465326-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 00/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.098745583-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 00/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.098986902-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 06/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.099152347-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 15/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.099482716-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 01/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.099827482-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 07/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.099887166-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 01/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.100078319-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 16/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.100320400-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 02/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.100744287-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 08/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.100835892-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 02/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.100989123-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 17/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.101236213-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 03/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.101632367-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 09/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.101699836-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 03/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.101939793-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 18/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.102100017-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 04/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.102532470-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 10/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.102580281-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 04/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.102832171-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 19/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.103068100-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 05/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.103432293-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 11/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.103478551-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 05/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.103783571-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 20/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.103969105-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 06/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.104320704-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 12/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.104372683-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 06/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.104626586-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 21/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.104813642-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 12/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.106686386-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 07/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.107050169-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 13/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.107145991-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 07/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.107427096-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 13/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.107492982-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 08/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.107925384-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 14/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.107953648-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 08/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.108153759-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 14/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.108258679-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 09/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.108430997-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 00/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.108768409-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 15/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.108796492-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 09/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.109001693-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 15/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.109123274-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 10/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.109295402-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 01/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.109596645-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 16/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.109665957-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 10/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.109833806-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 16/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.109962511-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 11/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.110149918-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 02/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.110438407-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 17/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.110493282-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 11/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.110673685-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 17/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.110774877-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 12/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.110989596-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 03/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.111257155-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 18/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.111299195-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 12/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.111500068-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 18/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.111609105-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 13/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.111803906-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 04/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.112045576-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 19/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.112165935-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 13/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.112356528-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 19/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.112423255-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 14/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.112676226-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 05/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.114440634-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 20/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.114449732-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 14/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.114459260-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 22/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.114866415-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 20/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.115251277-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 21/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.115311402-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 15/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.115385673-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 15/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.115548683-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 21/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.115684432-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Channel 23/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.116049536-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 22/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.116162522-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 16/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.116246712-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 16/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.116439409-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 22/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.116803642-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Channel 23/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.116817698-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 06/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.117467956-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Channel 23/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.117501440-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 17/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.118971608-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 07/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.118980966-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 18/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.118984502-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 17/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.120215906-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 18/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.121125717-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 08/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.121599790-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 19/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.121649444-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 19/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.122379965-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 09/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.123040241-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 20/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.123094965-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 20/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.123796000-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 10/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.124011069-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 21/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.124453301-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 21/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.125731494-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 11/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.128920884-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 12/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.131733126-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 13/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.132712069-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 22/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.132722168-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 22/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.140116541-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 14/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.150248475-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.150272931-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.150698561-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.151855934-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 15/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.153705865-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 16/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.155751769-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 17/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.161310438-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 18/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.163364467-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 19/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.177286755-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 20/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.177320279-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Channel 23/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.181693532-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 21/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.186318003-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 22/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.186764513-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Channel 23/0 : 2[b7000] -> 1[f000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.191184966-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Channel 23/0 : 1[f000] -> 0[7000] via P2P/IPC/read
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.241375271-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.241396551-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.241869130-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.270412465-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.270419498-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.270726783-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.276971448-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.276981387-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.276990995-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.316884462-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.316937132-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.317209190-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.321662626-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.321709525-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.321941958-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.324789427-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.324812200-05:00 ==== Dataset Statistics ====
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.324819815-05:00 Total samples: 43852
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.324835034-05:00 Training samples: 32922
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.324842137-05:00 Validation samples: 5460
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.327306638-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.327314253-05:00 1-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.327329141-05:00   Train positive: 1242 (3.77%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.327389305-05:00   Val positive: 186 (3.41%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.329736543-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.329745990-05:00 2-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.329748926-05:00   Train positive: 1638 (4.98%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.329751531-05:00   Val positive: 253 (4.63%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.332165826-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.332184241-05:00 3-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.332191725-05:00   Train positive: 1970 (5.98%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.332197917-05:00   Val positive: 312 (5.71%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.334550104-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.334564291-05:00 4-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.334568559-05:00   Train positive: 2226 (6.76%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.334572346-05:00   Val positive: 360 (6.59%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.336951554-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.336961002-05:00 5-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.336964038-05:00   Train positive: 2436 (7.40%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.336966613-05:00   Val positive: 402 (7.36%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.339340481-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.339356251-05:00 6-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.339360549-05:00   Train positive: 2562 (7.78%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.339363815-05:00   Val positive: 426 (7.80%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.339367803-05:00 ============================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.339370248-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.363794953-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.363824360-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.364068795-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.434307385-05:00 Created validation loader with 5153 samples
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.452555365-05:00 2025-05-09 03:38:05,452 - INFO - using MLP layer as FFN
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.453859928-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.453898061-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.454197791-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.455738594-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.455773520-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.455990734-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.461356476-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.461369471-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.461529074-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.481717257-05:00 msalehjahromi-torchrun-ftn-pwjxr:93:233 [0] NCCL INFO comm 0x55a7cd98dc60 rank 0 nranks 4 cudaDev 0 busId 7000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.481765128-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:237 [3] NCCL INFO comm 0x55ed9529d650 rank 3 nranks 4 cudaDev 3 busId bd000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.482064648-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.482079727-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.482254500-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.482857798-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:235 [2] NCCL INFO comm 0x55c813cd4a00 rank 2 nranks 4 cudaDev 2 busId b7000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.503912039-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:239 [1] NCCL INFO comm 0x560531471f20 rank 1 nranks 4 cudaDev 1 busId f000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.508859906-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO Connected all trees
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.508874774-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.509229539-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.514866227-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:238 [3] NCCL INFO comm 0x55ceefa19790 rank 3 nranks 4 cudaDev 3 busId bd000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.522978747-05:00 msalehjahromi-torchrun-ftn-pwjxr:105:226 [0] NCCL INFO comm 0x564735e095a0 rank 0 nranks 4 cudaDev 0 busId 7000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.523213123-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:227 [2] NCCL INFO comm 0x55ca890d9ba0 rank 2 nranks 4 cudaDev 2 busId b7000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.523237570-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:228 [1] NCCL INFO comm 0x55c1639aeda0 rank 1 nranks 4 cudaDev 1 busId f000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.567911755-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:240 [1] NCCL INFO comm 0x559b9f719000 rank 1 nranks 4 cudaDev 1 busId f000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.568868907-05:00 msalehjahromi-torchrun-ftn-pwjxr:87:224 [0] NCCL INFO comm 0x56332c3f0860 rank 0 nranks 4 cudaDev 0 busId 7000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.568971783-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:234 [3] NCCL INFO comm 0x56558e5a5b80 rank 3 nranks 4 cudaDev 3 busId bd000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.568979658-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:230 [2] NCCL INFO comm 0x557e201ecc50 rank 2 nranks 4 cudaDev 2 busId b7000 - Init COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847043838-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847093753-05:00 ==== Dataset Statistics ====
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847098643-05:00 Total samples: 43852
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847101127-05:00 Training samples: 32922
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847103793-05:00 Validation samples: 5460
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847882264-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847927240-05:00 ==== Dataset Statistics ====
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847931397-05:00 Total samples: 43852
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847934714-05:00 Training samples: 32922
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.847937519-05:00 Validation samples: 5460
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.849622807-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.849638396-05:00 1-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.849646853-05:00   Train positive: 1242 (3.77%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.849660098-05:00   Val positive: 186 (3.41%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.850495107-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.850507621-05:00 1-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.850516428-05:00   Train positive: 1242 (3.77%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.850526366-05:00   Val positive: 186 (3.41%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.852143825-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.852151990-05:00 2-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.852155697-05:00   Train positive: 1638 (4.98%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.852159084-05:00   Val positive: 253 (4.63%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.853041864-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.853053666-05:00 2-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.853057203-05:00   Train positive: 1638 (4.98%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.853060359-05:00   Val positive: 253 (4.63%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.854652299-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.854659423-05:00 3-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.854662599-05:00   Train positive: 1970 (5.98%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.854665224-05:00   Val positive: 312 (5.71%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.855593090-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.855601776-05:00 3-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.855604532-05:00   Train positive: 1970 (5.98%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.855607257-05:00   Val positive: 312 (5.71%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.857160052-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.857168328-05:00 4-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.857171985-05:00   Train positive: 2226 (6.76%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.857175001-05:00   Val positive: 360 (6.59%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.858130469-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.858140248-05:00 4-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.858143173-05:00   Train positive: 2226 (6.76%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.858145999-05:00   Val positive: 360 (6.59%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.859640042-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.859646485-05:00 5-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.859649220-05:00   Train positive: 2436 (7.40%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.859652256-05:00   Val positive: 402 (7.36%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.860636890-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.860645897-05:00 5-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.860649353-05:00   Train positive: 2436 (7.40%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.860652188-05:00   Val positive: 402 (7.36%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.862138698-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.862147034-05:00 6-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.862149578-05:00   Train positive: 2562 (7.78%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.862151983-05:00   Val positive: 426 (7.80%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.862154247-05:00 ============================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.862156321-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.863148169-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.863155012-05:00 6-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.863157917-05:00   Train positive: 2562 (7.78%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.863160312-05:00   Val positive: 426 (7.80%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.863163057-05:00 ============================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.863166284-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.870066055-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.870080924-05:00 ==== Dataset Statistics ====
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.870085352-05:00 Total samples: 43852
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.870087717-05:00 Training samples: 32922
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.870090432-05:00 Validation samples: 5460
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.872654752-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.872678497-05:00 1-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.872685972-05:00   Train positive: 1242 (3.77%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.872694027-05:00   Val positive: 186 (3.41%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.875181151-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.875190719-05:00 2-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.875193815-05:00   Train positive: 1638 (4.98%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.875196810-05:00   Val positive: 253 (4.63%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.877672051-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.877682531-05:00 3-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.877685587-05:00   Train positive: 1970 (5.98%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.877688262-05:00   Val positive: 312 (5.71%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.880354026-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.880361871-05:00 4-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.880364926-05:00   Train positive: 2226 (6.76%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.880367411-05:00   Val positive: 360 (6.59%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.882886977-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.882904279-05:00 5-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.882907686-05:00   Train positive: 2436 (7.40%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.882910351-05:00   Val positive: 402 (7.36%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.892599772-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.892617185-05:00 6-year-cancer:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.892620732-05:00   Train positive: 2562 (7.78%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.892624780-05:00   Val positive: 426 (7.80%)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.892627525-05:00 ============================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:05.892630731-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.004521595-05:00 Created validation loader with 5153 samples
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.006710161-05:00 Created validation loader with 5153 samples
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.022133515-05:00 2025-05-09 03:38:06,021 - INFO - using MLP layer as FFN
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.023734852-05:00 2025-05-09 03:38:06,023 - INFO - using MLP layer as FFN
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.028514028-05:00 Created validation loader with 5153 samples
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.046152098-05:00 2025-05-09 03:38:06,045 - INFO - using MLP layer as FFN
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.914338999-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.944721715-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.945257915-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.980984652-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.981025089-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.981034837-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.981041069-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:06.981016593-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.022437636-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.022457374-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.022460350-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.022493362-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.022499043-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.046126191-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.046143033-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.046152361-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.046230139-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.046259966-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.230684990-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.273795038-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.273813483-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.273808964-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.273840965-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.273847257-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.366153558-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.367147690-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.382768429-05:00 Verifying all parameters are on cuda:0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.383346399-05:00 All parameters successfully verified on cuda:0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.384092970-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.384391819-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.388996854-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.391353639-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.393641674-05:00 2025-05-09 03:38:07,393 - INFO - DinoVisionTransformer parameters: 176 (LR: 1.0000000000000002e-06)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.393693182-05:00 2025-05-09 03:38:07,393 - INFO - Aggregator parameters: 26 (LR: 0.0001)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.393977433-05:00 2025-05-09 03:38:07,393 - INFO - Setting unfreeze_strategy to 'all' to avoid DDP synchronization issues
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.396085495-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.397055111-05:00 2025-05-09 03:38:07,396 - INFO - using MLP layer as FFN
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.411253193-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.411290544-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.411302266-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.411306034-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.411346190-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.412369247-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.412377743-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.412403552-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.412437347-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.412440813-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.438864323-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.440490929-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.440636977-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.440646335-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.440711308-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.440770461-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.440861053-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.441259301-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.441265193-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.441296001-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.441340887-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.441345125-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.450255463-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.450269410-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.450296792-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.450324434-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.450328602-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.453683177-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.453692655-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.453713274-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.453748211-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.453754753-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.467610183-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.467627516-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.467643867-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.467684404-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.467690286-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.522461999-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.522518597-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.522590394-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.522475044-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.522594942-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.525567640-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.525585464-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.525606604-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.525660647-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.525675575-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.620556656-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.662485997-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.663792053-05:00 Found cls_token in weights with shape: torch.Size([1, 1, 768])
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.664255816-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.664262679-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.664290532-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.664316571-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.664323254-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.706197370-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.706214192-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.706218911-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.706237476-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.706267303-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.754244996-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:769: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.754288850-05:00   f"storage size: {model_ct.cls_token.storage().size()}")
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.754365265-05:00 Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.754384161-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:760: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.754395102-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.819547371-05:00 Verifying all parameters are on cuda:0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.820255559-05:00 All parameters successfully verified on cuda:0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.833217910-05:00 2025-05-09 03:38:07,833 - INFO - DinoVisionTransformer parameters: 176 (LR: 1.0000000000000002e-06)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.833271712-05:00 2025-05-09 03:38:07,833 - INFO - Aggregator parameters: 26 (LR: 0.0001)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.833552767-05:00 2025-05-09 03:38:07,833 - INFO - Setting unfreeze_strategy to 'all' to avoid DDP synchronization issues
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.836930576-05:00 2025-05-09 03:38:07,836 - INFO - using MLP layer as FFN
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.875544257-05:00 Verifying all parameters are on cuda:0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.876191198-05:00 All parameters successfully verified on cuda:0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.899289349-05:00 2025-05-09 03:38:07,899 - INFO - DinoVisionTransformer parameters: 176 (LR: 1.0000000000000002e-06)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.899362017-05:00 2025-05-09 03:38:07,899 - INFO - Aggregator parameters: 26 (LR: 0.0001)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.899659794-05:00 2025-05-09 03:38:07,899 - INFO - Setting unfreeze_strategy to 'all' to avoid DDP synchronization issues
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.902978480-05:00 2025-05-09 03:38:07,902 - INFO - using MLP layer as FFN
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.926809176-05:00 Verifying all parameters are on cuda:0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.927494800-05:00 All parameters successfully verified on cuda:0
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.937596957-05:00 2025-05-09 03:38:07,937 - INFO - DinoVisionTransformer parameters: 176 (LR: 1.0000000000000002e-06)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.937648145-05:00 2025-05-09 03:38:07,937 - INFO - Aggregator parameters: 26 (LR: 0.0001)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.937966791-05:00 2025-05-09 03:38:07,937 - INFO - Setting unfreeze_strategy to 'all' to avoid DDP synchronization issues
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:07.947786130-05:00 2025-05-09 03:38:07,947 - INFO - using MLP layer as FFN
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.121430167-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.121528454-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.161289690-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.161330758-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.314572633-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.314611447-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.378938035-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.378958134-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.492301222-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.492347500-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506209352-05:00 Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250509_033809.jsonl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506457104-05:00 Will save checkpoints to /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/checkpoints
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506470760-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506474376-05:00 Training Configuration:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506477973-05:00 Max chunks per sample: 18
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506485878-05:00 Learning rate: 0.0001
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506488123-05:00 Number of tasks: 6
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506490597-05:00 Validation frequency: 400 steps
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506492722-05:00 Number of epochs: 50
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506499695-05:00 Warmup steps: 1000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506501979-05:00 World size: 4
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506504063-05:00 Device: cuda
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506505967-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.506646203-05:00 Epoch 1: All parameters already unfrozen (strategy: all)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.521747284-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.521757383-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.527587069-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.527602167-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.549045850-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.549057171-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.569091411-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.569117080-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.602630695-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.602647286-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.620273523-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.620318138-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.620354196-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.620401226-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.659995362-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.660012776-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.670731175-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.670827859-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.672799863-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.672810523-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673143566-05:00 Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250509_033809.jsonl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673427767-05:00 Will save checkpoints to /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/checkpoints
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673452274-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673456091-05:00 Training Configuration:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673459538-05:00 Max chunks per sample: 18
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673462053-05:00 Learning rate: 0.0001
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673464688-05:00 Number of tasks: 6
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673467363-05:00 Validation frequency: 400 steps
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673469888-05:00 Number of epochs: 50
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673472423-05:00 Warmup steps: 1000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673474887-05:00 World size: 4
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673482111-05:00 Device: cuda
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673484445-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.673592601-05:00 Epoch 1: All parameters already unfrozen (strategy: all)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.680214344-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.680229292-05:00   if hasattr(m, n) and getattr(m, n).storage().size() == 0:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998489506-05:00 Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250509_033809.jsonl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998724894-05:00 Will save checkpoints to /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/checkpoints
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998735664-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998740143-05:00 Training Configuration:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998750071-05:00 Max chunks per sample: 18
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998753819-05:00 Learning rate: 0.0001
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998756965-05:00 Number of tasks: 6
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998759840-05:00 Validation frequency: 400 steps
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998762505-05:00 Number of epochs: 50
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998769649-05:00 Warmup steps: 1000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998772454-05:00 World size: 4
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998901279-05:00 Device: cuda
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.998912731-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:09.999043059-05:00 Epoch 1: All parameters already unfrozen (strategy: all)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029102349-05:00 Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250509_033810.jsonl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029325964-05:00 Will save checkpoints to /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/checkpoints
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029332868-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029336735-05:00 Training Configuration:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029340091-05:00 Max chunks per sample: 18
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029342977-05:00 Learning rate: 0.0001
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029347004-05:00 Number of tasks: 6
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029350671-05:00 Validation frequency: 400 steps
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029394815-05:00 Number of epochs: 50
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029398232-05:00 Warmup steps: 1000
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029401058-05:00 World size: 4
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029403743-05:00 Device: cuda
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029406127-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:10.029626316-05:00 Epoch 1: All parameters already unfrozen (strategy: all)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:24.114803214-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:24.114924485-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:24.114932280-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:24.114938011-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:24.117350743-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:24.117378295-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:24.117381812-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:24.117389366-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:25.129852699-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:25.130083839-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:25.130126410-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:25.130164292-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:25.132128681-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:25.132205096-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:25.132284638-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:25.132289156-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.497212977-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.497350859-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.497360888-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.497366910-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.498457646-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.498482954-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.498501008-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.498506158-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.853394933-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.853520102-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.853529109-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.853533738-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.854734603-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.854988587-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.855001902-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:27.855010879-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:28.154027407-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:28.154138599-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:28.154143799-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:28.154146935-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:28.156030610-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:28.156140780-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:28.156149897-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:28.156157020-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:30.432552601-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:30.432658171-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:30.432680925-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:30.432685023-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:30.434592894-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:30.434673146-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:30.434682635-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:30.434685701-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:34.482027383-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:34.482082919-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:34.482090985-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:34.482093680-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:34.483347436-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:34.483391550-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:34.483552536-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:34.483602401-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.066095576-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.066123459-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.066126665-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.066129461-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.067655054-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.067749373-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.067761977-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.067768269-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.257632255-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.257667612-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.257679585-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.257682731-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.259023303-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.259031789-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.259038381-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:37.259052247-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:38.324078299-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:38.324132232-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:38.324137913-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:38.324142792-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:38.325567704-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:38.325623440-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:38.325632778-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:38.325636765-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.355542265-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.355583805-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.355586890-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.355591960-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.356819597-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.356828483-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.356833994-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.356845736-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.934447797-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.934490879-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.934509424-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.934514494-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.935603998-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.935615800-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.935622723-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.935625769-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.947412051-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.947419996-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.947455444-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.947473628-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.948618437-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.948625430-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.948643344-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:39.948652252-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.164778660-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.164797767-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.164808006-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.164840999-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.165953897-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.165959848-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.165978464-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.165982151-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.275356806-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.275379790-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.275387835-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.275390741-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.276699121-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.276709140-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.276715673-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.276720872-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.667477293-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.667526837-05:00   and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.667532528-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.667645543-05:00   and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.669137322-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.669149716-05:00   storage_data_ptr = tensors[0].storage().data_ptr()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.669152682-05:00 /rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:38:40.669155798-05:00   if x.storage().data_ptr() != storage_data_ptr:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:48.443724913-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:48.443792061-05:00 Step 200 (Epoch 1):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:48.443796480-05:00 Loss: 0.0294
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:48.443799094-05:00 Learning rate: 0.000013
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:48.443802341-05:00 Accuracy: Task 0: 0.97 | Task 1: 0.96 | Task 2: 0.95 | Task 3: 0.82 | Task 4: 0.81 | Task 5: 0.74
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:48.444660905-05:00 2025-05-09 03:42:48,443 - INFO - iteration: 200 | epoch: 0 | loss: 0.0294 | lr: 0.0000 | type: training | acc_task0: 0.9750 | acc_task1: 0.9592 | acc_task2: 0.9536 | acc_task3: 0.8177 | acc_task4: 0.8138 | acc_task5: 0.7442
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:50.377706761-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:50.377756616-05:00 Step 200 (Epoch 1):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:50.377759922-05:00 Loss: 0.0299
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:50.377762477-05:00 Learning rate: 0.000013
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:50.377767045-05:00 Accuracy: Task 0: 0.94 | Task 1: 0.73 | Task 2: 0.95 | Task 3: 0.91 | Task 4: 0.92 | Task 5: 0.84
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:50.379604493-05:00 2025-05-09 03:42:50,377 - INFO - iteration: 200 | epoch: 0 | loss: 0.0299 | lr: 0.0000 | type: training | acc_task0: 0.9400 | acc_task1: 0.7296 | acc_task2: 0.9536 | acc_task3: 0.9115 | acc_task4: 0.9241 | acc_task5: 0.8372
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.189775139-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.189837859-05:00 Step 200 (Epoch 1):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.189841305-05:00 Loss: 0.0317
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.189843590-05:00 Learning rate: 0.000013
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.189846305-05:00 Accuracy: Task 0: 0.86 | Task 1: 0.96 | Task 2: 0.84 | Task 3: 0.94 | Task 4: 0.92 | Task 5: 0.52
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.191249676-05:00 2025-05-09 03:42:52,189 - INFO - iteration: 200 | epoch: 0 | loss: 0.0317 | lr: 0.0000 | type: training | acc_task0: 0.8600 | acc_task1: 0.9592 | acc_task2: 0.8351 | acc_task3: 0.9427 | acc_task4: 0.9241 | acc_task5: 0.5233
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.949803871-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.949873614-05:00 Step 200 (Epoch 1):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.949878443-05:00 Loss: 0.0299
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.949881269-05:00 Learning rate: 0.000013
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.949931645-05:00 Accuracy: Task 0: 0.97 | Task 1: 0.88 | Task 2: 0.79 | Task 3: 0.88 | Task 4: 0.70 | Task 5: 0.64
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:42:52.954173668-05:00 2025-05-09 03:42:52,949 - INFO - iteration: 200 | epoch: 0 | loss: 0.0299 | lr: 0.0000 | type: training | acc_task0: 0.9750 | acc_task1: 0.8776 | acc_task2: 0.7887 | acc_task3: 0.8802 | acc_task4: 0.7034 | acc_task5: 0.6395
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.093522618-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.093578845-05:00 Step 400 (Epoch 1):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.093582943-05:00 Loss: 0.0102
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.093585187-05:00 Learning rate: 0.000037
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.093638358-05:00 Accuracy: Task 0: 0.97 | Task 1: 0.96 | Task 2: 0.96 | Task 3: 0.89 | Task 4: 0.88 | Task 5: 0.81
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.094597934-05:00 2025-05-09 03:46:56,093 - INFO - iteration: 400 | epoch: 0 | loss: 0.0102 | lr: 0.0000 | type: training | acc_task0: 0.9750 | acc_task1: 0.9641 | acc_task2: 0.9583 | acc_task3: 0.8859 | acc_task4: 0.8767 | acc_task5: 0.8133
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.099993433-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.100044420-05:00 Reached validation point at step 400 (in epoch 1)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.285839021-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:46:56.285923091-05:00 === Starting validation at step 400 ===
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.334288316-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.334337590-05:00 Step 400 (Epoch 1):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.334342439-05:00 Loss: 0.0146
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.334346337-05:00 Learning rate: 0.000037
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.334351908-05:00 Accuracy: Task 0: 0.96 | Task 1: 0.85 | Task 2: 0.96 | Task 3: 0.93 | Task 4: 0.93 | Task 5: 0.86
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.334620098-05:00 2025-05-09 03:47:01,334 - INFO - iteration: 400 | epoch: 0 | loss: 0.0146 | lr: 0.0000 | type: training | acc_task0: 0.9575 | acc_task1: 0.8487 | acc_task2: 0.9583 | acc_task3: 0.9337 | acc_task4: 0.9315 | acc_task5: 0.8614
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.339647386-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.339689937-05:00 Reached validation point at step 400 (in epoch 1)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.550390299-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:01.550426528-05:00 === Starting validation at step 400 ===
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.058389446-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.058433740-05:00 Step 400 (Epoch 1):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.058437618-05:00 Loss: 0.0118
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.058440393-05:00 Learning rate: 0.000037
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.058452155-05:00 Accuracy: Task 0: 0.92 | Task 1: 0.96 | Task 2: 0.90 | Task 3: 0.95 | Task 4: 0.93 | Task 5: 0.70
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.059930379-05:00 2025-05-09 03:47:04,058 - INFO - iteration: 400 | epoch: 0 | loss: 0.0118 | lr: 0.0000 | type: training | acc_task0: 0.9175 | acc_task1: 0.9641 | acc_task2: 0.8984 | acc_task3: 0.9496 | acc_task4: 0.9315 | acc_task5: 0.6988
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.062727632-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.062750786-05:00 Reached validation point at step 400 (in epoch 1)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.309761406-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:04.309795190-05:00 === Starting validation at step 400 ===
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.358779031-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.358807855-05:00 Step 400 (Epoch 1):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.358811011-05:00 Loss: 0.0150
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.358813616-05:00 Learning rate: 0.000037
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.358818305-05:00 Accuracy: Task 0: 0.97 | Task 1: 0.92 | Task 2: 0.88 | Task 3: 0.92 | Task 4: 0.82 | Task 5: 0.76
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.360112939-05:00 2025-05-09 03:47:05,358 - INFO - iteration: 400 | epoch: 0 | loss: 0.0150 | lr: 0.0000 | type: training | acc_task0: 0.9750 | acc_task1: 0.9231 | acc_task2: 0.8750 | acc_task3: 0.9178 | acc_task4: 0.8219 | acc_task5: 0.7590
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.363499054-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.363522078-05:00 Reached validation point at step 400 (in epoch 1)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.587544916-05:00 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T22:47:05.587728044-05:00 === Starting validation at step 400 ===
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:05.649833599-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804025 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:05.766693475-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809422 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:05.778895747-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801421 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.042000922-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801705 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.049048183-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809705 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.064423995-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809720 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.114338908-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804488 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.115802243-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1800435 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.131343860-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804520 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.140954741-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1800461 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.141207031-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801786 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:06.160541757-05:00 [E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1800444 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.354027770-05:00 msalehjahromi-torchrun-ftn-pwjxr:90:194 [0] NCCL INFO comm 0x56558e5a5b80 rank 3 nranks 4 cudaDev 3 busId bd000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.361908057-05:00 msalehjahromi-torchrun-ftn-pwjxr:88:144 [0] NCCL INFO comm 0x559b9f719000 rank 1 nranks 4 cudaDev 1 busId f000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.362246581-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.362270968-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.362358244-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.362475497-05:00   what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804025 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.364276464-05:00 msalehjahromi-torchrun-ftn-pwjxr:89:182 [0] NCCL INFO comm 0x557e201ecc50 rank 2 nranks 4 cudaDev 2 busId b7000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.364560846-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.364623625-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.364715610-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.364723264-05:00   what():  [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804488 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.746535372-05:00 Error in training step at step 402: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804520 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750252797-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750307662-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750312791-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750317300-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 224, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750320386-05:00     self.scaler.scale(loss).backward()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750323231-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750328150-05:00     torch.autograd.backward(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750331096-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750334192-05:00     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.750340474-05:00 RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804520 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.779595008-05:00 Error in training step at step 403: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.779616379-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.779620667-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.779628532-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781603991-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781651903-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781656982-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781659878-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781665899-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781669727-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781728659-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781730913-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781733418-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781740381-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781743527-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781746002-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.781749068-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804529902-05:00 Error in training step at step 404: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804547426-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804550281-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804554279-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804793434-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804810767-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804815846-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804820976-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804825665-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804830164-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804885699-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804889687-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804893153-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804897512-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804901489-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804905136-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.804909044-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826367271-05:00 Error in training step at step 405: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826384484-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826387630-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826391968-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826544798-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826625732-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826629580-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826632686-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826635121-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826669335-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826672000-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826674355-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826676820-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826765388-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826804372-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826807509-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.826811356-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846187380-05:00 Error in training step at step 406: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846227747-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846231244-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846235411-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846370990-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846376440-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846379837-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846382191-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846384726-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846436634-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846440672-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846443939-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846446173-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846450631-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846453567-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846455771-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.846458586-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.866811110-05:00 Error in training step at step 407: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.866825166-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.866828092-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.866831278-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867008405-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867132621-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867139154-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867143181-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867303637-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867307174-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867309749-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867311923-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867314077-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867320279-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867323815-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867326100-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.867329035-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887096815-05:00 Error in training step at step 408: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887140037-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887143354-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887146730-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887232123-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887257381-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887262150-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887266218-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887313007-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887316553-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887319569-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887322525-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887325440-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887329188-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887333165-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887336441-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.887340439-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.907761963-05:00 Error in training step at step 409: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.907808812-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.907812800-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.907816817-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.907920384-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.907972494-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.907976651-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.907996990-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908000336-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908003282-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908006188-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908009043-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908014714-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908018291-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908021958-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908024943-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.908028681-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.934928634-05:00 Error in training step at step 410: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.934950105-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.934955034-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.934959192-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935098367-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935113175-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935126411-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935166437-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935171326-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935175164-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935178920-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935182097-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935185753-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935197516-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935203147-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935206683-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.935210941-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.955969617-05:00 Error in training step at step 411: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.955991268-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.955994615-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.955997130-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956059388-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956081310-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956104103-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956107449-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956110635-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956113741-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956116777-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956120004-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956123009-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956138829-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956142446-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956145522-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.956148678-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978012797-05:00 Error in training step at step 412: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978040460-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978044357-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978046982-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978096136-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978115693-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978144809-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978148556-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978151882-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978155018-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978158034-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978161070-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978164536-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978171640-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978175086-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978178282-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:29.978181449-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002639625-05:00 Error in training step at step 413: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002655555-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002660865-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002666616-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002798207-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002829657-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002855716-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002859033-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002862359-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002865906-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002869212-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002872268-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002875414-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002879041-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002885273-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002888679-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.002891965-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025137932-05:00 Error in training step at step 414: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025151598-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025157539-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025162368-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025354804-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025382707-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025386805-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025389921-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025393067-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025396063-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025399069-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.025402074-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.027055611-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.027060631-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.027064578-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.027068766-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.027072614-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052764868-05:00 Error in training step at step 415: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052779717-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052783744-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052787151-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052874958-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052891710-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052896499-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052899965-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052903091-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052906157-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052909484-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052912630-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052915745-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052922148-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052926035-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052929201-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.052932988-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073250435-05:00 Error in training step at step 416: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073263329-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073268409-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073273508-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073385101-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073486193-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073491323-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073494569-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073497525-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073500350-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073504037-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073506312-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073508586-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073516722-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073519477-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073521821-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.073524025-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100689113-05:00 Error in training step at step 417: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100720122-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100726104-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100731864-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100779856-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100795646-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100800505-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100803801-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100807138-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100810213-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100813290-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100816436-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100819732-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100833678-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100838808-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100841904-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.100848076-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121476483-05:00 Error in training step at step 418: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121498616-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121502092-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121504908-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121589609-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121605058-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121608455-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121610829-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121614306-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121616690-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121619085-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121621560-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121623904-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121630997-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121633763-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121637109-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.121639474-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142135740-05:00 Error in training step at step 419: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142148424-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142152302-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142180957-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142173873-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142205403-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142212617-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142215632-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142217867-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142221003-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142223157-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142229990-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142232575-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142234839-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142237424-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142239598-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.142241933-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.162936707-05:00 Error in training step at step 420: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.162975100-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.162980269-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163042899-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163028632-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163057256-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163061935-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163065362-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163069038-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163080380-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163083907-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163087404-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163090810-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163094677-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163099006-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163102442-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.163106941-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183454264-05:00 Error in training step at step 421: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183474712-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183478029-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183537452-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183552320-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183572619-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183576326-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183578620-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183580885-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183583058-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183585153-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183587206-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183589341-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183592737-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183595843-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183597947-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.183601003-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.354438885-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.354491565-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.354501935-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.354505632-05:00   what():  [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804520 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.389358470-05:00 msalehjahromi-torchrun-ftn-pwjxr:103:149 [0] NCCL INFO comm 0x555e921006e0 rank 2 nranks 4 cudaDev 2 busId b7000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.389638122-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.389747140-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.389767008-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.389773861-05:00   what():  [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801421 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.410335863-05:00 msalehjahromi-torchrun-ftn-pwjxr:104:164 [0] NCCL INFO comm 0x5574631cdc30 rank 3 nranks 4 cudaDev 3 busId bd000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.410748107-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.410789095-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.410800176-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.410804034-05:00   what():  [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801705 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.418119355-05:00 msalehjahromi-torchrun-ftn-pwjxr:101:188 [0] NCCL INFO comm 0x56292a414e60 rank 1 nranks 4 cudaDev 1 busId f000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.418418555-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.418425528-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.418526290-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.418538303-05:00   what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801786 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.467544486-05:00 msalehjahromi-torchrun-ftn-pwjxr:98:185 [0] NCCL INFO comm 0x55ed9529d650 rank 3 nranks 4 cudaDev 3 busId bd000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.467879374-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.467917797-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.467930291-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.467937324-05:00   what():  [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1800461 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.497104953-05:00 msalehjahromi-torchrun-ftn-pwjxr:108:173 [0] NCCL INFO comm 0x55ceefa19790 rank 3 nranks 4 cudaDev 3 busId bd000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.516959688-05:00 msalehjahromi-torchrun-ftn-pwjxr:97:203 [0] NCCL INFO comm 0x55c813cd4a00 rank 2 nranks 4 cudaDev 2 busId b7000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.517383184-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.517417690-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.517482483-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.517495137-05:00   what():  [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1800444 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.539977213-05:00 msalehjahromi-torchrun-ftn-pwjxr:107:155 [0] NCCL INFO comm 0x55ca890d9ba0 rank 2 nranks 4 cudaDev 2 busId b7000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.540771064-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.540832672-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.541085022-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.541145938-05:00   what():  [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809422 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.585704106-05:00 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 87 closing signal SIGTERM
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.586722775-05:00 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 90 closing signal SIGTERM
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.616319160-05:00 msalehjahromi-torchrun-ftn-pwjxr:95:179 [0] NCCL INFO comm 0x560531471f20 rank 1 nranks 4 cudaDev 1 busId f000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.616913190-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.617007861-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.617055972-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.617096920-05:00   what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1800435 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.643169348-05:00 msalehjahromi-torchrun-ftn-pwjxr:106:168 [0] NCCL INFO comm 0x55c1639aeda0 rank 1 nranks 4 cudaDev 1 busId f000 - Abort COMPLETE
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.826696239-05:00 Error in training step at step 402: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809720 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828181426-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828369373-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828402927-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828407265-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 224, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828411924-05:00     self.scaler.scale(loss).backward()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828417685-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828422414-05:00     torch.autograd.backward(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828425330-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828427895-05:00     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.828432113-05:00 RuntimeError: NCCL communicator was aborted on rank 3.  Original reason for failure was: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809720 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.852451274-05:00 Error in training step at step 403: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.852519613-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.852523711-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.852527278-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861127976-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861149347-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861152943-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861156781-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861159857-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861167181-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861170136-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861172791-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861175266-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861178392-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861181167-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861183602-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.861186147-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.875911301-05:00 Error in training step at step 404: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.875968359-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.875971916-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.875975042-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.875986404-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.875999930-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876004478-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876008656-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876012514-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876016311-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876019597-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876023134-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876030508-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876037541-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876042300-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876045927-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.876051418-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884416357-05:00 Error in training step at step 405: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884482443-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884487092-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884490738-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884547066-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884556323-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884559840-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884564229-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884567575-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884570460-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884573226-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884576522-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884579127-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884582503-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884586270-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884589727-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.884592753-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891650875-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891806942-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891816801-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891681533-05:00 Error in training step at step 406: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891887295-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891893016-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891896352-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891821710-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891916000-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891919747-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891923123-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891926219-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891929415-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891933814-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891938863-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.891942230-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.892052509-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907378897-05:00 Error in training step at step 407: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907437639-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907440955-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907443931-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907460643-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907464851-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907468858-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907471593-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907474378-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907476733-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907480410-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907485760-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907488636-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907496551-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907502252-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907505829-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.907582695-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.918958414-05:00 Error in training step at step 408: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919025632-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919029409-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919033226-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919059867-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919081608-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919087099-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919090365-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919094052-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919097629-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919100975-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919105103-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919108369-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919112347-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919116986-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919123728-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.919252323-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931472289-05:00 Error in training step at step 409: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931502216-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931506704-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931511313-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931513457-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931530900-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931547322-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931551510-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931556058-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931559866-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931563473-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931566829-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931570095-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931573862-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931578401-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931582138-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.931670326-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940819368-05:00 Error in training step at step 410: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940869974-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940872699-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940889852-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940894611-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940874954-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940907235-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940898989-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940916152-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940923105-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940927554-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940943584-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940990453-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.940996204-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.941005863-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.941009830-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.941094000-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945734642-05:00 Error in training step at step 411: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945789126-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945793033-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945796710-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945832839-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945854340-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945868927-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945872825-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945877323-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945880800-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945884076-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945887302-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945890729-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945898754-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945903333-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945906679-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.945923672-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.957374183-05:00 Error in training step at step 412: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.959083516-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.959088315-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.959091762-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.959182835-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.959197693-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.959202482-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.959206861-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.959211099-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.960887439-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.960893180-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.960896577-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.960900494-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.960911315-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.960915322-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.961007057-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.961010153-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.985634225-05:00 Error in training step at step 413: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.987255981-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.987260790-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.987264417-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989314439-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989323897-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989327063-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989330349-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989333044-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989335398-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989338073-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989340378-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989342993-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989345928-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989349205-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989451560-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.989455136-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.992316792-05:00 Error in training step at step 402: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809705 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993667162-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993723840-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993728539-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993732346-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 224, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993736384-05:00     self.scaler.scale(loss).backward()
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993741804-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993745091-05:00     torch.autograd.backward(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993748838-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993752274-05:00     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.993758346-05:00 RuntimeError: NCCL communicator was aborted on rank 1.  Original reason for failure was: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809705 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996205733-05:00 Error in training step at step 414: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996223297-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996227384-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996232164-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996280345-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996358063-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996362612-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996365347-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996367882-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996370727-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996373062-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996375566-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996377921-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996384834-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996387800-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996390204-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:30.996392979-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015522486-05:00 Error in training step at step 415: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015566449-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015570507-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015573754-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015615623-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015747855-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015753977-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015756511-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015759637-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015762242-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015764637-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015767182-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015769877-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015773373-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015776249-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015778894-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.015781449-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.028915664-05:00 Error in training step at step 416: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.028986139-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.028992210-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.028998702-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029004253-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029116807-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029122538-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029126035-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029130343-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029133669-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029137276-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029140913-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029144690-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029149339-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029153327-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029156833-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.029160340-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.030512363-05:00 Error in training step at step 403: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.030558270-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.030561246-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.030565504-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031943527-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031959507-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031964406-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031968724-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031972923-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031976840-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031980577-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031983292-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031985897-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031988813-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031992229-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031995035-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.031997810-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.039887565-05:00 Error in training step at step 404: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.039935336-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.039943231-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.039947048-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040096172-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040171044-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040201402-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040205590-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040209778-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040213846-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040217563-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040220969-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040224606-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040248662-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040254203-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040257749-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.040262288-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041378312-05:00 Error in training step at step 417: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041391357-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041396697-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041485937-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041473132-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041520422-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041525392-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041528027-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041530812-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041533678-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041536072-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041538557-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041540871-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041548225-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041551762-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041554367-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.041557583-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048149809-05:00 Error in training step at step 418: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048173674-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048188963-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048193512-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048197339-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048201627-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048207088-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048176509-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048253887-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048259357-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048233718-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048267833-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048271350-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048286449-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048289274-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048291739-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048295336-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048409793-05:00 Error in training step at step 405: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048418841-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048423459-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048437005-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048727458-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048746174-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048749169-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048751774-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048754309-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048756794-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048759559-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048762505-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048764960-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048774447-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048778014-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048780539-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.048783174-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055031655-05:00 Error in training step at step 419: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055063266-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055092531-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055101218-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055105787-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055110265-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055173205-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055067374-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055190017-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055193854-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055179617-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055203522-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055207240-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055224803-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055229803-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055233880-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055239351-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055928763-05:00 Error in training step at step 406: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055946737-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.055950324-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056029174-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056170904-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056195971-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056213745-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056218173-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056222672-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056226489-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056230377-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056242540-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056246648-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056250595-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056255965-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056262277-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.056267527-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062430426-05:00 Error in training step at step 420: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062451997-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062457047-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062494237-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062466605-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062508495-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062513394-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062525457-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062530236-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062533612-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062536899-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062540836-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062546176-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062549883-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062554151-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062558009-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.062561826-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.071885069-05:00 Error in training step at step 407: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.071905027-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.071918383-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.071958890-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072113343-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072142158-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072146958-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072151356-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072155394-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072159602-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072163379-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072167497-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072171394-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072197053-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072209236-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072213584-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.072217532-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074415885-05:00 Error in training step at step 421: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074430864-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074454028-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074460510-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074511147-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074550281-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074554149-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074556704-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074559369-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074561643-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074565650-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074569508-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074573485-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074586881-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074590688-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074593373-05:00 Parameter indices which did not receive grad for rank 3: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.074597000-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082042079-05:00 Error in training step at step 408: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082092966-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082179220-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082183969-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082222682-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082249974-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082255445-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082258982-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082262598-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082266436-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082270664-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082274201-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082277587-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082303827-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082311752-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082314187-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.082318074-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101472387-05:00 Error in training step at step 409: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101505780-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101566706-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101575232-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101743483-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101775363-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101779781-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101791343-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101798808-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101802244-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101805120-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101807705-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101810159-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101813265-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101816502-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101818946-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.101822342-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.117782246-05:00 Error in training step at step 410: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.117799358-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.117872367-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.117877247-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118222193-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118275765-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118279773-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118282969-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118286455-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118289040-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118291625-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118306544-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118309269-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118312345-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118316392-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118319609-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.118323837-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129260139-05:00 Error in training step at step 411: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129287562-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129364468-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129370019-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129589666-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129623000-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129627438-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129641305-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129645242-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129648328-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129651644-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129657746-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129661884-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129666062-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129670871-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129674638-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.129678636-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139582043-05:00 Error in training step at step 412: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139707232-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139711480-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139715096-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139950675-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139968559-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139973007-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139976404-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139979680-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139983598-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139987134-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139990400-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.139993657-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.140082085-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.140091884-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.140094298-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.140097725-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148629552-05:00 Error in training step at step 413: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148785328-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148791019-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148794345-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148871302-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148924503-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148928651-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148931156-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148933600-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148936817-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148939101-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148941255-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148943559-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148951945-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148955402-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148957756-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.148961003-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154049086-05:00 Error in training step at step 414: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154162882-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154166980-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154169965-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154247573-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154288271-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154292088-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154295414-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154297829-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154299993-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154302237-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154304371-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154306986-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154310413-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154313659-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154316074-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.154319189-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161583113-05:00 Error in training step at step 415: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161693874-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161697642-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161700316-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161791871-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161831927-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161835885-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161839031-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161841535-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161856774-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161859089-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161861523-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161863938-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161867635-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161871332-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161874288-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.161877424-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168218531-05:00 Error in training step at step 416: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168318822-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168323251-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168327709-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168398173-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168457736-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168462876-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168465461-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168468277-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168470611-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168472936-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168475410-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168477875-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168486441-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168491320-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168495278-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.168507622-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172121850-05:00 Error in training step at step 417: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172195079-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172197564-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172200670-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172285892-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172309587-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172313074-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172315438-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172318254-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172320558-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172322712-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172324816-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172326940-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172332170-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172336318-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172338552-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.172341107-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.176929449-05:00 Error in training step at step 418: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177023338-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177031423-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177033938-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177048325-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177056180-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177059737-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177061981-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177064115-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177066270-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177068443-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177070688-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177073894-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177155469-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177263846-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177281619-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.177295816-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.182858062-05:00 Error in training step at step 419: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.182925220-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.182929397-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.182933936-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.182978290-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183042423-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183147382-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183181788-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183213558-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183242814-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183264235-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183283582-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183301986-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183366319-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183405864-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183426864-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.183453715-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190127937-05:00 Error in training step at step 420: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190203641-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190211967-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190268174-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190271621-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190286469-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190289725-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190292540-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190295105-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190297430-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190274366-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190301517-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190329581-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190336143-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190341133-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190344479-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.190348537-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198629526-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198653883-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 392, in fit
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198658401-05:00     step_metrics = self._train_step(chunks, labels, mask, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198662299-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py", line 209, in _train_step
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198666286-05:00     logits = self.model(chunks, spacing)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198669653-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198673089-05:00     return forward_call(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198677968-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1139, in forward
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198687987-05:00     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198648222-05:00 Error in training step at step 421: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198697937-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198701684-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198706773-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198730378-05:00 RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss. 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198783099-05:00 If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198786986-05:00 Parameter indices which did not receive grad for rank 1: 0 1 2 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ...
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.198791254-05:00  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.497608535-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.497668058-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.497680481-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.497685120-05:00   what():  [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809720 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.643620190-05:00 [E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.643674172-05:00 [E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.643686065-05:00 terminate called after throwing an instance of 'std::runtime_error'
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.643719128-05:00   what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=6410, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1809705 milliseconds before timing out.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.932242487-05:00 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 88) of binary: /usr/bin/python3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.935978738-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.936023984-05:00   File "/usr/local/bin/torchrun", line 8, in <module>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.936141798-05:00     sys.exit(main())
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.936158881-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.937572531-05:00     return f(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.937653145-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.937924000-05:00     run(args)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.937933428-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.938528270-05:00     elastic_launch(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.938540122-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.938924785-05:00     return launch_agent(self._config, self._entrypoint, list(args))
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939007302-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939260003-05:00     raise ChildFailedError(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939278057-05:00 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939282255-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939285922-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py FAILED
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939289409-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939292995-05:00 Failures:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939296031-05:00 [1]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939299157-05:00   time      : 2025-05-09_04:17:30
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939302313-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939304968-05:00   rank      : 2 (local_rank: 2)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939307112-05:00   exitcode  : -6 (pid: 89)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939378348-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939380933-05:00   traceback : Signal 6 (SIGABRT) received by PID 89
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939383438-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939385662-05:00 Root Cause (first observed failure):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939387906-05:00 [0]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939389990-05:00   time      : 2025-05-09_04:17:30
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939392204-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939394609-05:00   rank      : 1 (local_rank: 1)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939396713-05:00   exitcode  : -6 (pid: 88)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939398837-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939401141-05:00   traceback : Signal 6 (SIGABRT) received by PID 88
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:31.939403466-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:32.501862673-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:32.501924170-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py", line 267, in <module>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:32.503269310-05:00     main(args) 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:32.503376684-05:00   File "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py", line 217, in main
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:32.503920699-05:00     subprocess.run(torchrun_command, check=True)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:32.503942902-05:00   File "/usr/lib/python3.10/subprocess.py", line 526, in run
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:32.505089213-05:00     raise CalledProcessError(retcode, process.args,
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:32.505294143-05:00 subprocess.CalledProcessError: Command '['torchrun', '--nproc_per_node=4', '--master_port=33107', '/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py', '--csv', '/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv', '--accum-steps', '100', '--num-workers', '10', '--epochs', '50', '--lr', '0.0001', '--weight-decay', '0.0001', '--optimizer', 'adamw', '--num-attn-heads', '3', '--num-layers', '2', '--dropout', '0.3', '--unfreeze-strategy', 'all', '--output', '/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu', '--print-every', '200', '--val-every', '400', '--metrics-dir', '/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu']' returned non-zero exit status 1.
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.558531709-05:00 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 99 closing signal SIGTERM
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.570808232-05:00 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 93 closing signal SIGTERM
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.589295014-05:00 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 72 closing signal SIGTERM
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.589331013-05:00 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 73 closing signal SIGTERM
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.589377762-05:00 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 74 closing signal SIGTERM
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.609657215-05:00 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 105 closing signal SIGTERM
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.653891287-05:00 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 75) of binary: /usr/bin/python3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.657194223-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.657217798-05:00   File "/usr/local/bin/torchrun", line 8, in <module>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.657305795-05:00     sys.exit(main())
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.657313570-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.657926847-05:00     return f(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.657976271-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.658648060-05:00     run(args)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.658657157-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659071856-05:00     elastic_launch(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659081013-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659362099-05:00     return launch_agent(self._config, self._entrypoint, list(args))
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659372498-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659586395-05:00     raise ChildFailedError(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659592136-05:00 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659596064-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659600542-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py FAILED
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659604199-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659607596-05:00 Failures:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659610381-05:00   <NO_OTHER_FAILURES>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659612986-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659615421-05:00 Root Cause (first observed failure):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659618376-05:00 [0]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659620831-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659623105-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659625380-05:00   rank      : 3 (local_rank: 3)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659628035-05:00   exitcode  : 1 (pid: 75)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659630289-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659632563-05:00   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.659635889-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.983748389-05:00 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 101) of binary: /usr/bin/python3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.986477543-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.986485859-05:00   File "/usr/local/bin/torchrun", line 8, in <module>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.986493894-05:00     sys.exit(main())
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.986497331-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.987043700-05:00     return f(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.987059460-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.987396422-05:00     run(args)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.987422591-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.987807945-05:00     elastic_launch(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.987819196-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988021621-05:00     return launch_agent(self._config, self._entrypoint, list(args))
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988040688-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988175885-05:00     raise ChildFailedError(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988229397-05:00 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988232082-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988234286-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py FAILED
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988236440-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988238674-05:00 Failures:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988240809-05:00 [1]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988242922-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988245017-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988247070-05:00   rank      : 2 (local_rank: 2)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988249174-05:00   exitcode  : -6 (pid: 103)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988251509-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988253563-05:00   traceback : Signal 6 (SIGABRT) received by PID 103
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988255637-05:00 [2]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988257701-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988259734-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988261809-05:00   rank      : 3 (local_rank: 3)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988263842-05:00   exitcode  : -6 (pid: 104)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988265876-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988267890-05:00   traceback : Signal 6 (SIGABRT) received by PID 104
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988269934-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988271998-05:00 Root Cause (first observed failure):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988274012-05:00 [0]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988276056-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988278100-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988280224-05:00   rank      : 1 (local_rank: 1)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988282568-05:00   exitcode  : -6 (pid: 101)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988284832-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988286856-05:00   traceback : Signal 6 (SIGABRT) received by PID 101
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:35.988289912-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.026768370-05:00 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 106) of binary: /usr/bin/python3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.029901132-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.029917644-05:00   File "/usr/local/bin/torchrun", line 8, in <module>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.029971646-05:00     sys.exit(main())
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.029987577-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.030461989-05:00     return f(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.030470616-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.030981117-05:00     run(args)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.030996426-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031294433-05:00     elastic_launch(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031307578-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031468104-05:00     return launch_agent(self._config, self._entrypoint, list(args))
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031480538-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031618470-05:00     raise ChildFailedError(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031625884-05:00 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031670930-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031674016-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py FAILED
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031677593-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031680007-05:00 Failures:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031682312-05:00 [1]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031684947-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031687412-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031689876-05:00   rank      : 2 (local_rank: 2)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031692161-05:00   exitcode  : -6 (pid: 107)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031694896-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031697210-05:00   traceback : Signal 6 (SIGABRT) received by PID 107
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031699274-05:00 [2]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031701538-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031703743-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031705947-05:00   rank      : 3 (local_rank: 3)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031708211-05:00   exitcode  : -6 (pid: 108)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031710345-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031712569-05:00   traceback : Signal 6 (SIGABRT) received by PID 108
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031714613-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031716938-05:00 Root Cause (first observed failure):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031719783-05:00 [0]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031722829-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031725153-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031727458-05:00   rank      : 1 (local_rank: 1)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031729862-05:00   exitcode  : -6 (pid: 106)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031732277-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031734541-05:00   traceback : Signal 6 (SIGABRT) received by PID 106
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.031736786-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.089472873-05:00 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 95) of binary: /usr/bin/python3
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.099504104-05:00 Traceback (most recent call last):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.099532358-05:00   File "/usr/local/bin/torchrun", line 8, in <module>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.099568427-05:00     sys.exit(main())
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.099579247-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.099978006-05:00     return f(*args, **kwargs)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.099987063-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.100343672-05:00     run(args)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.100352880-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.100883500-05:00     elastic_launch(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.100890313-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101345459-05:00     return launch_agent(self._config, self._entrypoint, list(args))
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101359996-05:00   File "/rsrch1/ip/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101791708-05:00     raise ChildFailedError(
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101802108-05:00 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101817347-05:00 ============================================================
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101820272-05:00 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py FAILED
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101823228-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101826344-05:00 Failures:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101831794-05:00 [1]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101834620-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101837184-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101839589-05:00   rank      : 2 (local_rank: 2)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101841994-05:00   exitcode  : -6 (pid: 97)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101844478-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101846622-05:00   traceback : Signal 6 (SIGABRT) received by PID 97
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101848777-05:00 [2]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101851101-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101853426-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101855630-05:00   rank      : 3 (local_rank: 3)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101858064-05:00   exitcode  : -6 (pid: 98)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101860529-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101862933-05:00   traceback : Signal 6 (SIGABRT) received by PID 98
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101865077-05:00 ------------------------------------------------------------
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101867392-05:00 Root Cause (first observed failure):
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101869556-05:00 [0]:
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101871910-05:00   time      : 2025-05-09_04:17:35
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101874215-05:00   host      : msalehjahromi-torchrun-ftn-pwjxr
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101876369-05:00   rank      : 1 (local_rank: 1)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101879204-05:00   exitcode  : -6 (pid: 95)
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101881389-05:00   error_file: <N/A>
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101883523-05:00   traceback : Signal 6 (SIGABRT) received by PID 95
[pod/msalehjahromi-torchrun-ftn-pwjxr/main] 2025-05-08T23:17:36.101885697-05:00 ============================================================
