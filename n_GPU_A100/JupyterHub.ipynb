{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2aed02-3d9c-4f45-b9fd-92d71e1238b6",
   "metadata": {},
   "source": [
    "# 1. DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6f2e39-b110-4be8-82bd-5443f49ca638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pip', 'install', '-q', '--extra-index-url', 'https://download.pytorch.org/whl/cu117', 'torch==2.0.0', 'torchvision==0.15.0', 'omegaconf', 'torchmetrics==0.10.3', 'fvcore', 'iopath', 'xformers==0.0.18', 'submitit', 'numpy<2.0']\n",
      "['pip', 'install', '-q', '--extra-index-url', 'https://pypi.nvidia.com', 'cuml-cu11']\n",
      "['pip', 'install', '-q', 'black==22.6.0', 'flake8==5.0.4', 'pylint==2.15.0']\n",
      "['pip', 'install', '-q', 'mmsegmentation==0.27.0']\n",
      "['pip', 'install', '-q', 'mmcv-full==1.5.0']\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "import os\n",
    "import subprocess\n",
    "pip_commands = [\n",
    "    [\"pip\", \"install\",\"-q\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\", \n",
    "     \"torch==2.0.0\", \"torchvision==0.15.0\", \"omegaconf\", \"torchmetrics==0.10.3\", \n",
    "     \"fvcore\", \"iopath\", \"xformers==0.0.18\", \"submitit\", \"numpy<2.0\"],\n",
    "    [\"pip\", \"install\", \"-q\",  \"--extra-index-url\", \"https://pypi.nvidia.com\", \"cuml-cu11\"],\n",
    "    [\"pip\", \"install\",\"-q\",  \"black==22.6.0\", \"flake8==5.0.4\", \"pylint==2.15.0\"],\n",
    "    [\"pip\", \"install\", \"-q\", \"mmsegmentation==0.27.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"mmcv-full==1.5.0\"]\n",
    "]\n",
    "\n",
    "for cmd in pip_commands:\n",
    "    try:\n",
    "        print(cmd)\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install packages with command: {cmd}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcbc99-2187-44d4-ba49-db2ff8a4ed85",
   "metadata": {},
   "source": [
    "### The following got stuck after\n",
    "\n",
    "2025-05-05 22:04:16,876 - INFO - Step 100 | Loss: 3.616974\n",
    "\n",
    "2025-05-05 22:05:19,696 - INFO - Step 200 | Loss: 0.003891\n",
    "\n",
    "2025-05-05 22:06:32,967 - INFO - Step 300 | Loss: 0.032542\n",
    "\n",
    "2025-05-05 22:08:02,341 - INFO - Step 400 | Loss: 0.011597\n",
    "\n",
    "### and also did not save the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05767f94-9f7b-48ca-ba19-0d76d8e0b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "2025-05-05 22:53:18,192 - INFO - Starting training on 4 GPUs with full model copies (DDP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 22:53:22,103 - INFO - using MLP layer as FFN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n",
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n",
      "Verifying all parameters are on cuda:0\n",
      "All parameters successfully verified on cuda:0\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:651: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:642: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:651: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:642: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:651: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:642: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:651: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:642: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "2025-05-05 22:53:23,660 - INFO - DinoVisionTransformer parameters: 176 (LR: 1e-05)\n",
      "2025-05-05 22:53:23,660 - INFO - Aggregator parameters: 26 (LR: 0.0001)\n",
      "2025-05-05 22:53:23,661 - INFO - Setting unfreeze_strategy to 'all' to avoid DDP synchronization issues\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250505_225323.jsonl\n",
      "Epoch 1: All parameters already unfrozen (strategy: all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 22:54:38,671 - INFO - total_loss: 0.0230 | acc1: 0.8000 | acc3: 0.7475 | acc6: 0.5682 | pos_count_1yr: 4-96 | pos_count_3yr: 8-91 | pos_count_6yr: 10-34 | iteration: 100 | epoch: 0 | lr: 0.0000 | accumulation_step: 19 | accum_size: 20 | type: train\n",
      "2025-05-05 22:57:33,918 - INFO - total_loss: 0.0134 | acc1: 0.8950 | acc3: 0.8660 | acc6: 0.7326 | pos_count_1yr: 5-195 | pos_count_3yr: 9-185 | pos_count_6yr: 14-72 | iteration: 200 | epoch: 0 | lr: 0.0000 | accumulation_step: 19 | accum_size: 20 | type: train\n",
      "2025-05-05 23:00:48,531 - INFO - total_loss: 0.0104 | acc1: 0.9267 | acc3: 0.9031 | acc6: 0.8031 | pos_count_1yr: 6-294 | pos_count_3yr: 11-278 | pos_count_6yr: 16-111 | iteration: 300 | epoch: 0 | lr: 0.0000 | accumulation_step: 19 | accum_size: 20 | type: train\n",
      "2025-05-05 23:03:01,053 - INFO - total_loss: 0.0109 | acc1: 0.9350 | acc3: 0.9141 | acc6: 0.8072 | pos_count_1yr: 10-390 | pos_count_3yr: 16-368 | pos_count_6yr: 23-143 | iteration: 400 | epoch: 0 | lr: 0.0000 | accumulation_step: 19 | accum_size: 20 | type: train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation at step 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E ProcessGroupNCCL.cpp:828] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7990, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801241 milliseconds before timing out.\n",
      "[E ProcessGroupNCCL.cpp:828] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7990, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801733 milliseconds before timing out.\n",
      "[E ProcessGroupNCCL.cpp:828] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7990, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1804191 milliseconds before timing out.\n",
      "[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.\n",
      "terminate called after throwing an instance of 'std::runtime_error'\n",
      "  what():  [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7990, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801241 milliseconds before timing out.\n",
      "[E ProcessGroupNCCL.cpp:455] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[E ProcessGroupNCCL.cpp:460] To avoid data inconsistency, we are taking the entire process down.\n",
      "terminate called after throwing an instance of 'std::runtime_error'\n",
      "  what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=7990, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801733 milliseconds before timing out.\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 23340 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 23343 closing signal SIGTERM\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 23341) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\n",
      "    run(args)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-05-05_23:35:44\n",
      "  host      : jupyter-msalehjahromi\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : -6 (pid: 23342)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 6 (SIGABRT) received by PID 23342\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-05-05_23:35:44\n",
      "  host      : jupyter-msalehjahromi\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : -6 (pid: 23341)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 6 (SIGABRT) received by PID 23341\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: torchrun --nproc_per_node=4 --master_port=34995 /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py --csv /rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv --accum-steps 20 --num-workers 5 --epochs 10 --lr 0.0001 --weight-decay 0.0001 --optimizer adamw --num-attn-heads 3 --num-layers 2 --dropout 0.3 --unfreeze-strategy gradual --output ./output_ddp --print-every 100 --val-every 500 --metrics-dir /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py\", line 267, in <module>\n",
      "    main(args) \n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py\", line 217, in main\n",
      "    subprocess.run(torchrun_command, check=True)\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['torchrun', '--nproc_per_node=4', '--master_port=34995', '/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py', '--csv', '/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv', '--accum-steps', '20', '--num-workers', '5', '--epochs', '10', '--lr', '0.0001', '--weight-decay', '0.0001', '--optimizer', 'adamw', '--num-attn-heads', '3', '--num-layers', '2', '--dropout', '0.3', '--unfreeze-strategy', 'gradual', '--output', './output_ddp', '--print-every', '100', '--val-every', '500', '--metrics-dir', '/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Command '['python3', '/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py', '--num-gpus', '4', '--csv', '/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv', '--accum-steps', '20', '--num-workers', '5', '--epochs', '10', '--lr', '0.0001', '--weight-decay', '0.0001', '--optimizer', 'adamw', '--num-attn-heads', '3', '--num-layers', '2', '--dropout', '0.3', '--unfreeze-strategy', 'gradual', '--output', './output_ddp', '--print-every', '100', '--val-every', '500', '--metrics-dir', '/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu']' returned non-zero exit status 1.\n",
      "Command output: None\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "import os\n",
    "import subprocess\n",
    "# Set required environment variables\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"4\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_WORLD_SIZE\"] = \"4\"\n",
    "\n",
    "# Build the command with --install-packages flag removed\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py\",\n",
    "    \"--num-gpus\", \"4\",\n",
    "    \"--csv\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv\",\n",
    "    \"--accum-steps\", \"20\",\n",
    "    \"--num-workers\", \"5\",\n",
    "    \"--epochs\", \"10\",\n",
    "    \"--lr\", \"0.0001\",\n",
    "    \"--weight-decay\", \"0.0001\",\n",
    "    \"--optimizer\", \"adamw\",\n",
    "    \"--num-attn-heads\", \"3\",\n",
    "    \"--num-layers\", \"2\",\n",
    "    \"--dropout\", \"0.3\",\n",
    "    \"--unfreeze-strategy\", \"gradual\",\n",
    "    \"--output\", \"./output_ddp\",\n",
    "    \"--print-every\", \"100\",\n",
    "    \"--val-every\", \"500\",\n",
    "    \"--metrics-dir\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(f\"Command output: {e.output if hasattr(e, 'output') else 'No output available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f09fe-7240-4938-805d-dc5e1434916a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msalehjahromi (py3.10.12)",
   "language": "python",
   "name": "msalehjahromi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
