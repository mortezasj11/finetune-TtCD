{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2aed02-3d9c-4f45-b9fd-92d71e1238b6",
   "metadata": {},
   "source": [
    "# 1. DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baeaf912-771e-48be-8d7e-56dbdd7b5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#integration syncronization impliment integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6f2e39-b110-4be8-82bd-5443f49ca638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pip', 'install', '-q', '--extra-index-url', 'https://download.pytorch.org/whl/cu117', 'torch==2.0.0', 'torchvision==0.15.0', 'omegaconf', 'torchmetrics==0.10.3', 'fvcore', 'iopath', 'xformers==0.0.18', 'submitit', 'numpy<2.0']\n",
      "['pip', 'install', '-q', '--extra-index-url', 'https://pypi.nvidia.com', 'cuml-cu11']\n",
      "['pip', 'install', '-q', 'black==22.6.0', 'flake8==5.0.4', 'pylint==2.15.0']\n",
      "['pip', 'install', '-q', 'mmsegmentation==0.27.0']\n",
      "['pip', 'install', '-q', 'mmcv-full==1.5.0']\n",
      "['pip', 'install', '-q', 'nibabel']\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "import os\n",
    "import subprocess\n",
    "pip_commands = [\n",
    "    [\"pip\", \"install\",\"-q\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\", \n",
    "     \"torch==2.0.0\", \"torchvision==0.15.0\", \"omegaconf\", \"torchmetrics==0.10.3\", \n",
    "     \"fvcore\", \"iopath\", \"xformers==0.0.18\", \"submitit\", \"numpy<2.0\"],\n",
    "    [\"pip\", \"install\", \"-q\",  \"--extra-index-url\", \"https://pypi.nvidia.com\", \"cuml-cu11\"],\n",
    "    [\"pip\", \"install\",\"-q\",  \"black==22.6.0\", \"flake8==5.0.4\", \"pylint==2.15.0\"],\n",
    "    [\"pip\", \"install\", \"-q\", \"mmsegmentation==0.27.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"mmcv-full==1.5.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"nibabel\"]\n",
    "]\n",
    "\n",
    "for cmd in pip_commands:\n",
    "    try:\n",
    "        print(cmd)\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install packages with command: {cmd}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcbc99-2187-44d4-ba49-db2ff8a4ed85",
   "metadata": {},
   "source": [
    "### The following (x_ddp.py not x_ddp_full.py) got stuck after\n",
    "\n",
    "2025-05-05 22:04:16,876 - INFO - Step 100 | Loss: 3.616974\n",
    "\n",
    "2025-05-05 22:05:19,696 - INFO - Step 200 | Loss: 0.003891\n",
    "\n",
    "2025-05-05 22:06:32,967 - INFO - Step 300 | Loss: 0.032542\n",
    "\n",
    "2025-05-05 22:08:02,341 - INFO - Step 400 | Loss: 0.011597\n",
    "\n",
    "### and also did not save the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05767f94-9f7b-48ca-ba19-0d76d8e0b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 22:54:32,397 - INFO - Starting training on 4 GPUs with full model copies (DDP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Dataset Statistics ====\n",
      "Total samples: 43852\n",
      "Training samples: 32922\n",
      "Validation samples: 5460\n",
      "\n",
      "1-year-cancer:\n",
      "  Train positive: 1242 (3.77%)\n",
      "  Val positive: 186 (3.41%)\n",
      "\n",
      "2-year-cancer:\n",
      "  Train positive: 1638 (4.98%)\n",
      "  Val positive: 253 (4.63%)\n",
      "\n",
      "3-year-cancer:\n",
      "  Train positive: 1970 (5.98%)\n",
      "  Val positive: 312 (5.71%)\n",
      "\n",
      "4-year-cancer:\n",
      "  Train positive: 2226 (6.76%)\n",
      "  Val positive: 360 (6.59%)\n",
      "\n",
      "5-year-cancer:\n",
      "  Train positive: 2436 (7.40%)\n",
      "  Val positive: 402 (7.36%)\n",
      "\n",
      "6-year-cancer:\n",
      "  Train positive: 2562 (7.78%)\n",
      "  Val positive: 426 (7.80%)\n",
      "============================\n",
      "\n",
      "Total validation samples: 5460\n",
      "Created distributed validation loader, each rank processes ~1289 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 22:54:37,722 - INFO - using MLP layer as FFN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n",
      "\n",
      "Verifying all parameters are on cuda:0\n",
      "All parameters successfully verified on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:893: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:884: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:893: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:893: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:884: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:884: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:893: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:884: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "2025-05-12 22:54:39,561 - INFO - DinoVisionTransformer parameters: 176 (LR: 1.0000000000000002e-06)\n",
      "2025-05-12 22:54:39,561 - INFO - Aggregator parameters: 26 (LR: 0.0001)\n",
      "2025-05-12 22:54:39,561 - INFO - Setting unfreeze_strategy to 'all' to avoid DDP synchronization issues\n",
      "2025-05-12 22:54:39,565 - INFO - using MLP layer as FFN\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:116: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250512_225441.jsonl\n",
      "Will save checkpoints to ./output_ddp/checkpoints\n",
      "\n",
      "Training Configuration:\n",
      "Max chunks per sample: 60\n",
      "Learning rate: 0.0001\n",
      "Number of tasks: 6\n",
      "Validation frequency: 100 steps\n",
      "Number of epochs: 10\n",
      "Warmup steps: 1000\n",
      "World size: 4\n",
      "Device: cuda\n",
      "\n",
      "Epoch 1: All parameters already unfrozen (strategy: all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 50 (Epoch 1):\n",
      "Loss: 0.9318\n",
      "Learning rate: 0.000005\n",
      "Accuracy: Task 0: 0.96 | Task 1: 0.86 | Task 2: 0.04 | Task 3: 0.06 | Task 4: 0.10 | Task 5: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 22:56:05,461 - INFO - iteration: 50 | epoch: 0 | loss: 0.9318 | lr: 0.0000 | type: training | acc_task0: 0.9600 | acc_task1: 0.8571 | acc_task2: 0.0408 | acc_task3: 0.0612 | acc_task4: 0.1000 | acc_task5: 0.8235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached validation point at step 100 (in epoch 1)\n",
      "\n",
      "=== Starting distributed validation at step 100 ===\n",
      "## 33 ## torch.Size([1289, 1, 6]) torch.Size([1289, 6]) torch.Size([1289, 6])\n",
      "## 33 ## torch.Size([1289, 1, 6]) torch.Size([1289, 6]) torch.Size([1289, 6])\n",
      "## 33 ## torch.Size([1289, 1, 6]) torch.Size([1289, 6]) torch.Size([1289, 6])\n",
      "## 33 ## torch.Size([1289, 1, 6]) torch.Size([1289, 6]) torch.Size([1289, 6])\n",
      "## 34 ## torch.Size([1289, 1, 6]) torch.Size([1289, 6]) torch.Size([1289, 6])\n",
      "## 34 ## 4 4 4\n",
      "#########\n",
      "## 36 ## ## 34 ## torch.Size([1289, 1, 6]) torch.Size([1289, 6]) torch.Size([1289, 6])\n",
      "## 34 ## ## 34 ##4 4 4\n",
      "#########\n",
      " ## 36 ## torch.Size([1289, 1, 6]) torch.Size([1289, 6]) torch.Size([1289, 6])\n",
      "## 34 ## 4 4 4\n",
      "#########\n",
      "## 36 ## ## 34 ## torch.Size([1289, 1, 6]) torch.Size([1289, 6]) torch.Size([1289, 6])\n",
      "## 34 ## 4 4 4\n",
      "#########\n",
      "## 36 ## [tensor([1289], device='cuda:0'), tensor([1289], device='cuda:0'), tensor([1289], device='cuda:0'), tensor([1289], device='cuda:0')] tensor([1289], device='cuda:0')\n",
      "\n",
      "[tensor([1289], device='cuda:1'), tensor([1289], device='cuda:1'), tensor([1289], device='cuda:1'), tensor([1289], device='cuda:1')] [tensor([1289], device='cuda:3'), tensor([1289], device='cuda:3'), tensor([1289], device='cuda:3'), tensor([1289], device='cuda:3')] tensor([1289], device='cuda:1')\n",
      "\n",
      "tensor([1289], device='cuda:3')\n",
      "\n",
      "[tensor([1289], device='cuda:2'), tensor([1289], device='cuda:2'), tensor([1289], device='cuda:2'), tensor([1289], device='cuda:2')] tensor([1289], device='cuda:2')\n",
      "\n",
      "Evaluated 5156 samples across 4 processes\n",
      "Average loss: 0.8149\n",
      "Validation completed in 1403.21 seconds\n",
      "=== Distributed validation complete at step 100 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 23:20:42,074 - INFO - samples_evaluated: 5156 | avg_loss: 0.8149 | auc_task0: 0.4734 | acc_task0: 0.0000 | auc_task1: 0.5176 | acc_task1: 0.0352 | auc_task2: 0.5369 | acc_task2: 0.9655 | auc_task3: 0.5060 | acc_task3: 0.9338 | auc_task4: 0.5050 | acc_task4: 1.0000 | auc_task5: 0.5115 | acc_task5: 0.0000 | iteration: 100 | epoch: 0 | lr: 0.0000 | validation_time_seconds: 1403.2111 | type: validation\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100 (Epoch 1):\n",
      "Loss: 1.4976\n",
      "Learning rate: 0.000006\n",
      "Accuracy: Task 0: 0.96 | Task 1: 0.86 | Task 2: 0.08 | Task 3: 0.09 | Task 4: 0.13 | Task 5: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 23:20:44,227 - INFO - iteration: 100 | epoch: 0 | loss: 1.4976 | lr: 0.0000 | type: training | acc_task0: 0.9600 | acc_task1: 0.8586 | acc_task2: 0.0808 | acc_task3: 0.0918 | acc_task4: 0.1286 | acc_task5: 0.7727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 150 (Epoch 1):\n",
      "Loss: 0.0483\n",
      "Learning rate: 0.000009\n",
      "Accuracy: Task 0: 0.97 | Task 1: 0.90 | Task 2: 0.06 | Task 3: 0.08 | Task 4: 0.10 | Task 5: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 23:22:43,924 - INFO - iteration: 150 | epoch: 0 | loss: 0.0483 | lr: 0.0000 | type: training | acc_task0: 0.9667 | acc_task1: 0.8986 | acc_task2: 0.0616 | acc_task3: 0.0764 | acc_task4: 0.1028 | acc_task5: 0.8060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached validation point at step 200 (in epoch 1)\n",
      "\n",
      "=== Starting distributed validation at step 200 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 64108 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 64109 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 64110 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 64111 closing signal SIGINT\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b5e7d7520>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7efd589eb520>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe3da1e7520>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1438, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1438, in _shutdown_workers\n",
      "    self._mark_worker_as_unavailable(worker_id, shutdown=True)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1380, in _mark_worker_as_unavailable\n",
      "    self._mark_worker_as_unavailable(worker_id, shutdown=True)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1386, in _mark_worker_as_unavailable\n",
      "    assert self._workers_status[worker_id] or (self._persistent_workers and shutdown)\n",
      "KeyboardInterrupt: \n",
      "    q.put(None)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 96, in put\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1430, in _shutdown_workers\n",
      "    self._notempty.notify()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 375, in notify\n",
      "    waiter.release()\n",
      "KeyboardInterrupt: \n",
      "    self._workers_done_event.set()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 334, in set\n",
      "    def set(self):\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 1128, in <module>\n",
      "    main(args)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 984, in main\n",
      "    trainer.fit(\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 572, in fit\n",
      "    self._run_validation(val_loader)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 650, in _run_validation\n",
      "    metrics = self.evaluate(val_loader)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 289, in evaluate\n",
      "    logits = self.model(chunks, spacing)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
      "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
      "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 297, in forward\n",
      "    feats = torch.cat([self._chunk_embed(x[i]) for i in range(S)], dim=0)  # [S, 768]\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 297, in <listcomp>\n",
      "    feats = torch.cat([self._chunk_embed(x[i]) for i in range(S)], dim=0)  # [S, 768]\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 287, in _chunk_embed\n",
      "    return self.base(chunk.unsqueeze(0))       # → [1, 768]\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/models/vision_transformer.py\", line 325, in forward\n",
      "Traceback (most recent call last):\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 1128, in <module>\n",
      "    main(args)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 984, in main\n",
      "    trainer.fit(\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 572, in fit\n",
      "    self._run_validation(val_loader)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 650, in _run_validation\n",
      "    metrics = self.evaluate(val_loader)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 289, in evaluate\n",
      "    logits = self.model(chunks, spacing)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
      "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
      "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 297, in forward\n",
      "    feats = torch.cat([self._chunk_embed(x[i]) for i in range(S)], dim=0)  # [S, 768]\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 297, in <listcomp>\n",
      "    feats = torch.cat([self._chunk_embed(x[i]) for i in range(S)], dim=0)  # [S, 768]\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 287, in _chunk_embed\n",
      "    return self.base(chunk.unsqueeze(0))       # → [1, 768]\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/models/vision_transformer.py\", line 325, in forward\n",
      "Traceback (most recent call last):\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 1128, in <module>\n",
      "    ret = self.forward_features(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/models/vision_transformer.py\", line 261, in forward_features\n",
      "    x = blk(x)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/models/vision_transformer.py\", line 40, in forward\n",
      "    x = b(x)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py\", line 254, in forward\n",
      "    main(args)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 984, in main\n",
      "    trainer.fit(\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 572, in fit\n",
      "    self._run_validation(val_loader)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 650, in _run_validation\n",
      "    metrics = self.evaluate(val_loader)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py\", line 289, in evaluate\n",
      "    logits = self.model(chunks, spacing)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
      "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
      "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 297, in forward\n",
      "    feats = torch.cat([self._chunk_embed(x[i]) for i in range(S)], dim=0)  # [S, 768]\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 297, in <listcomp>\n",
      "    feats = torch.cat([self._chunk_embed(x[i]) for i in range(S)], dim=0)  # [S, 768]\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/model_utils.py\", line 287, in _chunk_embed\n",
      "    return self.base(chunk.unsqueeze(0))       # → [1, 768]\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/models/vision_transformer.py\", line 325, in forward\n",
      "    ret = self.forward_features(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/models/vision_transformer.py\", line 261, in forward_features\n",
      "    x = blk(x)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/models/vision_transformer.py\", line 40, in forward\n",
      "    x = b(x)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py\", line 254, in forward\n",
      "        return super().forward(x_or_x_list)return super().forward(x_or_x_list)\n",
      "\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py\", line 112, in forward\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py\", line 113, in forward\n",
      "    x = x + attn_residual_func(x)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py\", line 91, in attn_residual_func\n",
      "    x = x + ffn_residual_func(x)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py\", line 94, in ffn_residual_func\n",
      "    return self.ls1(self.attn(self.norm1(x)))\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return self.ls2(self.mlp(self.norm2(x)))\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py\", line 84, in forward\n",
      "KeyboardInterrupt\n",
      "    x = memory_efficient_attention(q, k, v, attn_bias=attn_bias)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 196, in memory_efficient_attention\n",
      "    return _memory_efficient_attention(\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 294, in _memory_efficient_attention\n",
      "    return _memory_efficient_attention_forward(\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 314, in _memory_efficient_attention_forward\n",
      "    out, *_ = op.apply(inp, needs_gradient=False)\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py\", line 175, in apply\n",
      "    out, lse, rng_seed, rng_offset = cls.OPERATOR(\n",
      "  File \"/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/_ops.py\", line 502, in __call__\n",
      "    return self._op(*args, **kwargs or {})\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Run the command\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1146\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "# Set required environment variables\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"4\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_WORLD_SIZE\"] = \"4\"\n",
    "\n",
    "# Build the command with --install-packages flag removed\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py\",\n",
    "    \"--num-gpus\", \"4\",\n",
    "    \"--csv\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv\",\n",
    "    \"--accum-steps\", \"200\", #200\n",
    "    \"--num-workers\", \"5\",\n",
    "    \"--epochs\", \"10\",\n",
    "    \"--lr\", \"0.0001\",\n",
    "    \"--weight-decay\", \"0.0001\",\n",
    "    \"--optimizer\", \"adamw\",\n",
    "    \"--num-attn-heads\", \"3\",\n",
    "    \"--num-layers\", \"2\",\n",
    "    \"--dropout\", \"0.3\",\n",
    "    \"--unfreeze-strategy\", \"all\",  ##\n",
    "    \"--output\", \"./output_ddp\",\n",
    "    \"--print-every\", \"2000\", #1000\n",
    "    \"--val-every\", \"24000\", #50k\n",
    "    \"--metrics-dir\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(f\"Command output: {e.output if hasattr(e, 'output') else 'No output available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f09fe-7240-4938-805d-dc5e1434916a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd92e8e-7b09-4b03-8315-0c9a79954215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816d95a-5e31-46ae-b11b-ab7fc9daa2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msalehjahromi (py3.10.12)",
   "language": "python",
   "name": "msalehjahromi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
