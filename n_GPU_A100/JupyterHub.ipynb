{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2aed02-3d9c-4f45-b9fd-92d71e1238b6",
   "metadata": {},
   "source": [
    "# 1. DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6f2e39-b110-4be8-82bd-5443f49ca638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pip', 'install', '-q', '--extra-index-url', 'https://download.pytorch.org/whl/cu117', 'torch==2.0.0', 'torchvision==0.15.0', 'omegaconf', 'torchmetrics==0.10.3', 'fvcore', 'iopath', 'xformers==0.0.18', 'submitit', 'numpy<2.0']\n",
      "['pip', 'install', '-q', '--extra-index-url', 'https://pypi.nvidia.com', 'cuml-cu11']\n",
      "['pip', 'install', '-q', 'black==22.6.0', 'flake8==5.0.4', 'pylint==2.15.0']\n",
      "['pip', 'install', '-q', 'mmsegmentation==0.27.0']\n",
      "['pip', 'install', '-q', 'mmcv-full==1.5.0']\n",
      "['pip', 'install', '-q', 'nibabel']\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "import os\n",
    "import subprocess\n",
    "pip_commands = [\n",
    "    [\"pip\", \"install\",\"-q\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\", \n",
    "     \"torch==2.0.0\", \"torchvision==0.15.0\", \"omegaconf\", \"torchmetrics==0.10.3\", \n",
    "     \"fvcore\", \"iopath\", \"xformers==0.0.18\", \"submitit\", \"numpy<2.0\"],\n",
    "    [\"pip\", \"install\", \"-q\",  \"--extra-index-url\", \"https://pypi.nvidia.com\", \"cuml-cu11\"],\n",
    "    [\"pip\", \"install\",\"-q\",  \"black==22.6.0\", \"flake8==5.0.4\", \"pylint==2.15.0\"],\n",
    "    [\"pip\", \"install\", \"-q\", \"mmsegmentation==0.27.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"mmcv-full==1.5.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"nibabel\"]\n",
    "]\n",
    "\n",
    "for cmd in pip_commands:\n",
    "    try:\n",
    "        print(cmd)\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install packages with command: {cmd}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcbc99-2187-44d4-ba49-db2ff8a4ed85",
   "metadata": {},
   "source": [
    "### The following (x_ddp.py not x_ddp_full.py) got stuck after\n",
    "\n",
    "2025-05-05 22:04:16,876 - INFO - Step 100 | Loss: 3.616974\n",
    "\n",
    "2025-05-05 22:05:19,696 - INFO - Step 200 | Loss: 0.003891\n",
    "\n",
    "2025-05-05 22:06:32,967 - INFO - Step 300 | Loss: 0.032542\n",
    "\n",
    "2025-05-05 22:08:02,341 - INFO - Step 400 | Loss: 0.011597\n",
    "\n",
    "### and also did not save the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05767f94-9f7b-48ca-ba19-0d76d8e0b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 23:55:39,556 - INFO - Starting training on 4 GPUs with full model copies (DDP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Dataset Statistics ====\n",
      "Total samples: 43852\n",
      "Training samples: 32922\n",
      "Validation samples: 5460\n",
      "\n",
      "1-year-cancer:\n",
      "  Train positive: 1242 (3.77%)\n",
      "  Val positive: 186 (3.41%)\n",
      "\n",
      "2-year-cancer:\n",
      "  Train positive: 1638 (4.98%)\n",
      "  Val positive: 253 (4.63%)\n",
      "\n",
      "3-year-cancer:\n",
      "  Train positive: 1970 (5.98%)\n",
      "  Val positive: 312 (5.71%)\n",
      "\n",
      "4-year-cancer:\n",
      "  Train positive: 2226 (6.76%)\n",
      "  Val positive: 360 (6.59%)\n",
      "\n",
      "5-year-cancer:\n",
      "  Train positive: 2436 (7.40%)\n",
      "  Val positive: 402 (7.36%)\n",
      "\n",
      "6-year-cancer:\n",
      "  Train positive: 2562 (7.78%)\n",
      "  Val positive: 426 (7.80%)\n",
      "============================\n",
      "\n",
      "Using first 100 samples from validation set (total: 5460)\n",
      "Created validation loader with 88 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 23:55:43,044 - INFO - using MLP layer as FFN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:729: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:720: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n",
      "Found cls_token in weights with shape: torch.Size([1, 1, 768])\n",
      "Verifying all parameters are on cuda:0\n",
      "All parameters successfully verified on cuda:0\n",
      "Cls token shape after loading: torch.Size([1, 1, 768]), requires_grad: True, storage size: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:729: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:720: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:729: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:720: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:729: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  f\"storage size: {model_ct.cls_token.storage().size()}\")\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:720: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:  # Check if meta/empty\n",
      "2025-05-07 23:55:45,049 - INFO - DinoVisionTransformer parameters: 176 (LR: 1.0000000000000002e-06)\n",
      "2025-05-07 23:55:45,049 - INFO - Aggregator parameters: 26 (LR: 0.0001)\n",
      "2025-05-07 23:55:45,049 - INFO - Setting unfreeze_strategy to 'all' to avoid DDP synchronization issues\n",
      "2025-05-07 23:55:45,061 - INFO - using MLP layer as FFN\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:110: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:110: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:110: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_full.py:110: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250507_235547.jsonl\n",
      "\n",
      "Training Configuration:\n",
      "Max chunks per sample: 66\n",
      "Learning rate: 0.0001\n",
      "Number of tasks: 6\n",
      "Validation frequency: 100 steps\n",
      "Number of epochs: 10\n",
      "Warmup steps: 1000\n",
      "World size: 4\n",
      "Device: cuda\n",
      "\n",
      "Epoch 1: All parameters already unfrozen (strategy: all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 50 (Epoch 1):\n",
      "Loss: 0.0765\n",
      "Learning rate: 0.000005\n",
      "Accuracy: Task 0: 0.96 | Task 1: 0.96 | Task 2: 0.94 | Task 3: 0.94 | Task 4: 0.90 | Task 5: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 23:57:11,957 - INFO - iteration: 50 | epoch: 0 | loss: 0.0765 | lr: 0.0000 | type: training | acc_task0: 0.9600 | acc_task1: 0.9592 | acc_task2: 0.9388 | acc_task3: 0.9388 | acc_task4: 0.9000 | acc_task5: 0.8235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100 (Epoch 1):\n",
      "Loss: 3.3317\n",
      "Learning rate: 0.000006\n",
      "Accuracy: Task 0: 0.96 | Task 1: 0.93 | Task 2: 0.91 | Task 3: 0.91 | Task 4: 0.87 | Task 5: 0.77\n",
      "\n",
      "Reached validation point at step 100 (in epoch 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 23:58:25,271 - INFO - iteration: 100 | epoch: 0 | loss: 3.3317 | lr: 0.0000 | type: training | acc_task0: 0.9600 | acc_task1: 0.9293 | acc_task2: 0.9091 | acc_task3: 0.9082 | acc_task4: 0.8714 | acc_task5: 0.7727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting validation at step 100 ===\n",
      "Evaluated 88 samples\n",
      "Average loss: 1.1286\n",
      "Validation completed in 32.61 seconds\n",
      "=== Validation complete at step 100 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 23:58:58,348 - INFO - samples_evaluated: 88 | avg_loss: 1.1286 | auc_task0: 0.7113 | acc_task0: 0.9545 | auc_task1: 0.5686 | acc_task1: 0.8023 | auc_task2: 0.5594 | acc_task2: 0.5802 | auc_task3: 0.5922 | acc_task3: 0.4444 | auc_task4: 0.4711 | acc_task4: 0.2727 | auc_task5: 0.2909 | acc_task5: 0.0833 | iteration: 100 | epoch: 0 | lr: 0.0000 | validation_time_seconds: 32.6126 | type: validation\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "# Set required environment variables\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"4\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_WORLD_SIZE\"] = \"4\"\n",
    "\n",
    "# Build the command with --install-packages flag removed\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_ddp_launcher.py\",\n",
    "    \"--num-gpus\", \"4\",\n",
    "    \"--csv\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv\",\n",
    "    \"--accum-steps\", \"20\",\n",
    "    \"--num-workers\", \"5\",\n",
    "    \"--epochs\", \"10\",\n",
    "    \"--lr\", \"0.0001\",\n",
    "    \"--weight-decay\", \"0.0001\",\n",
    "    \"--optimizer\", \"adamw\",\n",
    "    \"--num-attn-heads\", \"3\",\n",
    "    \"--num-layers\", \"2\",\n",
    "    \"--dropout\", \"0.3\",\n",
    "    \"--unfreeze-strategy\", \"all\",  ##\n",
    "    \"--output\", \"./output_ddp\",\n",
    "    \"--print-every\", \"50\",\n",
    "    \"--val-every\", \"100\",\n",
    "    \"--metrics-dir\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(f\"Command output: {e.output if hasattr(e, 'output') else 'No output available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f09fe-7240-4938-805d-dc5e1434916a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msalehjahromi (py3.10.12)",
   "language": "python",
   "name": "msalehjahromi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
