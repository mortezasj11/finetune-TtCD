{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c4ce2-0713-4b6a-83e7-d24ff5c274fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pip', 'install', '-q', '--extra-index-url', 'https://download.pytorch.org/whl/cu117', 'torch==2.0.0', 'torchvision==0.15.0', 'omegaconf', 'torchmetrics==0.10.3', 'fvcore', 'iopath', 'xformers==0.0.18', 'submitit', 'numpy<2.0']\n",
      "['pip', 'install', '-q', '--extra-index-url', 'https://pypi.nvidia.com', 'cuml-cu11']\n",
      "['pip', 'install', '-q', 'black==22.6.0', 'flake8==5.0.4', 'pylint==2.15.0']\n",
      "['pip', 'install', '-q', 'mmsegmentation==0.27.0']\n",
      "['pip', 'install', '-q', 'mmcv-full==1.5.0']\n",
      "['pip', 'install', '-q', 'nibabel']\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "import os\n",
    "import subprocess\n",
    "pip_commands = [\n",
    "    [\"pip\", \"install\",\"-q\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\", \n",
    "     \"torch==2.0.0\", \"torchvision==0.15.0\", \"omegaconf\", \"torchmetrics==0.10.3\", \n",
    "     \"fvcore\", \"iopath\", \"xformers==0.0.18\", \"submitit\", \"numpy<2.0\"],\n",
    "    [\"pip\", \"install\", \"-q\",  \"--extra-index-url\", \"https://pypi.nvidia.com\", \"cuml-cu11\"],\n",
    "    [\"pip\", \"install\",\"-q\",  \"black==22.6.0\", \"flake8==5.0.4\", \"pylint==2.15.0\"],\n",
    "    [\"pip\", \"install\", \"-q\", \"mmsegmentation==0.27.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"mmcv-full==1.5.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"nibabel\"]\n",
    "]\n",
    "\n",
    "for cmd in pip_commands:\n",
    "    try:\n",
    "        print(cmd)\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install packages with command: {cmd}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074c6bc-bb8c-4bba-9cc5-506fd40d3f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['._.DS_Store', '1_GPU_H100', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:19:19,463 - INFO - Starting FSDP training on 4 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB\n",
      "Total GPU memory: 39.3812255859375 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:19:23,261 - INFO - using MLP layer as FFN\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_finetune_fsdp.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(base, n) and getattr(base, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_finetune_fsdp.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(base, n) and getattr(base, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_finetune_fsdp.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(base, n) and getattr(base, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_finetune_fsdp.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(base, n) and getattr(base, n).storage().size() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250514_041936.jsonl\n",
      "Will save checkpoints to ./output_ddp/checkpoints\n",
      "\n",
      "Training configuration:\n",
      "  World size    : 4\n",
      "  Device        : cuda:0\n",
      "  Epochs        : 10\n",
      "  LR            : 0.0001\n",
      "  Tasks         : 6\n",
      "Epoch 1: All parameters already unfrozen (strategy: all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100 (Epoch 1):\n",
      "Loss: 1.2678\n",
      "Learning rate: 0.000006\n",
      "Accuracy: Task 0: 0.77 | Task 1: 0.93 | Task 2: 0.08 | Task 3: 0.91 | Task 4: 0.87 | Task 5: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:21:03,277 - INFO - iteration: 100 | epoch: 0 | loss: 1.2678 | lr: 0.0000 | type: training | acc_task0: 0.7700 | acc_task1: 0.9293 | acc_task2: 0.0808 | acc_task3: 0.9082 | acc_task4: 0.8714 | acc_task5: 0.7727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 200 (Epoch 1):\n",
      "Loss: 0.1161\n",
      "Learning rate: 0.000013\n",
      "Accuracy: Task 0: 0.88 | Task 1: 0.96 | Task 2: 0.34 | Task 3: 0.94 | Task 4: 0.92 | Task 5: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:22:53,944 - INFO - iteration: 200 | epoch: 0 | loss: 0.1161 | lr: 0.0000 | type: training | acc_task0: 0.8800 | acc_task1: 0.9592 | acc_task2: 0.3351 | acc_task3: 0.9427 | acc_task4: 0.9241 | acc_task5: 0.8256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 300 (Epoch 1):\n",
      "Loss: 0.0536\n",
      "Learning rate: 0.000024\n",
      "Accuracy: Task 0: 0.92 | Task 1: 0.97 | Task 2: 0.55 | Task 3: 0.95 | Task 4: 0.94 | Task 5: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:24:31,647 - INFO - iteration: 300 | epoch: 0 | loss: 0.0536 | lr: 0.0000 | type: training | acc_task0: 0.9167 | acc_task1: 0.9660 | acc_task2: 0.5467 | acc_task3: 0.9542 | acc_task4: 0.9404 | acc_task5: 0.8661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 400 (Epoch 1):\n",
      "Loss: 0.0272\n",
      "Learning rate: 0.000037\n",
      "Accuracy: Task 0: 0.93 | Task 1: 0.96 | Task 2: 0.65 | Task 3: 0.95 | Task 4: 0.93 | Task 5: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:25:46,800 - INFO - iteration: 400 | epoch: 0 | loss: 0.0272 | lr: 0.0000 | type: training | acc_task0: 0.9275 | acc_task1: 0.9641 | acc_task2: 0.6458 | acc_task3: 0.9496 | acc_task4: 0.9315 | acc_task5: 0.8554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 500 (Epoch 1):\n",
      "Loss: 0.0346\n",
      "Learning rate: 0.000052\n",
      "Accuracy: Task 0: 0.94 | Task 1: 0.97 | Task 2: 0.71 | Task 3: 0.95 | Task 4: 0.93 | Task 5: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:26:57,576 - INFO - iteration: 500 | epoch: 0 | loss: 0.0346 | lr: 0.0001 | type: training | acc_task0: 0.9380 | acc_task1: 0.9650 | acc_task2: 0.7104 | acc_task3: 0.9487 | acc_task4: 0.9258 | acc_task5: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 600 (Epoch 1):\n",
      "Loss: 0.0955\n",
      "Learning rate: 0.000067\n",
      "Accuracy: Task 0: 0.95 | Task 1: 0.97 | Task 2: 0.76 | Task 3: 0.95 | Task 4: 0.93 | Task 5: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:28:09,291 - INFO - iteration: 600 | epoch: 0 | loss: 0.0955 | lr: 0.0001 | type: training | acc_task0: 0.9483 | acc_task1: 0.9691 | acc_task2: 0.7557 | acc_task3: 0.9514 | acc_task4: 0.9307 | acc_task5: 0.8511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 700 (Epoch 1):\n",
      "Loss: 2.0168\n",
      "Learning rate: 0.000080\n",
      "Accuracy: Task 0: 0.95 | Task 1: 0.97 | Task 2: 0.79 | Task 3: 0.95 | Task 4: 0.93 | Task 5: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:29:47,895 - INFO - iteration: 700 | epoch: 0 | loss: 2.0168 | lr: 0.0001 | type: training | acc_task0: 0.9543 | acc_task1: 0.9706 | acc_task2: 0.7851 | acc_task3: 0.9507 | acc_task4: 0.9286 | acc_task5: 0.8530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 800 (Epoch 1):\n",
      "Loss: 0.0198\n",
      "Learning rate: 0.000091\n",
      "Accuracy: Task 0: 0.96 | Task 1: 0.97 | Task 2: 0.81 | Task 3: 0.96 | Task 4: 0.94 | Task 5: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:31:05,051 - INFO - iteration: 800 | epoch: 0 | loss: 0.0198 | lr: 0.0001 | type: training | acc_task0: 0.9600 | acc_task1: 0.9730 | acc_task2: 0.8105 | acc_task3: 0.9555 | acc_task4: 0.9361 | acc_task5: 0.8673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 900 (Epoch 1):\n",
      "Loss: 0.0229\n",
      "Learning rate: 0.000098\n",
      "Accuracy: Task 0: 0.96 | Task 1: 0.97 | Task 2: 0.82 | Task 3: 0.95 | Task 4: 0.93 | Task 5: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:32:39,966 - INFO - iteration: 900 | epoch: 0 | loss: 0.0229 | lr: 0.0001 | type: training | acc_task0: 0.9611 | acc_task1: 0.9692 | acc_task2: 0.8225 | acc_task3: 0.9510 | acc_task4: 0.9299 | acc_task5: 0.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000 (Epoch 1):\n",
      "Loss: 0.0331\n",
      "Learning rate: 0.000100\n",
      "Accuracy: Task 0: 0.96 | Task 1: 0.97 | Task 2: 0.84 | Task 3: 0.95 | Task 4: 0.93 | Task 5: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:34:15,551 - INFO - iteration: 1000 | epoch: 0 | loss: 0.0331 | lr: 0.0001 | type: training | acc_task0: 0.9640 | acc_task1: 0.9693 | acc_task2: 0.8373 | acc_task3: 0.9527 | acc_task4: 0.9316 | acc_task5: 0.8644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1100 (Epoch 1):\n",
      "Loss: 0.0587\n",
      "Learning rate: 0.000100\n",
      "Accuracy: Task 0: 0.97 | Task 1: 0.97 | Task 2: 0.85 | Task 3: 0.95 | Task 4: 0.93 | Task 5: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:35:59,067 - INFO - iteration: 1100 | epoch: 0 | loss: 0.0587 | lr: 0.0001 | type: training | acc_task0: 0.9664 | acc_task1: 0.9693 | acc_task2: 0.8494 | acc_task3: 0.9540 | acc_task4: 0.9342 | acc_task5: 0.8706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1200 (Epoch 1):\n",
      "Loss: 0.0127\n",
      "Learning rate: 0.000100\n",
      "Accuracy: Task 0: 0.97 | Task 1: 0.97 | Task 2: 0.86 | Task 3: 0.96 | Task 4: 0.94 | Task 5: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:37:49,978 - INFO - iteration: 1200 | epoch: 0 | loss: 0.0127 | lr: 0.0001 | type: training | acc_task0: 0.9675 | acc_task1: 0.9701 | acc_task2: 0.8601 | acc_task3: 0.9559 | acc_task4: 0.9352 | acc_task5: 0.8725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1300 (Epoch 1):\n",
      "Loss: 1.6680\n",
      "Learning rate: 0.000100\n",
      "Accuracy: Task 0: 0.97 | Task 1: 0.97 | Task 2: 0.87 | Task 3: 0.96 | Task 4: 0.94 | Task 5: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:39:26,283 - INFO - iteration: 1300 | epoch: 0 | loss: 1.6680 | lr: 0.0001 | type: training | acc_task0: 0.9692 | acc_task1: 0.9717 | acc_task2: 0.8699 | acc_task3: 0.9559 | acc_task4: 0.9356 | acc_task5: 0.8757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1400 (Epoch 1):\n",
      "Loss: 0.0185\n",
      "Learning rate: 0.000100\n",
      "Accuracy: Task 0: 0.97 | Task 1: 0.97 | Task 2: 0.88 | Task 3: 0.96 | Task 4: 0.94 | Task 5: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:40:56,705 - INFO - iteration: 1400 | epoch: 0 | loss: 0.0185 | lr: 0.0001 | type: training | acc_task0: 0.9714 | acc_task1: 0.9737 | acc_task2: 0.8793 | acc_task3: 0.9591 | acc_task4: 0.9398 | acc_task5: 0.8828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1500 (Epoch 1):\n",
      "Loss: 0.0505\n",
      "Learning rate: 0.000100\n",
      "Accuracy: Task 0: 0.97 | Task 1: 0.97 | Task 2: 0.89 | Task 3: 0.96 | Task 4: 0.94 | Task 5: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:42:19,332 - INFO - iteration: 1500 | epoch: 0 | loss: 0.0505 | lr: 0.0001 | type: training | acc_task0: 0.9733 | acc_task1: 0.9741 | acc_task2: 0.8860 | acc_task3: 0.9604 | acc_task4: 0.9421 | acc_task5: 0.8871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1600 (Epoch 1):\n",
      "Loss: 0.0416\n",
      "Learning rate: 0.000100\n",
      "Accuracy: Task 0: 0.97 | Task 1: 0.97 | Task 2: 0.89 | Task 3: 0.96 | Task 4: 0.94 | Task 5: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 04:43:37,264 - INFO - iteration: 1600 | epoch: 0 | loss: 0.0416 | lr: 0.0001 | type: training | acc_task0: 0.9750 | acc_task1: 0.9744 | acc_task2: 0.8910 | acc_task3: 0.9595 | acc_task4: 0.9414 | acc_task5: 0.8867\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "import os\n",
    "import subprocess\n",
    "# Set required environment variables\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"4\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_WORLD_SIZE\"] = \"4\"\n",
    "\n",
    "# Build the command with --install-packages flag removed\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100/2_run_fineTune_fsdp_launcher.py\",\n",
    "    \"--num-gpus\", \"4\",\n",
    "    \"--csv\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_test_.csv\",\n",
    "    \"--accum-steps\", \"25\", #200\n",
    "    \"--num-workers\", \"5\",\n",
    "    \"--epochs\", \"10\",\n",
    "    \"--lr\", \"0.0001\",\n",
    "    \"--weight-decay\", \"0.0001\",\n",
    "    \"--optimizer\", \"adamw\",\n",
    "    \"--num-attn-heads\", \"3\",\n",
    "    \"--num-layers\", \"2\",\n",
    "    \"--dropout\", \"0.3\",\n",
    "    \"--unfreeze-strategy\", \"all\",  ##\n",
    "    \"--output\", \"./output_ddp\",\n",
    "    \"--print-every\", \"100\", #1000\n",
    "    \"--val-every\", \"5000\", #50k\n",
    "    \"--metrics-dir\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(f\"Command output: {e.output if hasattr(e, 'output') else 'No output available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f573e49-ce5a-48bc-80e9-1d85a7b7ec05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032ebbb-b6f1-4e83-bf34-3cbccacf7ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad05b0-34cb-43b7-9ab0-96ef55ef054b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
