{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2aed02-3d9c-4f45-b9fd-92d71e1238b6",
   "metadata": {},
   "source": [
    "#  DDP\n",
    "\n",
    "## I cannot and should not use more than 1 GPUs in A100 to finetune this, it gets stuck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22ab11-89d3-4805-9cff-b2ee1542fb68",
   "metadata": {},
   "source": [
    "### 1. running on our DGX\n",
    "- docker run -it --rm --gpus '\"device=4,5,6,7\"' -p 12344:12344 --name msalehjahromi --shm-size=192G  --user $(id -u):$(id -g) --group-add 1944259512 --cpuset-cpus=49-96 -v /rsrch7/home/ip_rsrch/wulab/:/rsrch7/home/ip_rsrch/wulab -v /rsrch1/ip/msalehjahromi/:/rsrch1/ip/msalehjahromi --name mori_jupyter nnunetv2:msalehjahromi\n",
    "- cd /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100\n",
    "- jupyter notebook --ip 0.0.0.0 --port 12344\n",
    "- http://1mcprddgx05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb4b0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m2a974e97ed31            \u001b[m  Tue Jun 10 17:32:50 2025  \u001b[1m\u001b[30m535.216.01\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 26'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  628\u001b[m / \u001b[33m40960\u001b[m MB |\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 26'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  626\u001b[m / \u001b[33m40960\u001b[m MB |\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 26'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  628\u001b[m / \u001b[33m40960\u001b[m MB |\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 27'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  628\u001b[m / \u001b[33m40960\u001b[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f668c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =  \"1,2\" # \"2\"#\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.device_count())   # → 2\n",
    "print(torch.cuda.current_device()) # → 0 (this maps to your original GPU 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5180fba",
   "metadata": {},
   "source": [
    "### 2. Probably need installation, restart after and run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e6f2e39-b110-4be8-82bd-5443f49ca638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## !pip uninstall -q histolab -y\n",
    "# # Install required packages first\n",
    "# import os\n",
    "# import subprocess\n",
    "# pip_commands = [\n",
    "#     [\"pip\", \"install\",\"-q\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\", \n",
    "#      \"torch==2.0.0\", \"torchvision==0.15.0\", \"omegaconf\", \"torchmetrics==0.10.3\", \n",
    "#      \"fvcore\", \"iopath\", \"xformers==0.0.18\", \"submitit\", \"numpy<2.0\"],\n",
    "#     [\"pip\", \"install\", \"-q\",  \"--extra-index-url\", \"https://pypi.nvidia.com\", \"cuml-cu11\"],\n",
    "#     [\"pip\", \"install\",\"-q\",  \"black==22.6.0\", \"flake8==5.0.4\", \"pylint==2.15.0\"],\n",
    "#     [\"pip\", \"install\", \"-q\", \"mmsegmentation==0.27.0\"],\n",
    "#     [\"pip\", \"install\",\"-q\", \"mmcv-full==1.5.0\"],\n",
    "#     [\"pip\", \"install\",\"-q\", \"nibabel\"]\n",
    "# ]\n",
    "\n",
    "# for cmd in pip_commands:\n",
    "#     try:\n",
    "#         print(cmd)\n",
    "#         subprocess.run(cmd, check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Failed to install packages with command: {cmd}\")\n",
    "#         print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b58805",
   "metadata": {},
   "source": [
    "### 3. running, notice important hyperparams\n",
    "-  which model are use so vmin -1000., vmax +150, eps 0.00005\n",
    "- `num-workers`\n",
    "- `epochs`: 100\n",
    "- `accum-steps`: 2000 ?\n",
    "- `max-chunks`: 66 \n",
    "- `lr`:  lr*0.1 on base ?\n",
    "- `warmup-steps`: 5k\n",
    "- `print-every` : 5000\n",
    "- `val-every` : 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['n_GPU_A100_v2', '._.gitignore', '._.DS_Store', '._See_attention.ipynb', '1_GPU_H100', 'See_attention.ipynb', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['n_GPU_A100_v2', '._.gitignore', '._.DS_Store', '._See_attention.ipynb', '1_GPU_H100', 'See_attention.ipynb', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:34:50,820 - INFO - Starting training on 2 GPUs with full model copies (DDP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset created on rank \n",
      "Train dataset created on rank \n",
      "\n",
      "==== Dataset Statistics ====\n",
      "Total samples: 43852\n",
      "Training samples: 38382\n",
      "Validation samples: 5470\n",
      "\n",
      "1-year-cancer:\n",
      "  Train positive: 1428 (3.72%)\n",
      "  Val positive: 188 (3.44%)\n",
      "\n",
      "2-year-cancer:\n",
      "  Train positive: 1891 (4.93%)\n",
      "  Val positive: 256 (4.68%)\n",
      "\n",
      "3-year-cancer:\n",
      "  Train positive: 2282 (5.95%)\n",
      "  Val positive: 315 (5.76%)\n",
      "\n",
      "4-year-cancer:\n",
      "  Train positive: 2586 (6.74%)\n",
      "  Val positive: 363 (6.64%)\n",
      "\n",
      "5-year-cancer:\n",
      "  Train positive: 2838 (7.39%)\n",
      "  Val positive: 403 (7.37%)\n",
      "\n",
      "6-year-cancer:\n",
      "  Train positive: 2988 (7.78%)\n",
      "  Val positive: 429 (7.84%)\n",
      "============================\n",
      "\n",
      "Total validation samples: 5470\n",
      "Created distributed validation loader, each rank processes ~2568 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:34:52,931 - INFO - using MLP layer as FFN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on rank \n",
      "Model loaded on rank \n",
      "[resume] no /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_2GPU/checkpoints — fresh run\n",
      "Using unweighted BCEWithLogitsLoss\n",
      "Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250610_173456.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_full.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_full.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save checkpoints to /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_2GPU/checkpoints\n",
      "\n",
      "Training Configuration:\n",
      "Max chunks per sample: 64\n",
      "Learning rate: 6e-05\n",
      "Gradient accumulation steps: 200\n",
      "Number of tasks: 6\n",
      "Validation frequency: 40000 steps\n",
      "Number of epochs: 200\n",
      "Warmup steps: 5000\n",
      "World size: 2\n",
      "Device: cuda\n",
      "\n",
      "Epoch 1: All parameters already unfrozen (strategy: all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:34:56,665 - INFO - ------ Training Configuration ------\n",
      "2025-06-10 17:34:56,665 - INFO - Max chunks per sample         : 64\n",
      "2025-06-10 17:34:56,665 - INFO - Learning rate (aggregator)    : 6e-05\n",
      "2025-06-10 17:34:56,665 - INFO - Learning rate (base)          : 6.000000000000001e-07\n",
      "2025-06-10 17:34:56,665 - INFO - Gradient accumulation steps   : 200\n",
      "2025-06-10 17:34:56,665 - INFO - Warm-up steps                 : 5000\n",
      "2025-06-10 17:34:56,665 - INFO - Aggregator layers / heads     : 2 / 3\n",
      "2025-06-10 17:34:56,665 - INFO - Aggregator dropout            : 0.35\n",
      "2025-06-10 17:34:56,665 - INFO - Validation frequency          : 40000 steps\n",
      "2025-06-10 17:34:56,665 - INFO - Number of epochs              : 200\n",
      "2025-06-10 17:34:56,665 - INFO - World size                    : 2\n",
      "2025-06-10 17:34:56,665 - INFO - Device                        : cuda\n",
      "2025-06-10 17:34:56,665 - INFO - ------------------------------------\n",
      "\n",
      "/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "## Install required packages first\n",
    "n_GPU = \"2\"\n",
    "import os\n",
    "import subprocess\n",
    "# Set required environment variables\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = n_GPU\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_WORLD_SIZE\"] = n_GPU\n",
    "\n",
    "# Build the command with --install-packages flag removed\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_launcher.py\", ##changed\n",
    "    \"--num-gpus\", n_GPU,\n",
    "    \"--csv\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_.csv\",\n",
    "    \"--accum-steps\", \"200\", ###############\n",
    "    \"--num-workers\", \"2\",\n",
    "    \"--epochs\", \"200\", ###############\n",
    "    \"--lr\", \"0.00006\", ###############  .00039 is too much, 0.00036 is fine\n",
    "    \"--warmup-steps\", \"5000\", ##########\n",
    "    \"--weight-decay\", \"0.001\",\n",
    "    \"--optimizer\", \"adamw\",\n",
    "    \"--num-attn-heads\", \"3\",\n",
    "    \"--num-layers\", \"2\",\n",
    "    \"--dropout\", \"0.35\",\n",
    "    \"--unfreeze-strategy\", \"all\",  \n",
    "    \"--output\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_2GPU\",\n",
    "    \"--print-every\", \"1000\", ################ 2000\n",
    "    \"--val-every\", \"40000\", ################ 50k\n",
    "    \"--metrics-dir\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(f\"Command output: {e.output if hasattr(e, 'output') else 'No output available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a576c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
