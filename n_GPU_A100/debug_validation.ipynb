{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73837fd-8691-4d1b-b2cb-b77aa8a7f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch, numpy as np\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53c9a3-31eb-4d4b-bdd7-2f188f698664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce832861-d987-47db-a142-020e977451e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit tensor([[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]])\n",
      "target tensor([0., 0., 1., 0., 0., 0.])\n",
      "mask_tensor tensor([ True,  True,  True,  True, False, False])\n",
      "[tensor([[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]])]\n",
      "[tensor([0., 0., 1., 0., 0., 0.])]\n",
      "[tensor([ True,  True,  True,  True, False, False])]\n",
      "logit tensor([[ 0.7631,  0.0692,  0.1290, -0.9055,  0.9705, -1.3907]])\n",
      "target tensor([0., 0., 0., 1., 0., 0.])\n",
      "mask_tensor tensor([ True,  True,  True,  True,  True, False])\n",
      "[tensor([[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]]), tensor([[ 0.7631,  0.0692,  0.1290, -0.9055,  0.9705, -1.3907]])]\n",
      "[tensor([0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0.])]\n",
      "[tensor([ True,  True,  True,  True, False, False]), tensor([ True,  True,  True,  True,  True, False])]\n",
      "logit tensor([[-0.4765,  1.3066,  0.6974, -0.1251,  1.5011, -0.7810]])\n",
      "target tensor([0., 1., 0., 0., 0., 0.])\n",
      "mask_tensor tensor([ True,  True,  True,  True, False, False])\n",
      "[tensor([[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]]), tensor([[ 0.7631,  0.0692,  0.1290, -0.9055,  0.9705, -1.3907]]), tensor([[-0.4765,  1.3066,  0.6974, -0.1251,  1.5011, -0.7810]])]\n",
      "[tensor([0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 0.])]\n",
      "[tensor([ True,  True,  True,  True, False, False]), tensor([ True,  True,  True,  True,  True, False]), tensor([ True,  True,  True,  True, False, False])]\n",
      "logit tensor([[ 0.4167,  1.9153, -1.6339, -0.4343,  0.0440,  0.7993]])\n",
      "target tensor([0., 0., 1., 0., 0., 1.])\n",
      "mask_tensor tensor([True, True, True, True, True, True])\n",
      "[tensor([[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]]), tensor([[ 0.7631,  0.0692,  0.1290, -0.9055,  0.9705, -1.3907]]), tensor([[-0.4765,  1.3066,  0.6974, -0.1251,  1.5011, -0.7810]]), tensor([[ 0.4167,  1.9153, -1.6339, -0.4343,  0.0440,  0.7993]])]\n",
      "[tensor([0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 1.])]\n",
      "[tensor([ True,  True,  True,  True, False, False]), tensor([ True,  True,  True,  True,  True, False]), tensor([ True,  True,  True,  True, False, False]), tensor([True, True, True, True, True, True])]\n",
      "logit tensor([[-1.1750,  1.5364, -0.4331,  0.9411,  1.2316, -0.7922]])\n",
      "target tensor([0., 0., 0., 0., 0., 0.])\n",
      "mask_tensor tensor([ True, False, False, False, False, False])\n",
      "[tensor([[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]]), tensor([[ 0.7631,  0.0692,  0.1290, -0.9055,  0.9705, -1.3907]]), tensor([[-0.4765,  1.3066,  0.6974, -0.1251,  1.5011, -0.7810]]), tensor([[ 0.4167,  1.9153, -1.6339, -0.4343,  0.0440,  0.7993]]), tensor([[-1.1750,  1.5364, -0.4331,  0.9411,  1.2316, -0.7922]])]\n",
      "[tensor([0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0.])]\n",
      "[tensor([ True,  True,  True,  True, False, False]), tensor([ True,  True,  True,  True,  True, False]), tensor([ True,  True,  True,  True, False, False]), tensor([True, True, True, True, True, True]), tensor([ True, False, False, False, False, False])]\n",
      "logit tensor([[ 0.9767, -1.0743,  0.4011, -1.1673, -0.7503,  0.8902]])\n",
      "target tensor([0., 0., 0., 0., 0., 0.])\n",
      "mask_tensor tensor([False, False, False, False, False, False])\n",
      "[tensor([[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]]), tensor([[ 0.7631,  0.0692,  0.1290, -0.9055,  0.9705, -1.3907]]), tensor([[-0.4765,  1.3066,  0.6974, -0.1251,  1.5011, -0.7810]]), tensor([[ 0.4167,  1.9153, -1.6339, -0.4343,  0.0440,  0.7993]]), tensor([[-1.1750,  1.5364, -0.4331,  0.9411,  1.2316, -0.7922]]), tensor([[ 0.9767, -1.0743,  0.4011, -1.1673, -0.7503,  0.8902]])]\n",
      "[tensor([0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0.])]\n",
      "[tensor([ True,  True,  True,  True, False, False]), tensor([ True,  True,  True,  True,  True, False]), tensor([ True,  True,  True,  True, False, False]), tensor([True, True, True, True, True, True]), tensor([ True, False, False, False, False, False]), tensor([False, False, False, False, False, False])]\n",
      "logit tensor([[-0.7026, -0.7585,  0.1373, -0.2633, -0.0694,  0.2566]])\n",
      "target tensor([1., 0., 1., 0., 1., 0.])\n",
      "mask_tensor tensor([True, True, True, True, True, True])\n",
      "[tensor([[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]]), tensor([[ 0.7631,  0.0692,  0.1290, -0.9055,  0.9705, -1.3907]]), tensor([[-0.4765,  1.3066,  0.6974, -0.1251,  1.5011, -0.7810]]), tensor([[ 0.4167,  1.9153, -1.6339, -0.4343,  0.0440,  0.7993]]), tensor([[-1.1750,  1.5364, -0.4331,  0.9411,  1.2316, -0.7922]]), tensor([[ 0.9767, -1.0743,  0.4011, -1.1673, -0.7503,  0.8902]]), tensor([[-0.7026, -0.7585,  0.1373, -0.2633, -0.0694,  0.2566]])]\n",
      "[tensor([0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0.]), tensor([1., 0., 1., 0., 1., 0.])]\n",
      "[tensor([ True,  True,  True,  True, False, False]), tensor([ True,  True,  True,  True,  True, False]), tensor([ True,  True,  True,  True, False, False]), tensor([True, True, True, True, True, True]), tensor([ True, False, False, False, False, False]), tensor([False, False, False, False, False, False]), tensor([True, True, True, True, True, True])]\n",
      "torch.Size([7, 1, 6]) torch.Size([7, 6]) torch.Size([7, 6])\n",
      "7 4.618533492088318\n",
      "## 33 ## torch.Size([7, 1, 6]) torch.Size([7, 6]) torch.Size([7, 6])\n",
      "## 33 ## 1 1 1\n",
      "#########\n",
      "[tensor([7])] tensor([7])\n",
      "valid_preds [tensor([[[ 0.6554,  1.6408,  0.5262, -0.8042,  1.4421, -0.1529]],\n",
      "\n",
      "        [[ 0.7631,  0.0692,  0.1290, -0.9055,  0.9705, -1.3907]],\n",
      "\n",
      "        [[-0.4765,  1.3066,  0.6974, -0.1251,  1.5011, -0.7810]],\n",
      "\n",
      "        [[ 0.4167,  1.9153, -1.6339, -0.4343,  0.0440,  0.7993]],\n",
      "\n",
      "        [[-1.1750,  1.5364, -0.4331,  0.9411,  1.2316, -0.7922]],\n",
      "\n",
      "        [[ 0.9767, -1.0743,  0.4011, -1.1673, -0.7503,  0.8902]],\n",
      "\n",
      "        [[-0.7026, -0.7585,  0.1373, -0.2633, -0.0694,  0.2566]]])]\n",
      "valid_targets [tensor([[0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 1., 0.]])]\n",
      "valid_masks [tensor([[ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])]\n",
      "all_preds [[[ 0.6554077   1.640792    0.52616644 -0.80419254  1.4421139\n",
      "   -0.15286307]]\n",
      "\n",
      " [[ 0.763116    0.06922628  0.12902844 -0.905452    0.97053975\n",
      "   -1.390734  ]]\n",
      "\n",
      " [[-0.4764529   1.306595    0.6973835  -0.12508301  1.5011117\n",
      "   -0.7809909 ]]\n",
      "\n",
      " [[ 0.41668913  1.915283   -1.633898   -0.4342603   0.04396879\n",
      "    0.79926604]]\n",
      "\n",
      " [[-1.1750023   1.5363787  -0.43305933  0.94114196  1.2315835\n",
      "   -0.7921769 ]]\n",
      "\n",
      " [[ 0.9766892  -1.0743153   0.40108374 -1.1672862  -0.75030684\n",
      "    0.8901924 ]]\n",
      "\n",
      " [[-0.70259595 -0.758531    0.13733959 -0.26329172 -0.06935074\n",
      "    0.25660178]]]\n",
      "all_targets [[0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 0.]]\n",
      "all_masks [[ True  True  True  True False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True False False]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True False False False False False]\n",
      " [False False False False False False]\n",
      " [ True  True  True  True  True  True]]\n",
      "[ True  True  True  True  True False  True]\n",
      "task_targets [0. 0. 0. 0. 0. 1.]\n",
      "task_targets__ 2\n",
      "[ True  True  True  True False False  True]\n",
      "task_targets [0. 0. 1. 0. 0.]\n",
      "task_targets__ 2\n",
      "[ True  True  True  True False False  True]\n",
      "task_targets [1. 0. 0. 1. 1.]\n",
      "task_targets__ 2\n",
      "[ True  True  True  True False False  True]\n",
      "task_targets [0. 1. 0. 0. 0.]\n",
      "task_targets__ 2\n",
      "[False  True False  True False False  True]\n",
      "task_targets [0. 0. 1.]\n",
      "task_targets__ 2\n",
      "[False False False  True False False  True]\n",
      "task_targets [1. 0.]\n",
      "task_targets__ 2\n",
      "{'samples_evaluated': 7, 'avg_loss': 0.6597905158996582, 'auc_task0': 0.19999999999999996, 'acc_task0': 0.5, 'auc_task1': 0.5, 'acc_task1': 0.8, 'auc_task2': 0.3333333333333333, 'acc_task2': 0.8, 'auc_task3': 0.0, 'acc_task3': 0.0, 'auc_task4': 0.0, 'acc_task4': 0.6666666666666666, 'auc_task5': 1.0, 'acc_task5': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Stub-out torch.distributed for single-process testing\n",
    "# ------------------------------------------------------------------\n",
    "class _FakeDist:\n",
    "    def get_world_size(self): return 1\n",
    "    def all_reduce(self, *_, **__): pass\n",
    "    def all_gather(self, out_list, tensor): out_list[0].copy_(tensor)\n",
    "    def broadcast_object_list(self, *_, **__): pass\n",
    "import types, torch.distributed as dist\n",
    "for fn in [\"get_world_size\", \"all_reduce\", \"all_gather\", \"broadcast_object_list\"]:\n",
    "    setattr(dist, fn, getattr(_FakeDist(), fn))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Minimal dummy model that outputs a 1×6 logit vector\n",
    "# ------------------------------------------------------------------\n",
    "class DummyModel(torch.nn.Module):\n",
    "    def __init__(self, num_tasks=6):\n",
    "        super().__init__()\n",
    "        self.num_tasks = num_tasks\n",
    "    def forward(self, x, spacing=None):\n",
    "        B = 1          # your code slices to [1, tasks]\n",
    "        return torch.randn(B, self.num_tasks, device=x.device)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Build a fake val_loader producing 8 samples\n",
    "# ------------------------------------------------------------------\n",
    "def make_sample(target_vec):\n",
    "    \"\"\"\n",
    "    target_vec: list/array of length 6 containing {0,1,-1,2}\n",
    "    returns (chunks, labels, mask, spacing)\n",
    "    \"\"\"\n",
    "    chunks  = torch.randn(3, 3, 448, 448)          # [D=3, 3, 448, 448]\n",
    "    labels  = np.array(target_vec, dtype=np.float32)\n",
    "    mask    = labels != -1                         # False where label == -1\n",
    "    labels[labels == -1] = 0                       # placeholder, never used when mask False\n",
    "    spacing = 1.0\n",
    "    return chunks, labels, mask, spacing\n",
    "\n",
    "val_samples = [\n",
    "    make_sample([0, 0, 1, 0, -1, -1]),\n",
    "    make_sample([0, 0, 0, 1, 0,  -1]),\n",
    "    make_sample([0, 1, 0, 0, -1, -1]),\n",
    "    make_sample([0, 0, 1, 0, 0,   1]),\n",
    "    make_sample([0, -1,-1,-1,-1, -1]),\n",
    "    make_sample([-1,-1,-1,-1,-1, -1]),    # all missing\n",
    "    make_sample([1, 0, 1, 0, 1,  0]),     # <- has a spurious \"2\" for task2\n",
    "]\n",
    "\n",
    "val_loader = DataLoader(val_samples, batch_size=1, shuffle=False, collate_fn=lambda b: b[0])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Minimal trainer holding only the pieces evaluate() needs\n",
    "# ------------------------------------------------------------------\n",
    "class MiniTrainer:\n",
    "    def __init__(self):\n",
    "        self.args = SimpleNamespace(max_chunks=4, rank=0)\n",
    "        self.device = torch.device('cpu')\n",
    "        self.model  = DummyModel().to(self.device)\n",
    "        self.crit   = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.label_cols = [f\"task{i}\" for i in range(6)]\n",
    "    # --- paste your evaluate() here (no DDP ops will explode) ---\n",
    "    def evaluate(self, val_loader):\n",
    "        \"\"\"Evaluate model on validation set using distributed evaluation\"\"\"\n",
    "        self.model.eval()\n",
    "        local_loss_sum = 0.0\n",
    "        local_samples = 0\n",
    "        local_preds = []\n",
    "        local_targets = []\n",
    "        local_masks = []\n",
    "        \n",
    "        # Process validation samples assigned to this rank\n",
    "        for i, (chunks, labels, mask, spacing) in enumerate(val_loader):\n",
    "            chunks = chunks.squeeze(1).to(self.device)\n",
    "            \n",
    "            # Apply max_chunks constraint\n",
    "            if chunks.size(0) > self.args.max_chunks:\n",
    "                mid_idx = chunks.size(0) // 2\n",
    "                start_idx = max(0, mid_idx - self.args.max_chunks // 2)\n",
    "                end_idx = min(chunks.size(0), start_idx + self.args.max_chunks)\n",
    "                chunks = chunks[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = self.model(chunks, spacing)\n",
    "            \n",
    "            # Calculate loss\n",
    "            target = torch.tensor(labels, dtype=torch.float32, device=self.device)\n",
    "            mask_tensor = torch.tensor(mask, dtype=torch.bool, device=self.device)\n",
    "            #print(chunks.shape, logits.shape, target.shape, mask_tensor.shape)#torch.Size([max_chunks, 3, 448, 448]) torch.Size([1, 6]) torch.Size([6]) torch.Size([6])\n",
    "            print(\"logit\" ,logits )#logit tensor([[ 1.0563,  0.4275,  0.4167,  0.1464, -0.3098, -1.9752]],device='cuda:3')\n",
    "            print(\"target\" ,target)#target tensor([ 0., -1., -1., -1., -1., -1.], device='cuda:3')\n",
    "            print(\"mask_tensor\" ,mask_tensor)#mask_tensor tensor([ True, False, False, False, False, False], device='cuda:3')\n",
    "            loss = 0.0\n",
    "            for j in range(logits.size(1)):\n",
    "                if mask_tensor[j]:\n",
    "                    task_loss = self.crit(logits[0, j:j+1], target[j:j+1])\n",
    "                    # Mori:the weight in loss is probably not properly applied! \n",
    "                    # Maybe later I can change logit where mask is False to 0.0. Actually in training and not here. \n",
    "                    loss += task_loss\n",
    "            \n",
    "            # Normalize loss\n",
    "            active_tasks = mask_tensor.sum().item()\n",
    "            if active_tasks > 0:\n",
    "                loss = loss / active_tasks\n",
    "                local_loss_sum += loss.item()\n",
    "            \n",
    "            # Store predictions, targets, and masks\n",
    "            local_preds.append(logits.cpu())\n",
    "            local_targets.append(torch.tensor(labels, dtype=torch.float32))\n",
    "            local_masks.append(torch.tensor(mask, dtype=torch.bool))\n",
    "            \n",
    "            local_samples += 1\n",
    "            print(local_preds)\n",
    "            print(local_targets)\n",
    "            print(local_masks)\n",
    "        \n",
    "        # Convert to tensors for gathering\n",
    "        if local_preds:\n",
    "            # Use stack for all tensors to ensure consistent [S, T] shape\n",
    "            local_preds_tensor = torch.stack(local_preds, dim=0)\n",
    "            local_targets_tensor = torch.stack(local_targets, dim=0)  \n",
    "            local_masks_tensor = torch.stack(local_masks, dim=0)\n",
    "        else:\n",
    "            # Handle edge case where a rank might not get any validation samples\n",
    "            num_tasks = len(self.label_cols) \n",
    "            local_preds_tensor = torch.zeros((0, num_tasks), dtype=torch.float32)\n",
    "            local_targets_tensor = torch.zeros((0, num_tasks), dtype=torch.float32)\n",
    "            local_masks_tensor = torch.zeros((0, num_tasks), dtype=torch.bool)\n",
    "\n",
    "        \n",
    "        print(local_preds_tensor.shape, local_targets_tensor.shape, local_masks_tensor.shape)\n",
    "        #torch.Size([7, 1, 6]) torch.Size([7, 6]) torch.Size([7, 6])\n",
    "        # Move tensors to device for all_gather\n",
    "        local_preds_tensor = local_preds_tensor.to(self.device)\n",
    "        local_targets_tensor = local_targets_tensor.to(self.device)\n",
    "        local_masks_tensor = local_masks_tensor.to(self.device)\n",
    "        \n",
    "        # Compute total loss across all ranks\n",
    "        total_samples = torch.tensor([local_samples], dtype=torch.float32, device=self.device)\n",
    "        total_loss_sum = torch.tensor([local_loss_sum], dtype=torch.float32, device=self.device)\n",
    "        print(local_samples, local_loss_sum)#7 4.781846523284912\n",
    "        \n",
    "        dist.all_reduce(total_samples, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(total_loss_sum, op=dist.ReduceOp.SUM)\n",
    "        \n",
    "        avg_loss = total_loss_sum.item() / max(1, total_samples.item())\n",
    "        total_samples = int(total_samples.item())\n",
    "        \n",
    "        # Initialize metrics\n",
    "        metrics = {\n",
    "            'samples_evaluated': total_samples,\n",
    "            'avg_loss': avg_loss,\n",
    "        }\n",
    "        \n",
    "        # Get world size for gathering\n",
    "        world_size = dist.get_world_size()\n",
    "        \n",
    "        # Padding tensors to the same size for all_gather\n",
    "        # First find the maximum size across all ranks\n",
    "        local_size = torch.tensor([local_preds_tensor.shape[0]], dtype=torch.long, device=self.device)\n",
    "        all_sizes = [torch.ones(1, dtype=torch.long, device=self.device) for _ in range(world_size)]\n",
    "        dist.all_gather(all_sizes, local_size)\n",
    "        max_size = max([size.item() for size in all_sizes])\n",
    "        \n",
    "        \n",
    "        # Pad local tensors to max_size\n",
    "        num_tasks = len(self.label_cols)\n",
    "        if local_preds_tensor.shape[0] < max_size:\n",
    "            padding_size = max_size - local_preds_tensor.shape[0]\n",
    "            # Pad with zeros\n",
    "            preds_padding = torch.zeros((padding_size, num_tasks), dtype=torch.float32, device=self.device)\n",
    "            targets_padding = torch.zeros((padding_size, num_tasks), dtype=torch.float32, device=self.device)\n",
    "            masks_padding = torch.zeros((padding_size, num_tasks), dtype=torch.bool, device=self.device)\n",
    "            \n",
    "            # Concatenate original tensor with padding\n",
    "            local_preds_tensor = torch.cat([local_preds_tensor, preds_padding], dim=0)\n",
    "            local_targets_tensor = torch.cat([local_targets_tensor, targets_padding], dim=0)\n",
    "            local_masks_tensor = torch.cat([local_masks_tensor, masks_padding], dim=0)\n",
    "        \n",
    "        # Use all_gather for efficient collection on GPU\n",
    "        gathered_preds = [torch.empty_like(local_preds_tensor) for _ in range(world_size)]\n",
    "        gathered_targets = [torch.empty_like(local_targets_tensor) for _ in range(world_size)]\n",
    "        gathered_masks = [torch.empty_like(local_masks_tensor) for _ in range(world_size)]\n",
    "        \n",
    "        # Gather data from all ranks\n",
    "        dist.all_gather(gathered_preds, local_preds_tensor)\n",
    "        dist.all_gather(gathered_targets, local_targets_tensor)\n",
    "        dist.all_gather(gathered_masks, local_masks_tensor)\n",
    "        \n",
    "        print(\"## 33 ##\", gathered_preds[0].shape, gathered_targets[0].shape, gathered_masks[0].shape)\n",
    "        # torch.Size([7, 1, 6]) torch.Size([7, 6]) torch.Size([7, 6])\n",
    "        print(\"## 33 ##\", len(gathered_preds), len(gathered_targets), len(gathered_masks))\n",
    "        # 1 1 1\n",
    "        print(\"#########\")\n",
    "        print(all_sizes, local_size) # [tensor([7])] tensor([7])\n",
    "        # Remove padding and convert to numpy for metric calculation\n",
    "        valid_preds = []\n",
    "        valid_targets = []\n",
    "        valid_masks = []\n",
    "        \n",
    "        for i, size in enumerate([size.item() for size in all_sizes]):\n",
    "            if size > 0:\n",
    "                valid_preds.append(gathered_preds[i][:size])\n",
    "                valid_targets.append(gathered_targets[i][:size])\n",
    "                valid_masks.append(gathered_masks[i][:size])\n",
    "        print(\"valid_preds\", valid_preds)\n",
    "        print(\"valid_targets\", valid_targets)\n",
    "        print(\"valid_masks\", valid_masks)\n",
    "        # Combine all gathered data\n",
    "        all_preds = torch.cat(valid_preds, dim=0).cpu().numpy()\n",
    "        all_targets = torch.cat(valid_targets, dim=0).cpu().numpy()\n",
    "        all_masks = torch.cat(valid_masks, dim=0).cpu().numpy()\n",
    "\n",
    "        print(\"all_preds\", all_preds) #all_preds (7, 1, 6)\n",
    "        print(\"all_targets\", all_targets)#all_targets (7, 6)\n",
    "        print(\"all_masks\", all_masks)#all_masks (7, 6)\n",
    "        all_preds = torch.cat(valid_preds, dim=0).squeeze(1).cpu().numpy()\n",
    "        #print(all_preds.shape) #(7, 1, 6)\n",
    "        for i in range(all_preds.shape[1]):\n",
    "            valid_idx = all_masks[:, i] #gather all of the ith pred indices that not masked\n",
    "            print(valid_idx)\n",
    "            if valid_idx.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            task_preds   = all_preds[valid_idx, i]\n",
    "            task_targets = all_targets[valid_idx, i]\n",
    "\n",
    "            # 1️⃣ keep only entries where target is 0 or 1\n",
    "            keep = (task_targets == 0) | (task_targets == 1)\n",
    "            task_preds, task_targets = task_preds[keep], task_targets[keep]\n",
    "\n",
    "            # 2️⃣ debug: if anything was dropped, print the unique values once\n",
    "            if self.args.rank == 0 and np.unique(task_targets).size < np.unique(all_targets[valid_idx, i]).size:\n",
    "                bad_vals = set(all_targets[valid_idx, i]) - {0, 1}\n",
    "                print(f\"[WARN] Task {i}: dropped labels {bad_vals}\")\n",
    "            print(\"task_targets\", task_targets)\n",
    "            print(\"task_targets__\", np.unique(task_targets).size)\n",
    "            # 3️⃣ need both classes left to compute AUC\n",
    "            if np.unique(task_targets).size < 2:\n",
    "                metrics[f'auc_task{i}'] = np.nan\n",
    "                metrics[f'acc_task{i}'] = np.nan\n",
    "                continue\n",
    "\n",
    "            # --- DEBUG BLOCK ----------------------------------------------------\n",
    "            bad_vals = np.setdiff1d(task_targets, [0, 1])\n",
    "            if bad_vals.size > 0:\n",
    "                # Print once per task per validation\n",
    "                print(f\"[Rank {self.args.rank}] Task {i} – bad labels detected: {bad_vals}\")\n",
    "                # Optional: locate the offending rows (expensive, so enable only if needed)\n",
    "                bad_rows = np.where(~np.isin(task_targets, [0, 1]))[0]\n",
    "                print(f\"Bad rows (local indices): {bad_rows[:20]} ...\")\n",
    "                # You could also log IDs if your dataset returns them.\n",
    "\n",
    "                # Drop the invalid entries so metrics still compute\n",
    "                keep = np.isin(task_targets, [0, 1])\n",
    "                task_preds, task_targets = task_preds[keep], task_targets[keep]\n",
    "            # --------------------------------------------------------------------\n",
    "            # 4️⃣ metrics\n",
    "            auc = roc_auc_score(task_targets, task_preds)\n",
    "            metrics[f'auc_task{i}'] = auc\n",
    "\n",
    "            prob = 1 / (1 + np.exp(-task_preds))\n",
    "            acc  = (prob > 0.5).astype(int).mean()\n",
    "            metrics[f'acc_task{i}'] = acc\n",
    "        print(metrics)\n",
    "trainer = MiniTrainer()\n",
    "trainer.evaluate(val_loader)      # <-- runs end-to-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44e60837-0d3b-4850-ac62-38cf97cad231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(7)]\n",
      "0 7\n"
     ]
    }
   ],
   "source": [
    "all_sizes = [torch.tensor(7)] #[tensor([7])]\n",
    "print(all_sizes)\n",
    "for i, size in enumerate([size.item() for size in all_sizes]):\n",
    "    print(i, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221d0b2-0a5f-42bd-b803-90a290660406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msalehjahromi (py3.10.12)",
   "language": "python",
   "name": "msalehjahromi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
