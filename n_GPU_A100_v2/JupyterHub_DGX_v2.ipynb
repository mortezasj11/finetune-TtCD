{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2aed02-3d9c-4f45-b9fd-92d71e1238b6",
   "metadata": {},
   "source": [
    "#  DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22ab11-89d3-4805-9cff-b2ee1542fb68",
   "metadata": {},
   "source": [
    "### 1. running on our DGX\n",
    "- docker run -it --rm --gpus '\"device=4,5,6,7\"' -p 12344:12344 --name msalehjahromi --shm-size=192G  --user $(id -u):$(id -g) --group-add 1944259512 --cpuset-cpus=49-96 -v /rsrch7/home/ip_rsrch/wulab/:/rsrch7/home/ip_rsrch/wulab -v /rsrch1/ip/msalehjahromi/:/rsrch1/ip/msalehjahromi --name mori_jupyter nnunetv2:msalehjahromi\n",
    "- cd /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100\n",
    "- jupyter notebook --ip 0.0.0.0 --port 12344\n",
    "- http://1mcprddgx05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14ac30-7017-45e1-a689-710d830d9f36",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb4b0a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mjupyter-msalehjahromi   \u001b[m  Tue Jun 10 13:41:56 2025  \u001b[1m\u001b[30m550.54.15\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  637\u001b[m / \u001b[33m40960\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 31'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  637\u001b[m / \u001b[33m40960\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 31'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  637\u001b[m / \u001b[33m40960\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 45'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  637\u001b[m / \u001b[33m40960\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f668c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.device_count())   # → 2\n",
    "print(torch.cuda.current_device()) # → 0 (this maps to your original GPU 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5180fba",
   "metadata": {},
   "source": [
    "### 2. Probably need installation, restart after and run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6f2e39-b110-4be8-82bd-5443f49ca638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pip', 'install', '-q', '--extra-index-url', 'https://download.pytorch.org/whl/cu117', 'torch==2.0.0', 'torchvision==0.15.0', 'omegaconf', 'torchmetrics==0.10.3', 'fvcore', 'iopath', 'xformers==0.0.18', 'submitit', 'numpy<2.0']\n",
      "['pip', 'install', '-q', '--extra-index-url', 'https://pypi.nvidia.com', 'cuml-cu11']\n",
      "['pip', 'install', '-q', 'black==22.6.0', 'flake8==5.0.4', 'pylint==2.15.0']\n",
      "['pip', 'install', '-q', 'mmsegmentation==0.27.0']\n",
      "['pip', 'install', '-q', 'mmcv-full==1.5.0']\n",
      "['pip', 'install', '-q', 'nibabel']\n"
     ]
    }
   ],
   "source": [
    "## !pip uninstall -q histolab -y\n",
    "# Install required packages first\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "pip_commands = [\n",
    "    [\"pip\", \"install\",\"-q\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\", \n",
    "     \"torch==2.0.0\", \"torchvision==0.15.0\", \"omegaconf\", \"torchmetrics==0.10.3\", \n",
    "     \"fvcore\", \"iopath\", \"xformers==0.0.18\", \"submitit\", \"numpy<2.0\"],\n",
    "    [\"pip\", \"install\", \"-q\",  \"--extra-index-url\", \"https://pypi.nvidia.com\", \"cuml-cu11\"],\n",
    "    [\"pip\", \"install\",\"-q\",  \"black==22.6.0\", \"flake8==5.0.4\", \"pylint==2.15.0\"],\n",
    "    [\"pip\", \"install\", \"-q\", \"mmsegmentation==0.27.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"mmcv-full==1.5.0\"],\n",
    "    [\"pip\", \"install\",\"-q\", \"nibabel\"]\n",
    "]\n",
    "\n",
    "for cmd in pip_commands:\n",
    "    try:\n",
    "        print(cmd)\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install packages with command: {cmd}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b58805",
   "metadata": {},
   "source": [
    "### 3. running, notice important hyperparams\n",
    "-  which model are use so vmin -1000., vmax +150, eps 0.00005\n",
    "- `num-workers`\n",
    "- `epochs`: 100\n",
    "- `accum-steps`: 2000 ?\n",
    "- `max-chunks`: 66 \n",
    "- `lr`:  lr*0.1 on base ?\n",
    "- `warmup-steps`: 5k\n",
    "- `print-every` : 5000\n",
    "- `val-every` : 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "US1_J2KR.dcm:   0%|          | 38.0/154k [00:00<00:38, 3.97kB/s]\n",
      "US1_J2KR.dcm:   0%|          | 38.0/154k [00:00<00:41, 3.67kB/s]\n",
      "US1_J2KR.dcm:   0%|          | 38.0/154k [00:00<00:44, 3.48kB/s]\n",
      "US1_J2KR.dcm:   0%|          | 38.0/154k [00:00<00:43, 3.57kB/s]\n",
      "MR-SIEMENS-DICOM-WithOverlays.dcm:   0%|          | 125/511k [00:00<01:18, 6.54kB/s]\n",
      "MR-SIEMENS-DICOM-WithOverlays.dcm:   0%|          | 125/511k [00:00<01:33, 5.46kB/s]\n",
      "MR-SIEMENS-DICOM-WithOverlays.dcm:   0%|          | 125/511k [00:00<01:30, 5.62kB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/pydicom/data/data_manager.py:375: UserWarning: A download failure occurred while attempting to retrieve MR-SIEMENS-DICOM-WithOverlays.dcm\n",
      "  warn_and_log(\n",
      "/usr/local/lib/python3.10/dist-packages/pydicom/data/data_manager.py:375: UserWarning: A download failure occurred while attempting to retrieve MR-SIEMENS-DICOM-WithOverlays.dcm\n",
      "  warn_and_log(\n",
      "MR-SIEMENS-DICOM-WithOverlays.dcm:   0%|          | 125/511k [00:00<01:29, 5.72kB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/pydicom/data/data_manager.py:375: UserWarning: A download failure occurred while attempting to retrieve MR-SIEMENS-DICOM-WithOverlays.dcm\n",
      "  warn_and_log(\n",
      "OBXXXX1A.dcm:   0%|          | 119/486k [00:00<01:19, 6.11kB/s]\n",
      "OBXXXX1A.dcm:   0%|          | 119/486k [00:00<01:25, 5.68kB/s]\n",
      "OBXXXX1A.dcm:   0%|          | 119/486k [00:00<01:31, 5.34kB/s]\n",
      "MR-SIEMENS-DICOM-WithOverlays.dcm:   0%|          | 125/511k [00:00<01:21, 6.27kB/s]\n",
      "US1_UNCR.dcm:   0%|          | 226/923k [00:00<01:45, 8.72kB/s]\n",
      "US1_UNCR.dcm:   0%|          | 226/923k [00:00<01:47, 8.58kB/s]\n",
      "US1_UNCR.dcm:   0%|          | 226/923k [00:00<01:52, 8.17kB/s]\n",
      "US1_UNCR.dcm:   0%|          | 226/923k [00:00<01:56, 7.91kB/s]\n",
      "color3d_jpeg_baseline.dcm:   0%|          | 1.50k/6.14M [00:00<04:25, 23.1kB/s]\n",
      "color3d_jpeg_baseline.dcm:   0%|          | 1.50k/6.14M [00:00<04:15, 24.0kB/s]\n",
      "color3d_jpeg_baseline.dcm:   0%|          | 1.50k/6.14M [00:00<04:16, 23.9kB/s]\n",
      "color3d_jpeg_baseline.dcm:   0%|          | 1.50k/6.14M [00:00<04:07, 24.8kB/s]\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['n_GPU_A100_v2', '._.gitignore', '._.DS_Store', '._See_attention.ipynb', '1_GPU_H100', 'See_attention.ipynb', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['n_GPU_A100_v2', '._.gitignore', '._.DS_Store', '._See_attention.ipynb', '1_GPU_H100', 'See_attention.ipynb', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['n_GPU_A100_v2', '._.gitignore', '._.DS_Store', '._See_attention.ipynb', '1_GPU_H100', 'See_attention.ipynb', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['n_GPU_A100_v2', '._.gitignore', '._.DS_Store', '._See_attention.ipynb', '1_GPU_H100', 'See_attention.ipynb', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 13:43:20,033 - INFO - Starting training on 4 GPUs with full model copies (DDP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset created on rank \n",
      "Train dataset created on rank \n",
      "Train dataset created on rank Train dataset created on rank \n",
      "\n",
      "\n",
      "==== Dataset Statistics ====\n",
      "Total samples: 43852\n",
      "Training samples: 38382\n",
      "Validation samples: 5470\n",
      "\n",
      "1-year-cancer:\n",
      "  Train positive: 1428 (3.72%)\n",
      "  Val positive: 188 (3.44%)\n",
      "\n",
      "2-year-cancer:\n",
      "  Train positive: 1891 (4.93%)\n",
      "  Val positive: 256 (4.68%)\n",
      "\n",
      "3-year-cancer:\n",
      "  Train positive: 2282 (5.95%)\n",
      "  Val positive: 315 (5.76%)\n",
      "\n",
      "4-year-cancer:\n",
      "  Train positive: 2586 (6.74%)\n",
      "  Val positive: 363 (6.64%)\n",
      "\n",
      "5-year-cancer:\n",
      "  Train positive: 2838 (7.39%)\n",
      "  Val positive: 403 (7.37%)\n",
      "\n",
      "6-year-cancer:\n",
      "  Train positive: 2988 (7.78%)\n",
      "  Val positive: 429 (7.84%)\n",
      "============================\n",
      "\n",
      "Total validation samples: 5470\n",
      "Created distributed validation loader, each rank processes ~1284 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 13:43:24,276 - INFO - using MLP layer as FFN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on rank \n",
      "Model loaded on rank \n",
      "Model loaded on rank \n",
      "Model loaded on rank \n",
      "[resume] no /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_JH_d10_B64/checkpoints — fresh run\n",
      "Using unweighted BCEWithLogitsLoss\n",
      "Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250610_134336.jsonl\n",
      "Will save checkpoints to /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_JH_d10_B64/checkpoints\n",
      "\n",
      "Training Configuration:\n",
      "Max chunks per sample: 64\n",
      "Learning rate: 0.0001\n",
      "Gradient accumulation steps: 500\n",
      "Number of tasks: 6\n",
      "Validation frequency: 41000 steps\n",
      "Number of epochs: 100\n",
      "Warmup steps: 10\n",
      "World size: 4\n",
      "Device: cuda\n",
      "\n",
      "Epoch 1: All parameters already unfrozen (strategy: all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_full.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_full.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_full.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_full.py:134: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "2025-06-10 13:43:36,815 - INFO - ------ Training Configuration ------\n",
      "2025-06-10 13:43:36,815 - INFO - Max chunks per sample         : 64\n",
      "2025-06-10 13:43:36,815 - INFO - Learning rate (aggregator)    : 0.0001\n",
      "2025-06-10 13:43:36,815 - INFO - Learning rate (base)          : 1.0000000000000002e-06\n",
      "2025-06-10 13:43:36,815 - INFO - Gradient accumulation steps   : 500\n",
      "2025-06-10 13:43:36,815 - INFO - Warm-up steps                 : 10\n",
      "2025-06-10 13:43:36,815 - INFO - Aggregator layers / heads     : 2 / 3\n",
      "2025-06-10 13:43:36,815 - INFO - Aggregator dropout            : 0.35\n",
      "2025-06-10 13:43:36,815 - INFO - Validation frequency          : 41000 steps\n",
      "2025-06-10 13:43:36,815 - INFO - Number of epochs              : 100\n",
      "2025-06-10 13:43:36,815 - INFO - World size                    : 4\n",
      "2025-06-10 13:43:36,815 - INFO - Device                        : cuda\n",
      "2025-06-10 13:43:36,815 - INFO - ------------------------------------\n",
      "\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2000 (Epoch 1):\n",
      "Loss: 1.3478\n",
      "Learning rate: 0.000044\n",
      "\n",
      "Step    2000  Ep 01  loss 1.3478  lr 0.000044  fetch    0.5 ms  compute 1188.5 ms\n",
      "Accuracy:     T0:0.51  T1:0.50  T2:0.51  T3:0.52  T4:0.40  T5:0.73  \n",
      "Sensitivity:  T0:0.00  T1:0.00  T2:1.00  T3:1.00  T4:0.02  T5:0.99  \n",
      "Specificity:  T0:1.00  T1:1.00  T2:0.00  T3:0.00  T4:0.99  T5:0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:22:52,364 - INFO - Step    2000  Ep 01  loss 1.3478  lr 0.000044  fetch    0.5 ms  compute 1188.5 ms\n",
      "2025-06-10 14:22:52,365 - INFO - Accuracy:     T0:0.51  T1:0.50  T2:0.51  T3:0.52  T4:0.40  T5:0.73  \n",
      "2025-06-10 14:22:52,365 - INFO - Sensitivity:  T0:0.00  T1:0.00  T2:1.00  T3:1.00  T4:0.02  T5:0.99  \n",
      "2025-06-10 14:22:52,365 - INFO - Specificity:  T0:1.00  T1:1.00  T2:0.00  T3:0.00  T4:0.99  T5:0.01  \n",
      "2025-06-10 14:22:52,366 - INFO - iteration: 2000 | epoch: 0 | loss: 1.3478 | lr: 0.0000 | type: training | acc_task0: 0.5145 | sens_task0: 0.0000 | spec_task0: 0.9990 | acc_task1: 0.5048 | sens_task1: 0.0000 | spec_task1: 1.0000 | acc_task2: 0.5082 | sens_task2: 1.0000 | spec_task2: 0.0000 | acc_task3: 0.5202 | sens_task3: 1.0000 | spec_task3: 0.0000 | acc_task4: 0.3989 | sens_task4: 0.0189 | spec_task4: 0.9862 | acc_task5: 0.7299 | sens_task5: 0.9921 | spec_task5: 0.0108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3000 (Epoch 1):\n",
      "Loss: 0.8921\n",
      "Learning rate: 0.000076\n",
      "\n",
      "Step    3000  Ep 01  loss 0.8921  lr 0.000076  fetch    0.5 ms  compute 1264.6 ms\n",
      "Accuracy:     T0:0.50  T1:0.49  T2:0.52  T3:0.54  T4:0.41  T5:0.74  \n",
      "Sensitivity:  T0:0.00  T1:0.00  T2:1.00  T3:1.00  T4:0.10  T5:0.99  \n",
      "Specificity:  T0:1.00  T1:1.00  T2:0.00  T3:0.00  T4:0.91  T5:0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 14:43:29,839 - INFO - Step    3000  Ep 01  loss 0.8921  lr 0.000076  fetch    0.5 ms  compute 1264.6 ms\n",
      "2025-06-10 14:43:29,840 - INFO - Accuracy:     T0:0.50  T1:0.49  T2:0.52  T3:0.54  T4:0.41  T5:0.74  \n",
      "2025-06-10 14:43:29,840 - INFO - Sensitivity:  T0:0.00  T1:0.00  T2:1.00  T3:1.00  T4:0.10  T5:0.99  \n",
      "2025-06-10 14:43:29,840 - INFO - Specificity:  T0:1.00  T1:1.00  T2:0.00  T3:0.00  T4:0.91  T5:0.01  \n",
      "2025-06-10 14:43:29,840 - INFO - iteration: 3000 | epoch: 0 | loss: 0.8921 | lr: 0.0001 | type: training | acc_task0: 0.5017 | sens_task0: 0.0027 | spec_task0: 0.9980 | acc_task1: 0.4906 | sens_task1: 0.0033 | spec_task1: 0.9986 | acc_task2: 0.5230 | sens_task2: 1.0000 | spec_task2: 0.0000 | acc_task3: 0.5350 | sens_task3: 1.0000 | spec_task3: 0.0000 | acc_task4: 0.4085 | sens_task4: 0.0981 | spec_task4: 0.9094 | acc_task5: 0.7441 | sens_task5: 0.9942 | spec_task5: 0.0094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4000 (Epoch 1):\n",
      "Loss: 0.7495\n",
      "Learning rate: 0.000097\n",
      "\n",
      "Step    4000  Ep 01  loss 0.7495  lr 0.000097  fetch    0.5 ms  compute 1167.8 ms\n",
      "Accuracy:     T0:0.49  T1:0.49  T2:0.53  T3:0.54  T4:0.45  T5:0.75  \n",
      "Sensitivity:  T0:0.04  T1:0.12  T2:0.92  T3:0.99  T4:0.29  T5:1.00  \n",
      "Specificity:  T0:0.96  T1:0.90  T2:0.09  T3:0.01  T4:0.71  T5:0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 15:03:48,945 - INFO - Step    4000  Ep 01  loss 0.7495  lr 0.000097  fetch    0.5 ms  compute 1167.8 ms\n",
      "2025-06-10 15:03:48,946 - INFO - Accuracy:     T0:0.49  T1:0.49  T2:0.53  T3:0.54  T4:0.45  T5:0.75  \n",
      "2025-06-10 15:03:48,946 - INFO - Sensitivity:  T0:0.04  T1:0.12  T2:0.92  T3:0.99  T4:0.29  T5:1.00  \n",
      "2025-06-10 15:03:48,946 - INFO - Specificity:  T0:0.96  T1:0.90  T2:0.09  T3:0.01  T4:0.71  T5:0.01  \n",
      "2025-06-10 15:03:48,946 - INFO - iteration: 4000 | epoch: 0 | loss: 0.7495 | lr: 0.0001 | type: training | acc_task0: 0.4935 | sens_task0: 0.0411 | spec_task0: 0.9555 | acc_task1: 0.4947 | sens_task1: 0.1161 | spec_task1: 0.8991 | acc_task2: 0.5270 | sens_task2: 0.9172 | spec_task2: 0.0900 | acc_task3: 0.5400 | sens_task3: 0.9909 | spec_task3: 0.0102 | acc_task4: 0.4484 | sens_task4: 0.2894 | spec_task4: 0.7081 | acc_task5: 0.7472 | sens_task5: 0.9957 | spec_task5: 0.0071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5000 (Epoch 1):\n",
      "Loss: 0.7407\n",
      "Learning rate: 0.000100\n",
      "\n",
      "Step    5000  Ep 01  loss 0.7407  lr 0.000100  fetch    0.4 ms  compute 1255.7 ms\n",
      "Accuracy:     T0:0.50  T1:0.50  T2:0.51  T3:0.52  T4:0.49  T5:0.75  \n",
      "Sensitivity:  T0:0.18  T1:0.25  T2:0.73  T3:0.79  T4:0.43  T5:1.00  \n",
      "Specificity:  T0:0.82  T1:0.76  T2:0.27  T3:0.20  T4:0.57  T5:0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 15:24:08,423 - INFO - Step    5000  Ep 01  loss 0.7407  lr 0.000100  fetch    0.4 ms  compute 1255.7 ms\n",
      "2025-06-10 15:24:08,423 - INFO - Accuracy:     T0:0.50  T1:0.50  T2:0.51  T3:0.52  T4:0.49  T5:0.75  \n",
      "2025-06-10 15:24:08,424 - INFO - Sensitivity:  T0:0.18  T1:0.25  T2:0.73  T3:0.79  T4:0.43  T5:1.00  \n",
      "2025-06-10 15:24:08,424 - INFO - Specificity:  T0:0.82  T1:0.76  T2:0.27  T3:0.20  T4:0.57  T5:0.01  \n",
      "2025-06-10 15:24:08,424 - INFO - iteration: 5000 | epoch: 0 | loss: 0.7407 | lr: 0.0001 | type: training | acc_task0: 0.4952 | sens_task0: 0.1802 | spec_task0: 0.8194 | acc_task1: 0.4992 | sens_task1: 0.2540 | spec_task1: 0.7631 | acc_task2: 0.5134 | sens_task2: 0.7333 | spec_task2: 0.2659 | acc_task3: 0.5226 | sens_task3: 0.7943 | spec_task3: 0.2014 | acc_task4: 0.4850 | sens_task4: 0.4335 | spec_task4: 0.5701 | acc_task5: 0.7482 | sens_task5: 0.9966 | spec_task5: 0.0057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6000 (Epoch 1):\n",
      "Loss: 0.8358\n",
      "Learning rate: 0.000100\n",
      "\n",
      "Step    6000  Ep 01  loss 0.8358  lr 0.000100  fetch    1.1 ms  compute 1228.1 ms\n",
      "Accuracy:     T0:0.50  T1:0.51  T2:0.51  T3:0.51  T4:0.50  T5:0.75  \n",
      "Sensitivity:  T0:0.29  T1:0.30  T2:0.61  T3:0.67  T4:0.52  T5:1.00  \n",
      "Specificity:  T0:0.71  T1:0.73  T2:0.39  T3:0.34  T4:0.47  T5:0.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 15:44:57,885 - INFO - Step    6000  Ep 01  loss 0.8358  lr 0.000100  fetch    1.1 ms  compute 1228.1 ms\n",
      "2025-06-10 15:44:57,900 - INFO - Accuracy:     T0:0.50  T1:0.51  T2:0.51  T3:0.51  T4:0.50  T5:0.75  \n",
      "2025-06-10 15:44:57,900 - INFO - Sensitivity:  T0:0.29  T1:0.30  T2:0.61  T3:0.67  T4:0.52  T5:1.00  \n",
      "2025-06-10 15:44:57,901 - INFO - Specificity:  T0:0.71  T1:0.73  T2:0.39  T3:0.34  T4:0.47  T5:0.00  \n",
      "2025-06-10 15:44:57,901 - INFO - iteration: 6000 | epoch: 0 | loss: 0.8358 | lr: 0.0001 | type: training | acc_task0: 0.4968 | sens_task0: 0.2879 | spec_task0: 0.7082 | acc_task1: 0.5083 | sens_task1: 0.2985 | spec_task1: 0.7309 | acc_task2: 0.5086 | sens_task2: 0.6141 | spec_task2: 0.3910 | acc_task3: 0.5149 | sens_task3: 0.6658 | spec_task3: 0.3386 | acc_task4: 0.5043 | sens_task4: 0.5249 | spec_task4: 0.4707 | acc_task5: 0.7470 | sens_task5: 0.9971 | spec_task5: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7000 (Epoch 1):\n",
      "Loss: 0.8163\n",
      "Learning rate: 0.000100\n",
      "\n",
      "Step    7000  Ep 01  loss 0.8163  lr 0.000100  fetch    0.4 ms  compute 1179.0 ms\n",
      "Accuracy:     T0:0.50  T1:0.51  T2:0.50  T3:0.51  T4:0.52  T5:0.74  \n",
      "Sensitivity:  T0:0.29  T1:0.27  T2:0.53  T3:0.57  T4:0.59  T5:1.00  \n",
      "Specificity:  T0:0.71  T1:0.76  T2:0.48  T3:0.43  T4:0.40  T5:0.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 16:05:42,604 - INFO - Step    7000  Ep 01  loss 0.8163  lr 0.000100  fetch    0.4 ms  compute 1179.0 ms\n",
      "2025-06-10 16:05:42,605 - INFO - Accuracy:     T0:0.50  T1:0.51  T2:0.50  T3:0.51  T4:0.52  T5:0.74  \n",
      "2025-06-10 16:05:42,605 - INFO - Sensitivity:  T0:0.29  T1:0.27  T2:0.53  T3:0.57  T4:0.59  T5:1.00  \n",
      "2025-06-10 16:05:42,605 - INFO - Specificity:  T0:0.71  T1:0.76  T2:0.48  T3:0.43  T4:0.40  T5:0.00  \n",
      "2025-06-10 16:05:42,606 - INFO - iteration: 7000 | epoch: 0 | loss: 0.8163 | lr: 0.0001 | type: training | acc_task0: 0.4981 | sens_task0: 0.2870 | spec_task0: 0.7124 | acc_task1: 0.5078 | sens_task1: 0.2698 | spec_task1: 0.7610 | acc_task2: 0.5031 | sens_task2: 0.5263 | spec_task2: 0.4773 | acc_task3: 0.5074 | sens_task3: 0.5707 | spec_task3: 0.4335 | acc_task4: 0.5194 | sens_task4: 0.5927 | spec_task4: 0.4011 | acc_task5: 0.7439 | sens_task5: 0.9975 | spec_task5: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 8000 (Epoch 1):\n",
      "Loss: 0.8363\n",
      "Learning rate: 0.000100\n",
      "\n",
      "Step    8000  Ep 01  loss 0.8363  lr 0.000100  fetch    0.4 ms  compute 1474.8 ms\n",
      "Accuracy:     T0:0.50  T1:0.51  T2:0.50  T3:0.51  T4:0.53  T5:0.75  \n",
      "Sensitivity:  T0:0.26  T1:0.26  T2:0.48  T3:0.52  T4:0.64  T5:1.00  \n",
      "Specificity:  T0:0.74  T1:0.77  T2:0.53  T3:0.49  T4:0.35  T5:0.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 16:27:59,185 - INFO - Step    8000  Ep 01  loss 0.8363  lr 0.000100  fetch    0.4 ms  compute 1474.8 ms\n",
      "2025-06-10 16:27:59,185 - INFO - Accuracy:     T0:0.50  T1:0.51  T2:0.50  T3:0.51  T4:0.53  T5:0.75  \n",
      "2025-06-10 16:27:59,185 - INFO - Sensitivity:  T0:0.26  T1:0.26  T2:0.48  T3:0.52  T4:0.64  T5:1.00  \n",
      "2025-06-10 16:27:59,185 - INFO - Specificity:  T0:0.74  T1:0.77  T2:0.53  T3:0.49  T4:0.35  T5:0.00  \n",
      "2025-06-10 16:27:59,186 - INFO - iteration: 8000 | epoch: 0 | loss: 0.8363 | lr: 0.0001 | type: training | acc_task0: 0.4998 | sens_task0: 0.2591 | spec_task0: 0.7444 | acc_task1: 0.5100 | sens_task1: 0.2636 | spec_task1: 0.7725 | acc_task2: 0.5017 | sens_task2: 0.4793 | spec_task2: 0.5268 | acc_task3: 0.5053 | sens_task3: 0.5154 | spec_task3: 0.4934 | acc_task4: 0.5312 | sens_task4: 0.6414 | spec_task4: 0.3533 | acc_task5: 0.7466 | sens_task5: 0.9979 | spec_task5: 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 9000 (Epoch 1):\n",
      "Loss: 0.6661\n",
      "Learning rate: 0.000100\n",
      "\n",
      "Step    9000  Ep 01  loss 0.6661  lr 0.000100  fetch    0.4 ms  compute 1229.2 ms\n",
      "Accuracy:     T0:0.50  T1:0.52  T2:0.51  T3:0.51  T4:0.54  T5:0.75  \n",
      "Sensitivity:  T0:0.24  T1:0.27  T2:0.51  T3:0.56  T4:0.68  T5:1.00  \n",
      "Specificity:  T0:0.77  T1:0.77  T2:0.50  T3:0.45  T4:0.32  T5:0.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 16:50:06,013 - INFO - Step    9000  Ep 01  loss 0.6661  lr 0.000100  fetch    0.4 ms  compute 1229.2 ms\n",
      "2025-06-10 16:50:06,017 - INFO - Accuracy:     T0:0.50  T1:0.52  T2:0.51  T3:0.51  T4:0.54  T5:0.75  \n",
      "2025-06-10 16:50:06,017 - INFO - Sensitivity:  T0:0.24  T1:0.27  T2:0.51  T3:0.56  T4:0.68  T5:1.00  \n",
      "2025-06-10 16:50:06,017 - INFO - Specificity:  T0:0.77  T1:0.77  T2:0.50  T3:0.45  T4:0.32  T5:0.00  \n",
      "2025-06-10 16:50:06,017 - INFO - iteration: 9000 | epoch: 0 | loss: 0.6661 | lr: 0.0001 | type: training | acc_task0: 0.5018 | sens_task0: 0.2380 | spec_task0: 0.7691 | acc_task1: 0.5150 | sens_task1: 0.2704 | spec_task1: 0.7747 | acc_task2: 0.5065 | sens_task2: 0.5109 | spec_task2: 0.5017 | acc_task3: 0.5095 | sens_task3: 0.5624 | spec_task3: 0.4478 | acc_task4: 0.5398 | sens_task4: 0.6754 | spec_task4: 0.3214 | acc_task5: 0.7476 | sens_task5: 0.9981 | spec_task5: 0.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10000 (Epoch 1):\n",
      "Loss: 0.7167\n",
      "Learning rate: 0.000100\n",
      "\n",
      "Step   10000  Ep 01  loss 0.7167  lr 0.000100  fetch    0.4 ms  compute 1561.6 ms\n",
      "Accuracy:     T0:0.50  T1:0.52  T2:0.51  T3:0.51  T4:0.54  T5:0.75  \n",
      "Sensitivity:  T0:0.22  T1:0.26  T2:0.55  T3:0.60  T4:0.69  T5:1.00  \n",
      "Specificity:  T0:0.79  T1:0.79  T2:0.47  T3:0.40  T4:0.31  T5:0.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:16:09,118 - INFO - Step   10000  Ep 01  loss 0.7167  lr 0.000100  fetch    0.4 ms  compute 1561.6 ms\n",
      "2025-06-10 17:16:09,119 - INFO - Accuracy:     T0:0.50  T1:0.52  T2:0.51  T3:0.51  T4:0.54  T5:0.75  \n",
      "2025-06-10 17:16:09,119 - INFO - Sensitivity:  T0:0.22  T1:0.26  T2:0.55  T3:0.60  T4:0.69  T5:1.00  \n",
      "2025-06-10 17:16:09,119 - INFO - Specificity:  T0:0.79  T1:0.79  T2:0.47  T3:0.40  T4:0.31  T5:0.00  \n",
      "2025-06-10 17:16:09,119 - INFO - iteration: 10000 | epoch: 0 | loss: 0.7167 | lr: 0.0001 | type: training | acc_task0: 0.5048 | sens_task0: 0.2172 | spec_task0: 0.7924 | acc_task1: 0.5184 | sens_task1: 0.2591 | spec_task1: 0.7901 | acc_task2: 0.5092 | sens_task2: 0.5469 | spec_task2: 0.4680 | acc_task3: 0.5089 | sens_task3: 0.6026 | spec_task3: 0.4013 | acc_task4: 0.5435 | sens_task4: 0.6885 | spec_task4: 0.3144 | acc_task5: 0.7454 | sens_task5: 0.9983 | spec_task5: 0.0028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model components to:\n",
      "  Base: base_ep1_it10000.pt\n",
      "  Aggregator: aggregator_ep1_it10000.pt\n",
      "Saved checkpoint at epoch 1 to /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_JH_d10_B64/checkpoints/model_ep1_it10000.pt\n",
      "\n",
      "Step 11000 (Epoch 1):\n",
      "Loss: 0.5253\n",
      "Learning rate: 0.000100\n",
      "\n",
      "Step   11000  Ep 01  loss 0.5253  lr 0.000100  fetch    0.4 ms  compute 1199.6 ms\n",
      "Accuracy:     T0:0.51  T1:0.52  T2:0.51  T3:0.51  T4:0.55  T5:0.75  \n",
      "Sensitivity:  T0:0.20  T1:0.24  T2:0.57  T3:0.64  T4:0.69  T5:1.00  \n",
      "Specificity:  T0:0.81  T1:0.81  T2:0.46  T3:0.37  T4:0.32  T5:0.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 17:40:36,985 - INFO - Step   11000  Ep 01  loss 0.5253  lr 0.000100  fetch    0.4 ms  compute 1199.6 ms\n",
      "2025-06-10 17:40:36,992 - INFO - Accuracy:     T0:0.51  T1:0.52  T2:0.51  T3:0.51  T4:0.55  T5:0.75  \n",
      "2025-06-10 17:40:36,992 - INFO - Sensitivity:  T0:0.20  T1:0.24  T2:0.57  T3:0.64  T4:0.69  T5:1.00  \n",
      "2025-06-10 17:40:36,992 - INFO - Specificity:  T0:0.81  T1:0.81  T2:0.46  T3:0.37  T4:0.32  T5:0.00  \n",
      "2025-06-10 17:40:36,992 - INFO - iteration: 11000 | epoch: 0 | loss: 0.5253 | lr: 0.0001 | type: training | acc_task0: 0.5050 | sens_task0: 0.1992 | spec_task0: 0.8105 | acc_task1: 0.5192 | sens_task1: 0.2442 | spec_task1: 0.8068 | acc_task2: 0.5149 | sens_task2: 0.5667 | spec_task2: 0.4582 | acc_task3: 0.5127 | sens_task3: 0.6351 | spec_task3: 0.3720 | acc_task4: 0.5477 | sens_task4: 0.6936 | spec_task4: 0.3180 | acc_task5: 0.7454 | sens_task5: 0.9984 | spec_task5: 0.0026\n"
     ]
    }
   ],
   "source": [
    "n_GPU = \"4\"\n",
    "# Set required environment variables\n",
    "import os\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = n_GPU # \"3\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_WORLD_SIZE\"] = n_GPU #  \"2\"\n",
    "# Build the command with --install-packages flag removed\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_launcher.py\", ##changed\n",
    "    \"--num-gpus\", n_GPU, # ,\n",
    "    \"--csv\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_.csv\",\n",
    "    \"--accum-steps\", \"500\", ###############\n",
    "    \"--num-workers\", \"3\",\n",
    "    \"--epochs\", \"100\", ###############\n",
    "    \"--lr\", \"0.0001\", ###############\n",
    "    \"--warmup-steps\", \"10\", ##########\n",
    "    \"--weight-decay\", \"0.01\",\n",
    "    \"--optimizer\", \"adamw\",\n",
    "    \"--num-attn-heads\", \"3\",\n",
    "    \"--num-layers\", \"2\",\n",
    "    \"--dropout\", \"0.35\",\n",
    "    \"--unfreeze-strategy\", \"all\",  \n",
    "    \"--output\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_JH_d10_B64\",\n",
    "    \"--print-every\", \"1000\", ################ 2000\n",
    "    \"--val-every\", \"41000\", ################ 50k\n",
    "    \"--metrics-dir\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu\"\n",
    "        ]\n",
    "# Run the command\n",
    "try:\n",
    "    import subprocess\n",
    "    subprocess.run(command, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(f\"Command output: {e.output if hasattr(e, 'output') else 'No output available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757284b-053c-4ee1-9dd3-ec1d2e9d4959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msalehjahromi (py3.10.12)",
   "language": "python",
   "name": "msalehjahromi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
