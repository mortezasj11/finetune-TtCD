{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2aed02-3d9c-4f45-b9fd-92d71e1238b6",
   "metadata": {},
   "source": [
    "#  DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22ab11-89d3-4805-9cff-b2ee1542fb68",
   "metadata": {},
   "source": [
    "### 1. running on our DGX\n",
    "- docker run -it --rm --gpus '\"device=4,5,6,7\"' -p 12344:12344 --name msalehjahromi --shm-size=192G  --user $(id -u):$(id -g) --group-add 1944259512 --cpuset-cpus=49-96 -v /rsrch7/home/ip_rsrch/wulab/:/rsrch7/home/ip_rsrch/wulab -v /rsrch1/ip/msalehjahromi/:/rsrch1/ip/msalehjahromi --name mori_jupyter nnunetv2:msalehjahromi\n",
    "- cd /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100\n",
    "- jupyter notebook --ip 0.0.0.0 --port 12344\n",
    "- http://1mcprddgx05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14ac30-7017-45e1-a689-710d830d9f36",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb4b0a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mjupyter-msalehjahromi   \u001b[m  Sun Jun  8 01:19:19 2025  \u001b[1m\u001b[30m550.54.15\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 30'C\u001b[m, \u001b[32m 22 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m25808\u001b[m / \u001b[33m40960\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 47'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m25728\u001b[m / \u001b[33m40960\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 41'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  637\u001b[m / \u001b[33m40960\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA A100-SXM4-40GB\u001b[m |\u001b[31m 40'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  637\u001b[m / \u001b[33m40960\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f668c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.device_count())   # → 2\n",
    "print(torch.cuda.current_device()) # → 0 (this maps to your original GPU 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5180fba",
   "metadata": {},
   "source": [
    "### 2. Probably need installation, restart after and run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6f2e39-b110-4be8-82bd-5443f49ca638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## !pip uninstall -q histolab -y\n",
    "# # Install required packages first\n",
    "# import os\n",
    "# import subprocess\n",
    "# pip_commands = [\n",
    "#     [\"pip\", \"install\",\"-q\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cu117\", \n",
    "#      \"torch==2.0.0\", \"torchvision==0.15.0\", \"omegaconf\", \"torchmetrics==0.10.3\", \n",
    "#      \"fvcore\", \"iopath\", \"xformers==0.0.18\", \"submitit\", \"numpy<2.0\"],\n",
    "#     [\"pip\", \"install\", \"-q\",  \"--extra-index-url\", \"https://pypi.nvidia.com\", \"cuml-cu11\"],\n",
    "#     [\"pip\", \"install\",\"-q\",  \"black==22.6.0\", \"flake8==5.0.4\", \"pylint==2.15.0\"],\n",
    "#     [\"pip\", \"install\", \"-q\", \"mmsegmentation==0.27.0\"],\n",
    "#     [\"pip\", \"install\",\"-q\", \"mmcv-full==1.5.0\"],\n",
    "#     [\"pip\", \"install\",\"-q\", \"nibabel\"]\n",
    "# ]\n",
    "\n",
    "# for cmd in pip_commands:\n",
    "#     try:\n",
    "#         print(cmd)\n",
    "#         subprocess.run(cmd, check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Failed to install packages with command: {cmd}\")\n",
    "#         print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b58805",
   "metadata": {},
   "source": [
    "### 3. running, notice important hyperparams\n",
    "-  which model are use so vmin -1000., vmax +150, eps 0.00005\n",
    "- `num-workers`\n",
    "- `epochs`: 100\n",
    "- `accum-steps`: 2000 ?\n",
    "- `max-chunks`: 66 \n",
    "- `lr`:  lr*0.1 on base ?\n",
    "- `warmup-steps`: 5k\n",
    "- `print-every` : 5000\n",
    "- `val-every` : 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/rsrch1/ip/msalehjahromi/codes/dinov2-torchrun-dataloader6/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the directory: ['n_GPU_A100_v2', '._.gitignore', '._.DS_Store', '._See_attention.ipynb', '1_GPU_H100', 'See_attention.ipynb', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n",
      "Files in the directory: ['n_GPU_A100_v2', '._.gitignore', '._.DS_Store', '._See_attention.ipynb', '1_GPU_H100', 'See_attention.ipynb', 'n_GPU_A100', 'metrics_single_gpu', 'n_H100onH100', '.DS_Store', '1_GPU_A100', 'ReadMe.md', '.gitignore', 'metrics_multi_gpu', '.git']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 01:19:29,668 - INFO - Starting training on 2 GPUs with full model copies (DDP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset created on rank \n",
      "\n",
      "==== Dataset Statistics ====\n",
      "Total samples: 43852\n",
      "Training samples: 38382\n",
      "Validation samples: 5470\n",
      "\n",
      "1-year-cancer:\n",
      "  Train positive: 1428 (3.72%)\n",
      "  Val positive: 188 (3.44%)\n",
      "\n",
      "2-year-cancer:\n",
      "  Train positive: 1891 (4.93%)\n",
      "  Val positive: 256 (4.68%)\n",
      "\n",
      "3-year-cancer:\n",
      "  Train positive: 2282 (5.95%)\n",
      "  Val positive: 315 (5.76%)\n",
      "\n",
      "4-year-cancer:\n",
      "  Train positive: 2586 (6.74%)\n",
      "  Val positive: 363 (6.64%)\n",
      "\n",
      "5-year-cancer:\n",
      "  Train positive: 2838 (7.39%)\n",
      "  Val positive: 403 (7.37%)\n",
      "\n",
      "6-year-cancer:\n",
      "  Train positive: 2988 (7.78%)\n",
      "  Val positive: 429 (7.84%)\n",
      "============================\n",
      "\n",
      "Total validation samples: 5470\n",
      "Created distributed validation loader, each rank processes ~2568 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 01:19:31,614 - INFO - using MLP layer as FFN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset created on rank \n",
      "Model loaded on rank \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_full.py:133: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on rank \n",
      "[resume] no model_epoch_*.pt — fresh run\n",
      "Using unweighted BCEWithLogitsLoss\n",
      "Will save metrics to: /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu/training_metrics_nGPU_DDP_20250608_011933.jsonl\n",
      "Will save checkpoints to /rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_JH_06c/checkpoints\n",
      "\n",
      "Training Configuration:\n",
      "Max chunks per sample: 72\n",
      "Learning rate: 8e-06\n",
      "Gradient accumulation steps: 100\n",
      "Number of tasks: 6\n",
      "Validation frequency: 41000 steps\n",
      "Number of epochs: 100\n",
      "Warmup steps: 50\n",
      "World size: 2\n",
      "Device: cuda\n",
      "\n",
      "Epoch 1: All parameters already unfrozen (strategy: all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_full.py:133: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if hasattr(m, n) and getattr(m, n).storage().size() == 0:\n",
      "2025-06-08 01:19:33,553 - INFO - ------ Training Configuration ------\n",
      "2025-06-08 01:19:33,553 - INFO - Max chunks per sample         : 72\n",
      "2025-06-08 01:19:33,553 - INFO - Learning rate (aggregator)    : 8e-06\n",
      "2025-06-08 01:19:33,553 - INFO - Learning rate (base)          : 8e-08\n",
      "2025-06-08 01:19:33,553 - INFO - Gradient accumulation steps   : 100\n",
      "2025-06-08 01:19:33,553 - INFO - Warm-up steps                 : 50\n",
      "2025-06-08 01:19:33,553 - INFO - Aggregator layers / heads     : 2 / 3\n",
      "2025-06-08 01:19:33,553 - INFO - Aggregator dropout            : 0.35\n",
      "2025-06-08 01:19:33,553 - INFO - Validation frequency          : 41000 steps\n",
      "2025-06-08 01:19:33,553 - INFO - Number of epochs              : 100\n",
      "2025-06-08 01:19:33,553 - INFO - World size                    : 2\n",
      "2025-06-08 01:19:33,553 - INFO - Device                        : cuda\n",
      "2025-06-08 01:19:33,553 - INFO - ------------------------------------\n",
      "\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:338: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.value.storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage_data_ptr = tensors[0].storage().data_ptr()\n",
      "/home/msalehjahromi/.local/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if x.storage().data_ptr() != storage_data_ptr:\n"
     ]
    }
   ],
   "source": [
    "## Install required packages first\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "# Set required environment variables\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"2\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_WORLD_SIZE\"] = \"2\"\n",
    "\n",
    "# Build the command with --install-packages flag removed\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/2_run_fineTune_ddp_launcher.py\", ##changed\n",
    "    \"--num-gpus\", \"2\",\n",
    "    \"--csv\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/nlst_event_train_val_.csv\",\n",
    "    \"--accum-steps\", \"100\", ###############\n",
    "    \"--num-workers\", \"8\",\n",
    "    \"--epochs\", \"100\", ###############\n",
    "    \"--lr\", \"0.000008\", ###############\n",
    "    \"--warmup-steps\", \"50\", ##########\n",
    "    \"--weight-decay\", \"0.001\",\n",
    "    \"--optimizer\", \"adamw\",\n",
    "    \"--num-attn-heads\", \"3\",\n",
    "    \"--num-layers\", \"2\",\n",
    "    \"--dropout\", \"0.35\",\n",
    "    \"--unfreeze-strategy\", \"all\",  \n",
    "    \"--output\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/n_GPU_A100_v2/output_ddp_p16_23_1150_ac200_JH_06c\",\n",
    "    \"--print-every\", \"2000\", ################ 2000\n",
    "    \"--val-every\", \"41000\", ################ 50k\n",
    "    \"--metrics-dir\", \"/rsrch1/ip/msalehjahromi/codes/FineTune/multiGPU/metrics_multi_gpu\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(f\"Command output: {e.output if hasattr(e, 'output') else 'No output available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757284b-053c-4ee1-9dd3-ec1d2e9d4959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msalehjahromi (py3.10.12)",
   "language": "python",
   "name": "msalehjahromi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
